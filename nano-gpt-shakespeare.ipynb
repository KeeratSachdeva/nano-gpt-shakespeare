{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEfhjL4JGF_"
      },
      "source": [
        "# Transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXLldIyoJLOA",
        "outputId": "96bf83dd-3933-4933-9c8d-f8b244c9ba93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-03 07:16:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-03 07:16:59 (18.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BOUQ00DkV8K",
        "outputId": "c5f43c9e-c06f-459f-ea00-b8756f5a72f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHgDXLe7kfoh",
        "outputId": "d2fe651c-cd49-4987-9a3a-553d0637abec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# All possible characters the model can see and emmit.\n",
        "\n",
        "vocabulary = sorted(list(set(text)))\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf-yuc9Fk-pw",
        "outputId": "1f6c5599-a606-4330-d19a-8bb376efaed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(vocabulary)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA530NQHlExD",
        "outputId": "ff3f3e18-3194-4da3-f4a3-5c8ee76cd26c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'\\n': 0,\n",
              "  ' ': 1,\n",
              "  '!': 2,\n",
              "  '$': 3,\n",
              "  '&': 4,\n",
              "  \"'\": 5,\n",
              "  ',': 6,\n",
              "  '-': 7,\n",
              "  '.': 8,\n",
              "  '3': 9,\n",
              "  ':': 10,\n",
              "  ';': 11,\n",
              "  '?': 12,\n",
              "  'A': 13,\n",
              "  'B': 14,\n",
              "  'C': 15,\n",
              "  'D': 16,\n",
              "  'E': 17,\n",
              "  'F': 18,\n",
              "  'G': 19,\n",
              "  'H': 20,\n",
              "  'I': 21,\n",
              "  'J': 22,\n",
              "  'K': 23,\n",
              "  'L': 24,\n",
              "  'M': 25,\n",
              "  'N': 26,\n",
              "  'O': 27,\n",
              "  'P': 28,\n",
              "  'Q': 29,\n",
              "  'R': 30,\n",
              "  'S': 31,\n",
              "  'T': 32,\n",
              "  'U': 33,\n",
              "  'V': 34,\n",
              "  'W': 35,\n",
              "  'X': 36,\n",
              "  'Y': 37,\n",
              "  'Z': 38,\n",
              "  'a': 39,\n",
              "  'b': 40,\n",
              "  'c': 41,\n",
              "  'd': 42,\n",
              "  'e': 43,\n",
              "  'f': 44,\n",
              "  'g': 45,\n",
              "  'h': 46,\n",
              "  'i': 47,\n",
              "  'j': 48,\n",
              "  'k': 49,\n",
              "  'l': 50,\n",
              "  'm': 51,\n",
              "  'n': 52,\n",
              "  'o': 53,\n",
              "  'p': 54,\n",
              "  'q': 55,\n",
              "  'r': 56,\n",
              "  's': 57,\n",
              "  't': 58,\n",
              "  'u': 59,\n",
              "  'v': 60,\n",
              "  'w': 61,\n",
              "  'x': 62,\n",
              "  'y': 63,\n",
              "  'z': 64},\n",
              " {0: '\\n',\n",
              "  1: ' ',\n",
              "  2: '!',\n",
              "  3: '$',\n",
              "  4: '&',\n",
              "  5: \"'\",\n",
              "  6: ',',\n",
              "  7: '-',\n",
              "  8: '.',\n",
              "  9: '3',\n",
              "  10: ':',\n",
              "  11: ';',\n",
              "  12: '?',\n",
              "  13: 'A',\n",
              "  14: 'B',\n",
              "  15: 'C',\n",
              "  16: 'D',\n",
              "  17: 'E',\n",
              "  18: 'F',\n",
              "  19: 'G',\n",
              "  20: 'H',\n",
              "  21: 'I',\n",
              "  22: 'J',\n",
              "  23: 'K',\n",
              "  24: 'L',\n",
              "  25: 'M',\n",
              "  26: 'N',\n",
              "  27: 'O',\n",
              "  28: 'P',\n",
              "  29: 'Q',\n",
              "  30: 'R',\n",
              "  31: 'S',\n",
              "  32: 'T',\n",
              "  33: 'U',\n",
              "  34: 'V',\n",
              "  35: 'W',\n",
              "  36: 'X',\n",
              "  37: 'Y',\n",
              "  38: 'Z',\n",
              "  39: 'a',\n",
              "  40: 'b',\n",
              "  41: 'c',\n",
              "  42: 'd',\n",
              "  43: 'e',\n",
              "  44: 'f',\n",
              "  45: 'g',\n",
              "  46: 'h',\n",
              "  47: 'i',\n",
              "  48: 'j',\n",
              "  49: 'k',\n",
              "  50: 'l',\n",
              "  51: 'm',\n",
              "  52: 'n',\n",
              "  53: 'o',\n",
              "  54: 'p',\n",
              "  55: 'q',\n",
              "  56: 'r',\n",
              "  57: 's',\n",
              "  58: 't',\n",
              "  59: 'u',\n",
              "  60: 'v',\n",
              "  61: 'w',\n",
              "  62: 'x',\n",
              "  63: 'y',\n",
              "  64: 'z'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stoi = {s: i for i, s in enumerate(vocabulary)}\n",
        "itos = {i: s for i, s in enumerate(vocabulary)}\n",
        "\n",
        "stoi, itos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkyJzuqQlXRY",
        "outputId": "d6fdbc2c-4b0b-4b5d-ae6b-6852256c68ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[20, 47, 1, 32, 46, 43, 56, 43, 2]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode = lambda s: [stoi[s] for s in list(s)]\n",
        "encode('Hi There!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "az6BW2i3lrdk",
        "outputId": "f955a567-f586-4401-b06d-1832d1db7162"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi There!'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "decode([20, 47, 1, 32, 46, 43, 56, 43, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQjgTgs1mTMb"
      },
      "outputs": [],
      "source": [
        "import torch # ML Framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f84yLxfpmA8P",
        "outputId": "b5b0b74f-b3d9-42bf-d501-f19d93c74dfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56,  ..., 45,  8,  0])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(encode(text))\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1wKFFy7mSEg",
        "outputId": "5e42aba4-8a94-4b20-f173-423c416423e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1115394, 1115394)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data), len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp-s5kokmgaz",
        "outputId": "a414bc9c-7b09-4207-fd1f-bdcf5850d661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1003854, 111540, 1115394)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_size = int(0.9 * len(data))\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "len(train_data), len(test_data), len(train_data) + len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7yKMw_Bm0q2",
        "outputId": "9b103d33-9de2-42c1-f667-c790fc3bd5f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8 # Context Window. (Maximum number of tokens the model can process at a time/ uske baad truncation).\n",
        "train_data[:block_size + 1] # Block size + 1 tokens me Block size possible combinations hai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N99GfjldnSIj",
        "outputId": "55d25996-5252-4f41-bf82-68fc1f005410"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 0])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randint(0, 5, (5, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH8zpHkvnmFB"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4 # Batches of inputs to the Neural Network / Batches of input sequence/tokens.\n",
        "block_size = 8\n",
        "\n",
        "def get_data(data_type):\n",
        "  _data = train_data if data_type == 'train' else test_data\n",
        "  idxs = torch.randint(0, len(_data) - block_size, (batch_size, )) # Batch size number of random indeces.\n",
        "  xb = torch.stack([_data[idx: idx + block_size] for idx in idxs], dim = 0)\n",
        "  yb = torch.stack([_data[idx + 1: idx + block_size + 1] for idx in idxs], dim = 0)\n",
        "  return xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV335ZM1qYo0",
        "outputId": "51df1f72-e781-48a8-bcc1-25fe2e40d77d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
              " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
              "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
              "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
              "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Har batch (4) me block size (8) examples packed hai. Total: 32 sequences of text.\n",
        "\n",
        "xb, yb = get_data('train')\n",
        "xb, yb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76FunT1Yqngg"
      },
      "source": [
        "## Basic Bi-Gram model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCaTiUUEqiCr"
      },
      "outputs": [],
      "source": [
        "# Upar humne one hot encoding ki thi. Yaha hum embeddings use karenge.\n",
        "# Kind of a vector representation of my discrete/distinct tokens (65)/ vocabulary.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vz2WBpJrlxz",
        "outputId": "b479ef4f-7be5-404b-e421-3c8794257f5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(5, 3)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer = nn.Embedding(5, 3) # 5 embeddings of size 3. Possible options/indeces -> 0, 1, 2, 3, 4\n",
        "embedding_layer # You get a vector for all your possible tokens. Initially these numbers/embeddings are random/->updated (with training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFviP4f5r0CS",
        "outputId": "5e39a200-5338-434d-f2d3-d4b0dee650a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.2054, -0.9122, -1.2502], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer(torch.tensor(0)) # 0D -> 1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TWmlfqmsQ1Z",
        "outputId": "7440a368-e0a7-4e2e-b5d2-d002433659a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2054, -0.9122, -1.2502],\n",
              "        [ 0.8032, -0.2071,  0.0544]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer(torch.tensor([0, 1])) # 1D -> 2D (Kind of an output that can simpley be fed into a NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBKTp6uUs2Ge",
        "outputId": "dfa6b609-c86e-46b6-9872-835c56b52887"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.2054, -0.9122, -1.2502],\n",
              "         [ 0.8032, -0.2071,  0.0544]],\n",
              "\n",
              "        [[ 0.1378, -0.3889,  0.5133],\n",
              "         [ 0.3319,  0.6300,  0.5815]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer(torch.tensor([[0, 1], [2, 3]])) # 2D -> 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOv8L-Fu59M7",
        "outputId": "b45ccb29-67d5-4866-8760-54c351b5821e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer(torch.tensor([[0, 1], [2, 3]])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTgdKWxPtIXT",
        "outputId": "64ddf8b2-b399-47da-ffb2-07db1f9d56b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8]),\n",
              " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "         [25, 17, 27, 10,  0, 21,  1, 54]]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xb.shape, xb # (B, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pcOJOA0t8ND",
        "outputId": "5d02e10f-4fb4-42b2-e69f-452a55e2c78b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 65])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.Embedding(vocab_size, vocab_size)(xb).shape # 1 embedding for each token in the vocabulary.\n",
        "# A 65-sized vector for all the 4 X 8 = 32 tokens. # B, T, C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q79DX8nXMmYj",
        "outputId": "3382a1da-2ae5-4caa-c4f8-152528478a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function cross_entropy in module torch.nn.functional:\n",
            "\n",
            "cross_entropy(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean', label_smoothing: float = 0.0) -> torch.Tensor\n",
            "    Compute the cross entropy loss between input logits and target.\n",
            "    \n",
            "    See :class:`~torch.nn.CrossEntropyLoss` for details.\n",
            "    \n",
            "    Args:\n",
            "        input (Tensor) : Predicted unnormalized logits;\n",
            "            see Shape section below for supported shapes.\n",
            "        target (Tensor) : Ground truth class indices or class probabilities;\n",
            "            see Shape section below for supported shapes.\n",
            "        weight (Tensor, optional): a manual rescaling weight given to each\n",
            "            class. If given, has to be a Tensor of size `C`\n",
            "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "            the losses are averaged over each loss element in the batch. Note that for\n",
            "            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "            when reduce is ``False``. Default: ``True``\n",
            "        ignore_index (int, optional): Specifies a target value that is ignored\n",
            "            and does not contribute to the input gradient. When :attr:`size_average` is\n",
            "            ``True``, the loss is averaged over non-ignored targets. Note that\n",
            "            :attr:`ignore_index` is only applicable when the target contains class indices.\n",
            "            Default: -100\n",
            "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "            losses are averaged or summed over observations for each minibatch depending\n",
            "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "            ``'mean'``: the sum of the output will be divided by the number of\n",
            "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
            "            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
            "            become a mixture of the original ground truth and a uniform distribution as described in\n",
            "            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
            "    \n",
            "    Shape:\n",
            "        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
            "          in the case of `K`-dimensional loss.\n",
            "        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
            "          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
            "          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
            "    \n",
            "        where:\n",
            "    \n",
            "        .. math::\n",
            "            \\begin{aligned}\n",
            "                C ={} & \\text{number of classes} \\\\\n",
            "                N ={} & \\text{batch size} \\\\\n",
            "            \\end{aligned}\n",
            "    \n",
            "    Examples::\n",
            "    \n",
            "        >>> # Example of target with class indices\n",
            "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
            "        >>> target = torch.randint(5, (3,), dtype=torch.int64)\n",
            "        >>> loss = F.cross_entropy(input, target)\n",
            "        >>> loss.backward()\n",
            "        >>>\n",
            "        >>> # Example of target with class probabilities\n",
            "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
            "        >>> target = torch.randn(3, 5).softmax(dim=1)\n",
            "        >>> loss = F.cross_entropy(input, target)\n",
            "        >>> loss.backward()\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(F.cross_entropy) # - log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dYo0OR4M2Nt",
        "outputId": "8bc06bd8-95d8-40c2-99b1-bd3dd6a506e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.2815,  0.4047,  0.0632,  ..., -0.2625,  1.1260, -0.1366],\n",
              "        [-0.5828,  1.0230, -1.1334,  ...,  0.1380, -0.0623,  0.4172],\n",
              "        [-1.3142, -0.2682, -0.1609,  ...,  0.9439, -2.1673,  2.0899],\n",
              "        ...,\n",
              "        [-0.7827,  2.4638,  0.7175,  ..., -0.2406,  0.5494,  0.0308],\n",
              "        [-0.1004,  0.2456,  0.0935,  ...,  0.0534, -0.1949,  0.8603],\n",
              "        [ 1.3676,  0.1808,  0.5740,  ...,  0.5384,  0.2382, -1.5247]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predicted unnormalized logits & Ground truth class indeces.\n",
        "logits = nn.Embedding(vocab_size, vocab_size)(xb)\n",
        "B, T, C = logits.shape\n",
        "logits.view(B * T, C) # Embeddings for all 32 tokens. # Unnormalized logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0R9XRykO2oh",
        "outputId": "a0cbc8ef-cd25-48c8-db9d-b247c136dbf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([43, 58,  5, 57,  1, 46, 43, 39, 53, 56,  1, 58, 46, 39, 58,  1, 58,  1,\n",
              "        58, 46, 39, 58,  1, 46, 17, 27, 10,  0, 21,  1, 54, 39])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Output tokens -> yb\n",
        "yb.view(-1, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfMJnkrVO4jJ",
        "outputId": "51913b6b-c398-4402-ffc7-182837fa9959"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.7724, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cross_entropy(logits.view(B * T, C), yb.view(-1, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlriiKMYQLGs",
        "outputId": "6ff12e7c-1675-4259-c4fe-5afaae4e5603"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.7724, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = logits.view(B * T, C)\n",
        "counts = logits.exp()\n",
        "probabilities = counts / counts.sum(1, True)\n",
        "-probabilities[range(len(logits)), yb.view(-1, )].log().mean() # Exactly the same as negative log likelihood that we calculated before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9C3wgIcSB_G",
        "outputId": "80df28bb-ec82-4c0a-d6bd-3ff996dce771"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0576, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# - sigma(pi * log(pi)) ??\n",
        "-(probabilities * probabilities.log()).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teG3yrMw8pWT"
      },
      "source": [
        "## Class implementation of NN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2ZzjM2bub8q"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # Channel Dimension (vocab_size).\n",
        "\n",
        "  def forward(self, x, y = None):\n",
        "    logits = self.token_embedding_table(x) # log counts. (B, T, C)\n",
        "\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, x, max_tokens):\n",
        "    for _ in range(max_tokens):\n",
        "      logits, loss = self(x) # Pura x pass kia. (B, T) -> (B, T, C)\n",
        "\n",
        "      logits = logits[:, -1, :] # Per batch (Last token ki embeddings le li) -> 65 (B * C) (Last token in time.)\n",
        "      probabilities = F.softmax(logits, dim = 1) # Auto-built function for us. to do e^x/sigma(e^x) (B * C)\n",
        "\n",
        "      x_next = torch.multinomial(probabilities, 1, replacement = True) # (B, 1)\n",
        "\n",
        "      x = torch.concat([x, x_next], dim = 1) # (B, T + 1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQLRCn6kLG-O",
        "outputId": "181c6f34-99e5-4a9f-c54f-5152f4b50bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[ 0.3498,  1.3103, -0.9592,  ...,  1.0153, -0.1428,  2.3518],\n",
              "          [ 2.2402,  0.6659,  0.8235,  ..., -1.1192,  2.5383, -0.4066],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          ...,\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          [ 0.3879, -1.0859,  1.5126,  ...,  0.3447,  0.8484, -0.2543],\n",
              "          [ 2.2402,  0.6659,  0.8235,  ..., -1.1192,  2.5383, -0.4066]],\n",
              " \n",
              "         [[-0.2413,  1.0492, -0.5896,  ..., -0.3442,  0.1582, -0.1497],\n",
              "          [-1.6302,  1.4898, -0.4206,  ..., -1.9137, -0.9732, -0.2703],\n",
              "          [-1.4160,  0.2531,  1.1756,  ..., -0.1245,  0.0146,  1.8470],\n",
              "          ...,\n",
              "          [ 0.3879, -1.0859,  1.5126,  ...,  0.3447,  0.8484, -0.2543],\n",
              "          [-0.2533,  0.5686,  0.3427,  ...,  0.7236,  1.4675,  1.3112],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001]],\n",
              " \n",
              "         [[-0.0115, -0.3571,  0.0865,  ...,  1.6329, -0.1404, -0.7568],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          ...,\n",
              "          [-0.2533,  0.5686,  0.3427,  ...,  0.7236,  1.4675,  1.3112],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263]],\n",
              " \n",
              "         [[-0.0950,  0.5261,  1.1286,  ..., -0.1676,  0.7951, -0.3607],\n",
              "          [-0.8099, -0.3329,  0.1935,  ...,  1.0668,  0.6677,  1.2049],\n",
              "          [-1.0002, -0.9106, -1.8551,  ..., -0.5195,  0.3978, -0.8861],\n",
              "          ...,\n",
              "          [-2.4369,  0.7748, -0.6426,  ..., -0.7241, -0.3591,  0.2482],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          [ 1.2191,  0.8378, -0.3448,  ...,  0.1703, -0.5210,  0.8319]]],\n",
              "        grad_fn=<EmbeddingBackward0>),\n",
              " None)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "model(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW3IlngwL5S5",
        "outputId": "7bfcea2f-94d5-4188-da92-69bce7b32c3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[ 0.3498,  1.3103, -0.9592,  ...,  1.0153, -0.1428,  2.3518],\n",
              "          [ 2.2402,  0.6659,  0.8235,  ..., -1.1192,  2.5383, -0.4066],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          ...,\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          [ 0.3879, -1.0859,  1.5126,  ...,  0.3447,  0.8484, -0.2543],\n",
              "          [ 2.2402,  0.6659,  0.8235,  ..., -1.1192,  2.5383, -0.4066]],\n",
              " \n",
              "         [[-0.2413,  1.0492, -0.5896,  ..., -0.3442,  0.1582, -0.1497],\n",
              "          [-1.6302,  1.4898, -0.4206,  ..., -1.9137, -0.9732, -0.2703],\n",
              "          [-1.4160,  0.2531,  1.1756,  ..., -0.1245,  0.0146,  1.8470],\n",
              "          ...,\n",
              "          [ 0.3879, -1.0859,  1.5126,  ...,  0.3447,  0.8484, -0.2543],\n",
              "          [-0.2533,  0.5686,  0.3427,  ...,  0.7236,  1.4675,  1.3112],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001]],\n",
              " \n",
              "         [[-0.0115, -0.3571,  0.0865,  ...,  1.6329, -0.1404, -0.7568],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          ...,\n",
              "          [-0.2533,  0.5686,  0.3427,  ...,  0.7236,  1.4675,  1.3112],\n",
              "          [-1.6217, -1.7992, -1.4361,  ..., -0.4915,  0.7770,  0.9001],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263]],\n",
              " \n",
              "         [[-0.0950,  0.5261,  1.1286,  ..., -0.1676,  0.7951, -0.3607],\n",
              "          [-0.8099, -0.3329,  0.1935,  ...,  1.0668,  0.6677,  1.2049],\n",
              "          [-1.0002, -0.9106, -1.8551,  ..., -0.5195,  0.3978, -0.8861],\n",
              "          ...,\n",
              "          [-2.4369,  0.7748, -0.6426,  ..., -0.7241, -0.3591,  0.2482],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          [ 1.2191,  0.8378, -0.3448,  ...,  0.1703, -0.5210,  0.8319]]],\n",
              "        grad_fn=<EmbeddingBackward0>),\n",
              " tensor(4.9826, grad_fn=<NllLossBackward0>))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(xb, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQR2kpWtXtej",
        "outputId": "f6be5c28-4f7e-4c81-a0cf-c6c328b48086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ViYMNUlzu'-hXWPRIRiNQuF\n",
            "UyEiysBx3KtjcOOCeOhUlM.pq:RTT;jdQuF:YEHrM!m;UmCzoFAxHtHu;&p&\n",
            "j$ bErVaOQu'WvX\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(torch.tensor([[0]]), 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIR8-5tWee76",
        "outputId": "f9c57c7f-5e3e-4f9c-cd48-62b92fc05a55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x795c00135ee0>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training process.\n",
        "\n",
        "model.parameters() # Use/Pass all the model parameters to the optimizer object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVQ946lzegy2",
        "outputId": "53c2bf12-9ce3-4440-8705-cf9086c4967f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([Parameter containing:\n",
              "  tensor([[ 1.1891,  0.4247,  1.4639,  ...,  0.8044, -1.9819,  1.0756],\n",
              "          [-0.5502, -0.2549,  0.7017,  ...,  0.1585, -1.6419, -1.5263],\n",
              "          [-0.0566,  0.9142, -0.2427,  ..., -0.1146, -1.3816, -0.8433],\n",
              "          ...,\n",
              "          [-0.0275, -0.8411, -0.5339,  ...,  0.0678,  0.6898, -0.2888],\n",
              "          [ 0.2983,  0.0298,  2.1243,  ...,  1.4154,  0.2682, -2.1083],\n",
              "          [ 0.6007, -0.8245,  0.1892,  ...,  0.5648,  0.1959, -0.5216]],\n",
              "         requires_grad=True)],\n",
              " torch.Size([65, 65]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.parameters()), list(model.parameters())[0].shape # vocab_size, vocab_size weights for an embedding layer of this dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo7i86xie43j",
        "outputId": "6195b5fe-63d8-4fba-a794-0afbaae743c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0.01\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX4lMYUtfdon",
        "outputId": "03d113f9-c4ef-4029-c83f-6b947d35ee99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2.492692470550537\n",
            "2.4917163848876953\n",
            "2.470090627670288\n",
            "2.3553788661956787\n",
            "2.432141065597534\n",
            "2.3853700160980225\n",
            "2.5103354454040527\n",
            "2.316120147705078\n",
            "2.533888101577759\n",
            "2.490656614303589\n",
            "2.3366808891296387\n",
            "2.50423002243042\n",
            "2.532947540283203\n",
            "2.4066920280456543\n",
            "2.4915266036987305\n",
            "2.4837071895599365\n",
            "2.5582199096679688\n",
            "2.538115978240967\n",
            "2.4007411003112793\n",
            "2.4214749336242676\n",
            "2.3402693271636963\n",
            "2.3532450199127197\n",
            "2.4605202674865723\n",
            "2.461646795272827\n",
            "2.4856483936309814\n",
            "2.5099072456359863\n",
            "2.5644752979278564\n",
            "2.435406446456909\n",
            "2.3799827098846436\n",
            "2.3693389892578125\n",
            "2.3270442485809326\n",
            "2.3448550701141357\n",
            "2.451723575592041\n",
            "2.5898542404174805\n",
            "2.482839822769165\n",
            "2.445624351501465\n",
            "2.5033938884735107\n",
            "2.4439496994018555\n",
            "2.482853412628174\n",
            "2.492849111557007\n",
            "2.4715545177459717\n",
            "2.421292304992676\n",
            "2.486783742904663\n",
            "2.494849681854248\n",
            "2.358307361602783\n",
            "2.5199904441833496\n",
            "2.3289263248443604\n",
            "2.539226770401001\n",
            "2.4005813598632812\n",
            "2.470928907394409\n",
            "2.3402035236358643\n",
            "2.3999080657958984\n",
            "2.3630290031433105\n",
            "2.5771641731262207\n",
            "2.4575107097625732\n",
            "2.459068536758423\n",
            "2.4263150691986084\n",
            "2.5282602310180664\n",
            "2.454862594604492\n",
            "2.4622089862823486\n",
            "2.5071284770965576\n",
            "2.279468059539795\n",
            "2.543992757797241\n",
            "2.4578332901000977\n",
            "2.529761791229248\n",
            "2.420419216156006\n",
            "2.2947371006011963\n",
            "2.4187216758728027\n",
            "2.4347710609436035\n",
            "2.417721748352051\n",
            "2.4869344234466553\n",
            "2.5584628582000732\n",
            "2.4987571239471436\n",
            "2.46594500541687\n",
            "2.382107734680176\n",
            "2.3547849655151367\n",
            "2.310865879058838\n",
            "2.3985755443573\n",
            "2.3619229793548584\n",
            "2.4723103046417236\n",
            "2.3999881744384766\n",
            "2.5066866874694824\n",
            "2.454977035522461\n",
            "2.4941160678863525\n",
            "2.583852767944336\n",
            "2.528784990310669\n",
            "2.550096035003662\n",
            "2.4910435676574707\n",
            "2.6071248054504395\n",
            "2.408128023147583\n",
            "2.5788135528564453\n",
            "2.484489917755127\n",
            "2.404686450958252\n",
            "2.2625346183776855\n",
            "2.5373547077178955\n",
            "2.376061201095581\n",
            "2.373612880706787\n",
            "2.4256057739257812\n",
            "2.423619031906128\n",
            "2.339420795440674\n",
            "2.465710401535034\n",
            "2.4186224937438965\n",
            "2.46197509765625\n",
            "2.424220323562622\n",
            "2.5346837043762207\n",
            "2.3994107246398926\n",
            "2.4441707134246826\n",
            "2.4246625900268555\n",
            "2.445429801940918\n",
            "2.466704845428467\n",
            "2.4855568408966064\n",
            "2.5304207801818848\n",
            "2.4395084381103516\n",
            "2.3268351554870605\n",
            "2.3857967853546143\n",
            "2.383424758911133\n",
            "2.4316656589508057\n",
            "2.3706276416778564\n",
            "2.504852533340454\n",
            "2.422759771347046\n",
            "2.39633846282959\n",
            "2.5217556953430176\n",
            "2.435863494873047\n",
            "2.5123603343963623\n",
            "2.4276154041290283\n",
            "2.5003514289855957\n",
            "2.458653688430786\n",
            "2.6108527183532715\n",
            "2.3076062202453613\n",
            "2.5230066776275635\n",
            "2.467620849609375\n",
            "2.4654834270477295\n",
            "2.449974298477173\n",
            "2.382628917694092\n",
            "2.5059046745300293\n",
            "2.4940292835235596\n",
            "2.3935093879699707\n",
            "2.4914252758026123\n",
            "2.4212725162506104\n",
            "2.4266743659973145\n",
            "2.391441822052002\n",
            "2.5558102130889893\n",
            "2.367426633834839\n",
            "2.3136720657348633\n",
            "2.4242167472839355\n",
            "2.50927734375\n",
            "2.4442272186279297\n",
            "2.4905083179473877\n",
            "2.4465482234954834\n",
            "2.4281246662139893\n",
            "2.4963889122009277\n",
            "2.3585031032562256\n",
            "2.4896533489227295\n",
            "2.487245559692383\n",
            "2.4050848484039307\n",
            "2.493488311767578\n",
            "2.5373971462249756\n",
            "2.464179515838623\n",
            "2.4307303428649902\n",
            "2.5371155738830566\n",
            "2.468446731567383\n",
            "2.355659246444702\n",
            "2.5435194969177246\n",
            "2.378124713897705\n",
            "2.318984031677246\n",
            "2.3867366313934326\n",
            "2.413957118988037\n",
            "2.4474985599517822\n",
            "2.5178418159484863\n",
            "2.442070722579956\n",
            "2.4823384284973145\n",
            "2.5308594703674316\n",
            "2.6051273345947266\n",
            "2.4818224906921387\n",
            "2.391690254211426\n",
            "2.423609733581543\n",
            "2.444981098175049\n",
            "2.3911662101745605\n",
            "2.6019694805145264\n",
            "2.3915085792541504\n",
            "2.4056928157806396\n",
            "2.552225112915039\n",
            "2.468280076980591\n",
            "2.509450674057007\n",
            "2.427485942840576\n",
            "2.321382522583008\n",
            "2.4823195934295654\n",
            "2.4169068336486816\n",
            "2.4861791133880615\n",
            "2.4284918308258057\n",
            "2.581155300140381\n",
            "2.466362237930298\n",
            "2.5476696491241455\n",
            "2.4143242835998535\n",
            "2.407569169998169\n",
            "2.3368194103240967\n",
            "2.525481700897217\n",
            "2.4751217365264893\n",
            "2.544949531555176\n",
            "2.4617092609405518\n",
            "2.364905834197998\n",
            "2.489858865737915\n",
            "2.554701089859009\n",
            "2.3915982246398926\n",
            "2.4017152786254883\n",
            "2.426279067993164\n",
            "2.55220627784729\n",
            "2.443291187286377\n",
            "2.5741500854492188\n",
            "2.3790600299835205\n",
            "2.4736320972442627\n",
            "2.4210710525512695\n",
            "2.5769989490509033\n",
            "2.5336971282958984\n",
            "2.4090232849121094\n",
            "2.448404312133789\n",
            "2.408287763595581\n",
            "2.3976335525512695\n",
            "2.568268299102783\n",
            "2.3589284420013428\n",
            "2.5499000549316406\n",
            "2.448336601257324\n",
            "2.5665531158447266\n",
            "2.5186846256256104\n",
            "2.5315940380096436\n",
            "2.414184808731079\n",
            "2.4693603515625\n",
            "2.572249174118042\n",
            "2.350555419921875\n",
            "2.461294651031494\n",
            "2.5058233737945557\n",
            "2.4559218883514404\n",
            "2.454678773880005\n",
            "2.6072444915771484\n",
            "2.446065902709961\n",
            "2.318887948989868\n",
            "2.4458324909210205\n",
            "2.4775145053863525\n",
            "2.6630046367645264\n",
            "2.3423945903778076\n",
            "2.3502824306488037\n",
            "2.5230743885040283\n",
            "2.458911180496216\n",
            "2.416088104248047\n",
            "2.468454122543335\n",
            "2.6804466247558594\n",
            "2.399290084838867\n",
            "2.4334661960601807\n",
            "2.4421377182006836\n",
            "2.435750961303711\n",
            "2.4678902626037598\n",
            "2.3876585960388184\n",
            "2.4870381355285645\n",
            "2.52778959274292\n",
            "2.397228479385376\n",
            "2.5299830436706543\n",
            "2.4749271869659424\n",
            "2.50362491607666\n",
            "2.5498669147491455\n",
            "2.6473352909088135\n",
            "2.466845750808716\n",
            "2.4990344047546387\n",
            "2.5650811195373535\n",
            "2.448739767074585\n",
            "2.4875357151031494\n",
            "2.349822521209717\n",
            "2.4983744621276855\n",
            "2.4194090366363525\n",
            "2.5357463359832764\n",
            "2.4711127281188965\n",
            "2.3809542655944824\n",
            "2.4768731594085693\n",
            "2.3737964630126953\n",
            "2.4507479667663574\n",
            "2.558493137359619\n",
            "2.540832996368408\n",
            "2.433751106262207\n",
            "2.437197685241699\n",
            "2.458212375640869\n",
            "2.3769447803497314\n",
            "2.398047924041748\n",
            "2.56306529045105\n",
            "2.4663007259368896\n",
            "2.408890962600708\n",
            "2.5130374431610107\n",
            "2.6833887100219727\n",
            "2.429917812347412\n",
            "2.478914499282837\n",
            "2.535059690475464\n",
            "2.4828805923461914\n",
            "2.390305519104004\n",
            "2.5448241233825684\n",
            "2.4655988216400146\n",
            "2.4284610748291016\n",
            "2.4094324111938477\n",
            "2.445164442062378\n",
            "2.384829044342041\n",
            "2.5355939865112305\n",
            "2.2893857955932617\n",
            "2.478393316268921\n",
            "2.349717855453491\n",
            "2.5652809143066406\n",
            "2.490330934524536\n",
            "2.447875738143921\n",
            "2.528280735015869\n",
            "2.465153455734253\n",
            "2.489450216293335\n",
            "2.449582576751709\n",
            "2.4278271198272705\n",
            "2.477426767349243\n",
            "2.438961982727051\n",
            "2.5522921085357666\n",
            "2.6757354736328125\n",
            "2.4014956951141357\n",
            "2.513559103012085\n",
            "2.6664390563964844\n",
            "2.5296108722686768\n",
            "2.461682081222534\n",
            "2.5909929275512695\n",
            "2.510531425476074\n",
            "2.430795192718506\n",
            "2.4752447605133057\n",
            "2.427138328552246\n",
            "2.4954206943511963\n",
            "2.439267158508301\n",
            "2.4934589862823486\n",
            "2.5586352348327637\n",
            "2.5257604122161865\n",
            "2.4909942150115967\n",
            "2.384761333465576\n",
            "2.4055981636047363\n",
            "2.4435243606567383\n",
            "2.3893234729766846\n",
            "2.520716428756714\n",
            "2.408977746963501\n",
            "2.3852803707122803\n",
            "2.4768972396850586\n",
            "2.409306526184082\n",
            "2.4096591472625732\n",
            "2.4263534545898438\n",
            "2.480414628982544\n",
            "2.4775943756103516\n",
            "2.4997754096984863\n",
            "2.588477373123169\n",
            "2.4180686473846436\n",
            "2.5946133136749268\n",
            "2.4266345500946045\n",
            "2.4429049491882324\n",
            "2.399630069732666\n",
            "2.487029790878296\n",
            "2.5015902519226074\n",
            "2.5138680934906006\n",
            "2.455536365509033\n",
            "2.518906593322754\n",
            "2.5131540298461914\n",
            "2.5609941482543945\n",
            "2.463794708251953\n",
            "2.5161354541778564\n",
            "2.477600336074829\n",
            "2.4688031673431396\n",
            "2.5100886821746826\n",
            "2.509805679321289\n",
            "2.4567558765411377\n",
            "2.4692790508270264\n",
            "2.389341115951538\n",
            "2.4323959350585938\n",
            "2.4522433280944824\n",
            "2.5792429447174072\n",
            "2.4194016456604004\n",
            "2.4522087574005127\n",
            "2.559234142303467\n",
            "2.364935874938965\n",
            "2.4539096355438232\n",
            "2.440439224243164\n",
            "2.4590163230895996\n",
            "2.5152833461761475\n",
            "2.4857072830200195\n",
            "2.380180835723877\n",
            "2.458615779876709\n",
            "2.4175078868865967\n",
            "2.456822633743286\n",
            "2.448390483856201\n",
            "2.3569564819335938\n",
            "2.380821943283081\n",
            "2.529942035675049\n",
            "2.3988466262817383\n",
            "2.4766716957092285\n",
            "2.4673943519592285\n",
            "2.522279739379883\n",
            "2.410447597503662\n",
            "2.543735980987549\n",
            "2.3420462608337402\n",
            "2.591053009033203\n",
            "2.55475115776062\n",
            "2.4795339107513428\n",
            "2.5283195972442627\n",
            "2.550832986831665\n",
            "2.4896562099456787\n",
            "2.3239569664001465\n",
            "2.376354217529297\n",
            "2.5885868072509766\n",
            "2.5061283111572266\n",
            "2.413344383239746\n",
            "2.519765615463257\n",
            "2.518503427505493\n",
            "2.3849434852600098\n",
            "2.419020652770996\n",
            "2.467806100845337\n",
            "2.389540910720825\n",
            "2.500364065170288\n",
            "2.428438186645508\n",
            "2.5055313110351562\n",
            "2.4169087409973145\n",
            "2.391461133956909\n",
            "2.4009382724761963\n",
            "2.413073778152466\n",
            "2.397400379180908\n",
            "2.472666025161743\n",
            "2.3573734760284424\n",
            "2.522932529449463\n",
            "2.404033660888672\n",
            "2.6820669174194336\n",
            "2.449089765548706\n",
            "2.4317493438720703\n",
            "2.402533769607544\n",
            "2.4709904193878174\n",
            "2.514885425567627\n",
            "2.4181673526763916\n",
            "2.427065849304199\n",
            "2.460286855697632\n",
            "2.4371204376220703\n",
            "2.4141764640808105\n",
            "2.3633570671081543\n",
            "2.4399847984313965\n",
            "2.32429838180542\n",
            "2.4517600536346436\n",
            "2.5274250507354736\n",
            "2.46688175201416\n",
            "2.4447827339172363\n",
            "2.4904298782348633\n",
            "2.368809223175049\n",
            "2.406907796859741\n",
            "2.419931173324585\n",
            "2.585021495819092\n",
            "2.5306549072265625\n",
            "2.447218179702759\n",
            "2.5408596992492676\n",
            "2.460853338241577\n",
            "2.514502763748169\n",
            "2.3711130619049072\n",
            "2.5237865447998047\n",
            "2.383671760559082\n",
            "2.4985427856445312\n",
            "2.5477797985076904\n",
            "2.5212085247039795\n",
            "2.433361530303955\n",
            "2.3460474014282227\n",
            "2.325101613998413\n",
            "2.469996690750122\n",
            "2.458484649658203\n",
            "2.5144948959350586\n",
            "2.377312183380127\n",
            "2.48943829536438\n",
            "2.5140464305877686\n",
            "2.4927470684051514\n",
            "2.5581305027008057\n",
            "2.3808157444000244\n",
            "2.367366313934326\n",
            "2.4599053859710693\n",
            "2.5116331577301025\n",
            "2.6518518924713135\n",
            "2.2993240356445312\n",
            "2.418165445327759\n",
            "2.5052947998046875\n",
            "2.4259135723114014\n",
            "2.4415059089660645\n",
            "2.458538770675659\n",
            "2.42183780670166\n",
            "2.4647176265716553\n",
            "2.427938222885132\n",
            "2.445416212081909\n",
            "2.550184488296509\n",
            "2.3987364768981934\n",
            "2.5354816913604736\n",
            "2.370814323425293\n",
            "2.4139466285705566\n",
            "2.502603530883789\n",
            "2.398607015609741\n",
            "2.518040180206299\n",
            "2.4452598094940186\n",
            "2.3321800231933594\n",
            "2.449760675430298\n",
            "2.474499464035034\n",
            "2.4324698448181152\n",
            "2.5201563835144043\n",
            "2.3126320838928223\n",
            "2.461031675338745\n",
            "2.4278223514556885\n",
            "2.505113363265991\n",
            "2.3772292137145996\n",
            "2.543208360671997\n",
            "2.3372080326080322\n",
            "2.4760899543762207\n",
            "2.407893419265747\n",
            "2.39473819732666\n",
            "2.3651010990142822\n",
            "2.497382879257202\n",
            "2.4544286727905273\n",
            "2.5238335132598877\n",
            "2.528482675552368\n",
            "2.4034042358398438\n",
            "2.5327534675598145\n",
            "2.546178102493286\n",
            "2.577112913131714\n",
            "2.6296772956848145\n",
            "2.4699673652648926\n",
            "2.519913673400879\n",
            "2.4994587898254395\n",
            "2.4870800971984863\n",
            "2.495187997817993\n",
            "2.4166743755340576\n",
            "2.491936206817627\n",
            "2.4110755920410156\n",
            "2.5346007347106934\n",
            "2.3992128372192383\n",
            "2.46589732170105\n",
            "2.57261061668396\n",
            "2.4478535652160645\n",
            "2.447545289993286\n",
            "2.4380440711975098\n",
            "2.365189552307129\n",
            "2.4588165283203125\n",
            "2.4335954189300537\n",
            "2.374403715133667\n",
            "2.483375072479248\n",
            "2.3183465003967285\n",
            "2.5532987117767334\n",
            "2.4343276023864746\n",
            "2.4712555408477783\n",
            "2.5283284187316895\n",
            "2.420769691467285\n",
            "2.5193240642547607\n",
            "2.399294853210449\n",
            "2.4751880168914795\n",
            "2.4368629455566406\n",
            "2.4842848777770996\n",
            "2.466231346130371\n",
            "2.395235538482666\n",
            "2.3030874729156494\n",
            "2.471275568008423\n",
            "2.4275646209716797\n",
            "2.464704751968384\n",
            "2.3555164337158203\n",
            "2.3488173484802246\n",
            "2.4767096042633057\n",
            "2.4076902866363525\n",
            "2.3819706439971924\n",
            "2.487004041671753\n",
            "2.465378522872925\n",
            "2.5436649322509766\n",
            "2.3872692584991455\n",
            "2.4799163341522217\n",
            "2.410404682159424\n",
            "2.361978054046631\n",
            "2.302597999572754\n",
            "2.4873597621917725\n",
            "2.4766430854797363\n",
            "2.519595146179199\n",
            "2.418212413787842\n",
            "2.5202395915985107\n",
            "2.4075982570648193\n",
            "2.419424295425415\n",
            "2.392775058746338\n",
            "2.704925060272217\n",
            "2.4543519020080566\n",
            "2.428893566131592\n",
            "2.445434808731079\n",
            "2.3322913646698\n",
            "2.4504168033599854\n",
            "2.5244834423065186\n",
            "2.6213009357452393\n",
            "2.437445640563965\n",
            "2.471198081970215\n",
            "2.4180421829223633\n",
            "2.408107042312622\n",
            "2.489008903503418\n",
            "2.581115484237671\n",
            "2.4110898971557617\n",
            "2.466582775115967\n",
            "2.391094923019409\n",
            "2.4652392864227295\n",
            "2.5153636932373047\n",
            "2.526071071624756\n",
            "2.3966190814971924\n",
            "2.4791717529296875\n",
            "2.4409477710723877\n",
            "2.4230141639709473\n",
            "2.451853036880493\n",
            "2.270918607711792\n",
            "2.4776203632354736\n",
            "2.4762020111083984\n",
            "2.494438409805298\n",
            "2.5605695247650146\n",
            "2.4659478664398193\n",
            "2.5097711086273193\n",
            "2.416170835494995\n",
            "2.4038586616516113\n",
            "2.4400289058685303\n",
            "2.5586495399475098\n",
            "2.4139389991760254\n",
            "2.544463872909546\n",
            "2.4706830978393555\n",
            "2.385528087615967\n",
            "2.4883604049682617\n",
            "2.5083200931549072\n",
            "2.4109067916870117\n",
            "2.492825746536255\n",
            "2.4620678424835205\n",
            "2.399198293685913\n",
            "2.4686434268951416\n",
            "2.444732904434204\n",
            "2.453866481781006\n",
            "2.4631094932556152\n",
            "2.4714512825012207\n",
            "2.399247407913208\n",
            "2.488765239715576\n",
            "2.4317269325256348\n",
            "2.5317189693450928\n",
            "2.4731876850128174\n",
            "2.3718678951263428\n",
            "2.453857421875\n",
            "2.402190685272217\n",
            "2.489870309829712\n",
            "2.4838707447052\n",
            "2.5031073093414307\n",
            "2.6261889934539795\n",
            "2.3791816234588623\n",
            "2.4544663429260254\n",
            "2.4309589862823486\n",
            "2.4945428371429443\n",
            "2.4724769592285156\n",
            "2.45632004737854\n",
            "2.453000545501709\n",
            "2.4422202110290527\n",
            "2.440871000289917\n",
            "2.4707045555114746\n",
            "2.4587879180908203\n",
            "2.467707395553589\n",
            "2.501290798187256\n",
            "2.4960618019104004\n",
            "2.4214439392089844\n",
            "2.4824347496032715\n",
            "2.5023622512817383\n",
            "2.29516863822937\n",
            "2.400435209274292\n",
            "2.3222463130950928\n",
            "2.5523581504821777\n",
            "2.417759656906128\n",
            "2.4843833446502686\n",
            "2.3139255046844482\n",
            "2.436728000640869\n",
            "2.4275753498077393\n",
            "2.6234779357910156\n",
            "2.4635860919952393\n",
            "2.395707607269287\n",
            "2.4614100456237793\n",
            "2.5081660747528076\n",
            "2.438617467880249\n",
            "2.5023460388183594\n",
            "2.4336235523223877\n",
            "2.4710419178009033\n",
            "2.485806703567505\n",
            "2.4231200218200684\n",
            "2.5064148902893066\n",
            "2.4154064655303955\n",
            "2.43300724029541\n",
            "2.398693799972534\n",
            "2.393754482269287\n",
            "2.4668617248535156\n",
            "2.636289596557617\n",
            "2.477201223373413\n",
            "2.486374616622925\n",
            "2.2961008548736572\n",
            "2.421025037765503\n",
            "2.504164934158325\n",
            "2.4114489555358887\n",
            "2.385185480117798\n",
            "2.393480062484741\n",
            "2.4324347972869873\n",
            "2.5216901302337646\n",
            "2.556488037109375\n",
            "2.50105881690979\n",
            "2.441340208053589\n",
            "2.423975706100464\n",
            "2.4978039264678955\n",
            "2.5790493488311768\n",
            "2.5381901264190674\n",
            "2.4604973793029785\n",
            "2.4047086238861084\n",
            "2.5195770263671875\n",
            "2.471719741821289\n",
            "2.4850542545318604\n",
            "2.5003297328948975\n",
            "2.4378068447113037\n",
            "2.3705332279205322\n",
            "2.4896981716156006\n",
            "2.401710271835327\n",
            "2.5283849239349365\n",
            "2.359483242034912\n",
            "2.5759150981903076\n",
            "2.453441858291626\n",
            "2.6140871047973633\n",
            "2.5654783248901367\n",
            "2.4991674423217773\n",
            "2.4679908752441406\n",
            "2.526573419570923\n",
            "2.5533969402313232\n",
            "2.4570271968841553\n",
            "2.408813714981079\n",
            "2.388472080230713\n",
            "2.487169027328491\n",
            "2.4096055030822754\n",
            "2.3770217895507812\n",
            "2.4460558891296387\n",
            "2.3963234424591064\n",
            "2.390474319458008\n",
            "2.445713520050049\n",
            "2.4514107704162598\n",
            "2.560215711593628\n",
            "2.4279680252075195\n",
            "2.47894287109375\n",
            "2.5414023399353027\n",
            "2.2935853004455566\n",
            "2.545452833175659\n",
            "2.430898427963257\n",
            "2.374917984008789\n",
            "2.498833179473877\n",
            "2.4965364933013916\n",
            "2.374258041381836\n",
            "2.384911060333252\n",
            "2.4079267978668213\n",
            "2.3723247051239014\n",
            "2.5694878101348877\n",
            "2.4293510913848877\n",
            "2.4468960762023926\n",
            "2.526498317718506\n",
            "2.4866714477539062\n",
            "2.594566822052002\n",
            "2.401808023452759\n",
            "2.5214622020721436\n",
            "2.4899487495422363\n",
            "2.4139761924743652\n",
            "2.6179518699645996\n",
            "2.586395502090454\n",
            "2.461322546005249\n",
            "2.463703155517578\n",
            "2.4159774780273438\n",
            "2.4904370307922363\n",
            "2.3089869022369385\n",
            "2.422543525695801\n",
            "2.4130642414093018\n",
            "2.366004228591919\n",
            "2.4304726123809814\n",
            "2.611788511276245\n",
            "2.4267642498016357\n",
            "2.431907892227173\n",
            "2.4049339294433594\n",
            "2.5672786235809326\n",
            "2.3560986518859863\n",
            "2.440620183944702\n",
            "2.3852014541625977\n",
            "2.3165249824523926\n",
            "2.535280704498291\n",
            "2.3589746952056885\n",
            "2.4756276607513428\n",
            "2.3540773391723633\n",
            "2.574817657470703\n",
            "2.535447835922241\n",
            "2.499674081802368\n",
            "2.321875810623169\n",
            "2.572336435317993\n",
            "2.342766284942627\n",
            "2.418159246444702\n",
            "2.409275770187378\n",
            "2.4740350246429443\n",
            "2.5454518795013428\n",
            "2.470733880996704\n",
            "2.382715940475464\n",
            "2.4393270015716553\n",
            "2.4998726844787598\n",
            "2.3935816287994385\n",
            "2.46221661567688\n",
            "2.320610284805298\n",
            "2.5737037658691406\n",
            "2.5552544593811035\n",
            "2.424147367477417\n",
            "2.487971782684326\n",
            "2.4454495906829834\n",
            "2.530109405517578\n",
            "2.4397871494293213\n",
            "2.4325389862060547\n",
            "2.3867669105529785\n",
            "2.381378650665283\n",
            "2.5475218296051025\n",
            "2.451742649078369\n",
            "2.4237756729125977\n",
            "2.515995502471924\n",
            "2.561131477355957\n",
            "2.4093101024627686\n",
            "2.3403115272521973\n",
            "2.4761528968811035\n",
            "2.5448570251464844\n",
            "2.5867903232574463\n",
            "2.3616652488708496\n",
            "2.419851541519165\n",
            "2.4192159175872803\n",
            "2.52156925201416\n",
            "2.4038290977478027\n",
            "2.340440511703491\n",
            "2.467535972595215\n",
            "2.543254852294922\n",
            "2.453765630722046\n",
            "2.539364814758301\n",
            "2.5440495014190674\n",
            "2.488769054412842\n",
            "2.3590240478515625\n",
            "2.4044759273529053\n",
            "2.400470733642578\n",
            "2.458833694458008\n",
            "2.420776605606079\n",
            "2.51596736907959\n",
            "2.359832525253296\n",
            "2.5737757682800293\n",
            "2.590848684310913\n",
            "2.4986889362335205\n",
            "2.43418025970459\n",
            "2.398486852645874\n",
            "2.559600830078125\n",
            "2.512802839279175\n",
            "2.44160795211792\n",
            "2.4592506885528564\n",
            "2.38459849357605\n",
            "2.4604063034057617\n",
            "2.491969108581543\n",
            "2.4621951580047607\n",
            "2.4035325050354004\n",
            "2.510477304458618\n",
            "2.466539144515991\n",
            "2.3063230514526367\n",
            "2.411736488342285\n",
            "2.5412046909332275\n",
            "2.4692349433898926\n",
            "2.4911391735076904\n",
            "2.5347886085510254\n",
            "2.3517181873321533\n",
            "2.3223440647125244\n",
            "2.4139063358306885\n",
            "2.389198064804077\n",
            "2.385190010070801\n",
            "2.4382212162017822\n",
            "2.5609049797058105\n",
            "2.4397008419036865\n",
            "2.449099063873291\n",
            "2.3984811305999756\n",
            "2.4582619667053223\n",
            "2.5304219722747803\n",
            "2.535689115524292\n",
            "2.3980765342712402\n",
            "2.57883882522583\n",
            "2.4512057304382324\n",
            "2.3464479446411133\n",
            "2.384422779083252\n",
            "2.549671173095703\n",
            "2.4902749061584473\n",
            "2.5256714820861816\n",
            "2.4879841804504395\n",
            "2.4829647541046143\n",
            "2.545064687728882\n",
            "2.3212966918945312\n",
            "2.418708324432373\n",
            "2.477320671081543\n",
            "2.4198927879333496\n",
            "2.418567419052124\n",
            "2.3893532752990723\n",
            "2.4995827674865723\n",
            "2.4365735054016113\n",
            "2.3571267127990723\n",
            "2.4743313789367676\n",
            "2.4148683547973633\n",
            "2.555152177810669\n",
            "2.66428279876709\n",
            "2.3724894523620605\n",
            "2.5864815711975098\n",
            "2.4400010108947754\n",
            "2.5064358711242676\n",
            "2.3762927055358887\n",
            "2.40628719329834\n",
            "2.463426351547241\n",
            "2.501441478729248\n",
            "2.5053460597991943\n",
            "2.5560407638549805\n",
            "2.270960569381714\n",
            "2.4415504932403564\n",
            "2.5384182929992676\n",
            "2.4460744857788086\n",
            "2.410205364227295\n",
            "2.396390676498413\n",
            "2.363849401473999\n",
            "2.4665849208831787\n",
            "2.4320900440216064\n",
            "2.5267584323883057\n",
            "2.3074562549591064\n",
            "2.5514016151428223\n",
            "2.37746000289917\n",
            "2.5032830238342285\n",
            "2.4567394256591797\n",
            "2.3847298622131348\n",
            "2.435080051422119\n",
            "2.4059736728668213\n",
            "2.4590160846710205\n",
            "2.368347644805908\n",
            "2.4685020446777344\n",
            "2.4144299030303955\n",
            "2.4325692653656006\n",
            "2.419126033782959\n",
            "2.495664596557617\n",
            "2.439197063446045\n",
            "2.499180555343628\n",
            "2.4636571407318115\n",
            "2.4817304611206055\n",
            "2.3841605186462402\n",
            "2.4664692878723145\n",
            "2.4455747604370117\n",
            "2.433511972427368\n",
            "2.392331600189209\n",
            "2.396135091781616\n",
            "2.4840259552001953\n",
            "2.329381227493286\n",
            "2.389448881149292\n",
            "2.417297124862671\n",
            "2.4523165225982666\n",
            "2.449361801147461\n",
            "2.409640312194824\n",
            "2.5393059253692627\n",
            "2.3995277881622314\n",
            "2.441513776779175\n",
            "2.446387767791748\n",
            "2.3833911418914795\n",
            "2.3025217056274414\n",
            "2.3676764965057373\n",
            "2.4510841369628906\n",
            "2.3524582386016846\n",
            "2.322711229324341\n",
            "2.4712181091308594\n",
            "2.3448894023895264\n",
            "2.5174193382263184\n",
            "2.5155203342437744\n",
            "2.400878667831421\n",
            "2.414238452911377\n",
            "2.547271728515625\n",
            "2.379319667816162\n",
            "2.2838025093078613\n",
            "2.3214828968048096\n",
            "2.405466318130493\n",
            "2.6426823139190674\n",
            "2.5295958518981934\n",
            "2.4394140243530273\n",
            "2.6284916400909424\n",
            "2.499267101287842\n",
            "2.489306926727295\n",
            "2.3952836990356445\n",
            "2.388350009918213\n",
            "2.423790693283081\n",
            "2.5319974422454834\n",
            "2.378645420074463\n",
            "2.45332670211792\n",
            "2.4344983100891113\n",
            "2.45493745803833\n",
            "2.5992510318756104\n",
            "2.5707154273986816\n",
            "2.4190216064453125\n",
            "2.4814772605895996\n",
            "2.4576284885406494\n",
            "2.516223430633545\n",
            "2.394211530685425\n",
            "2.6307084560394287\n",
            "2.29103946685791\n",
            "2.3469676971435547\n",
            "2.495042324066162\n",
            "2.382398843765259\n",
            "2.3992390632629395\n",
            "2.475492238998413\n",
            "2.3913822174072266\n",
            "2.331310987472534\n",
            "2.3919577598571777\n",
            "2.485379219055176\n",
            "2.5413904190063477\n",
            "2.4563326835632324\n",
            "2.476557731628418\n",
            "2.4679460525512695\n",
            "2.5607337951660156\n",
            "2.3656673431396484\n",
            "2.5468692779541016\n",
            "2.330862283706665\n",
            "2.399771213531494\n",
            "2.4636127948760986\n",
            "2.516089916229248\n",
            "2.4775688648223877\n",
            "2.581454277038574\n",
            "2.430454969406128\n",
            "2.421790838241577\n",
            "2.3868179321289062\n",
            "2.5094380378723145\n",
            "2.433567523956299\n",
            "2.468167304992676\n",
            "2.5730457305908203\n",
            "2.4011964797973633\n",
            "2.3751182556152344\n",
            "2.444551944732666\n",
            "2.408339262008667\n",
            "2.369771718978882\n",
            "2.4681971073150635\n",
            "2.4536728858947754\n",
            "2.408010721206665\n",
            "2.351954936981201\n",
            "2.368290901184082\n",
            "2.4470772743225098\n",
            "2.5529680252075195\n",
            "2.40146803855896\n",
            "2.3917174339294434\n",
            "2.4520137310028076\n",
            "2.359729528427124\n",
            "2.4280619621276855\n",
            "2.4674861431121826\n",
            "2.4281179904937744\n",
            "2.5040547847747803\n",
            "2.4306397438049316\n",
            "2.457217216491699\n",
            "2.448265314102173\n",
            "2.4219117164611816\n",
            "2.358362913131714\n",
            "2.371412515640259\n",
            "2.440969705581665\n",
            "2.4882891178131104\n",
            "2.5351192951202393\n",
            "2.4704842567443848\n",
            "2.5329749584198\n",
            "2.4649300575256348\n",
            "2.5949556827545166\n",
            "2.4976091384887695\n",
            "2.5698654651641846\n",
            "2.4116268157958984\n",
            "2.37290620803833\n",
            "2.5494701862335205\n",
            "2.3668079376220703\n",
            "2.5411856174468994\n",
            "2.4871022701263428\n",
            "2.4057955741882324\n",
            "2.383880853652954\n",
            "2.4293782711029053\n",
            "2.434809446334839\n",
            "2.401834487915039\n",
            "2.6531970500946045\n",
            "2.3977417945861816\n",
            "2.4466326236724854\n",
            "2.430885076522827\n",
            "2.365668773651123\n",
            "2.5510237216949463\n",
            "2.567962169647217\n",
            "2.4493627548217773\n",
            "2.394165515899658\n",
            "2.4183130264282227\n",
            "2.43210506439209\n",
            "2.570136308670044\n",
            "2.5583608150482178\n",
            "2.4264354705810547\n",
            "2.3938474655151367\n",
            "2.3922290802001953\n",
            "2.4654576778411865\n",
            "2.441540002822876\n",
            "2.4536380767822266\n",
            "2.449167251586914\n",
            "2.6129443645477295\n",
            "2.344444990158081\n",
            "2.447098970413208\n",
            "2.4661362171173096\n",
            "2.5789082050323486\n",
            "2.370295286178589\n",
            "2.5402047634124756\n",
            "2.515838861465454\n",
            "2.336679220199585\n",
            "2.428668260574341\n",
            "2.4168553352355957\n",
            "2.485201835632324\n",
            "2.3873181343078613\n",
            "2.544671058654785\n",
            "2.4306118488311768\n",
            "2.6482596397399902\n",
            "2.4922091960906982\n",
            "2.310962200164795\n",
            "2.3837199211120605\n",
            "2.4288649559020996\n",
            "2.4940052032470703\n",
            "2.405656099319458\n",
            "2.321457624435425\n",
            "2.4644339084625244\n",
            "2.510664463043213\n",
            "2.587219476699829\n",
            "2.520371675491333\n",
            "2.4638895988464355\n",
            "2.394252061843872\n",
            "2.4306046962738037\n",
            "2.5686535835266113\n",
            "2.4868831634521484\n",
            "2.3681185245513916\n",
            "2.389409065246582\n",
            "2.531583547592163\n",
            "2.3594040870666504\n",
            "2.5027408599853516\n",
            "2.532299757003784\n",
            "2.5653178691864014\n",
            "2.3736953735351562\n",
            "2.4376561641693115\n",
            "2.3538296222686768\n",
            "2.451719045639038\n",
            "2.446706533432007\n",
            "2.4438717365264893\n",
            "2.4460997581481934\n",
            "2.544523239135742\n",
            "2.454068422317505\n",
            "2.496899127960205\n",
            "2.5216751098632812\n",
            "2.436736583709717\n",
            "2.5657360553741455\n",
            "2.34798002243042\n",
            "2.4503846168518066\n",
            "2.407663106918335\n",
            "2.515902519226074\n",
            "2.4949872493743896\n",
            "2.502870559692383\n",
            "2.5458855628967285\n",
            "2.5711846351623535\n",
            "2.6056761741638184\n",
            "2.4003005027770996\n",
            "2.4694066047668457\n",
            "2.413389205932617\n",
            "2.5357537269592285\n",
            "2.474118947982788\n",
            "2.387983560562134\n",
            "2.496828079223633\n",
            "2.586031675338745\n",
            "2.4492430686950684\n",
            "2.48724102973938\n",
            "2.38232684135437\n",
            "2.442171812057495\n",
            "2.544600248336792\n",
            "2.464825391769409\n",
            "2.466675043106079\n",
            "2.330343008041382\n",
            "2.3619892597198486\n",
            "2.466158628463745\n",
            "2.5119476318359375\n",
            "2.3161256313323975\n",
            "2.428757429122925\n",
            "2.423689365386963\n",
            "2.4525318145751953\n",
            "2.3528265953063965\n",
            "2.4269087314605713\n",
            "2.4133641719818115\n",
            "2.36822247505188\n",
            "2.4392967224121094\n",
            "2.5220415592193604\n",
            "2.4242353439331055\n",
            "2.4518344402313232\n",
            "2.443932294845581\n",
            "2.555403709411621\n",
            "2.5034666061401367\n",
            "2.442174196243286\n",
            "2.4450161457061768\n",
            "2.5161757469177246\n",
            "2.5691275596618652\n",
            "2.4952902793884277\n",
            "2.4377448558807373\n",
            "2.5126638412475586\n",
            "2.4462926387786865\n",
            "2.4417905807495117\n",
            "2.4835643768310547\n",
            "2.3866028785705566\n",
            "2.5276198387145996\n",
            "2.5643343925476074\n",
            "2.4958643913269043\n",
            "2.4195258617401123\n",
            "2.416438579559326\n",
            "2.504578113555908\n",
            "2.4888272285461426\n",
            "2.5430498123168945\n",
            "2.4613890647888184\n",
            "2.597066879272461\n",
            "2.514282703399658\n",
            "2.370820999145508\n",
            "2.4476006031036377\n",
            "2.4669761657714844\n",
            "2.536229133605957\n",
            "2.436415910720825\n",
            "2.4472222328186035\n",
            "2.4090352058410645\n",
            "2.4689252376556396\n",
            "2.3924927711486816\n",
            "2.3889968395233154\n",
            "2.3738245964050293\n",
            "2.420593738555908\n",
            "2.34486985206604\n",
            "2.423898696899414\n",
            "2.5460546016693115\n",
            "2.4945428371429443\n",
            "2.4795424938201904\n",
            "2.4050796031951904\n",
            "2.3907978534698486\n",
            "2.427189826965332\n",
            "2.540804624557495\n",
            "2.360172986984253\n",
            "2.5434391498565674\n",
            "2.4381675720214844\n",
            "2.4669952392578125\n",
            "2.449939489364624\n",
            "2.4048540592193604\n",
            "2.5389902591705322\n",
            "2.4716360569000244\n",
            "2.334872245788574\n",
            "2.468883991241455\n",
            "2.4548702239990234\n",
            "2.466909170150757\n",
            "2.5467188358306885\n",
            "2.4684431552886963\n",
            "2.3833303451538086\n",
            "2.4436700344085693\n",
            "2.569086790084839\n",
            "2.437183141708374\n",
            "2.4537768363952637\n",
            "2.3621907234191895\n",
            "2.344858407974243\n",
            "2.3997714519500732\n",
            "2.3973100185394287\n",
            "2.625792980194092\n",
            "2.43890643119812\n",
            "2.575106620788574\n",
            "2.4423182010650635\n",
            "2.4147603511810303\n",
            "2.476576328277588\n",
            "2.481982946395874\n",
            "2.428614854812622\n",
            "2.4426136016845703\n",
            "2.5302975177764893\n",
            "2.3610832691192627\n",
            "2.3258697986602783\n",
            "2.336304187774658\n",
            "2.380387306213379\n",
            "2.509580373764038\n",
            "2.512345314025879\n",
            "2.4774389266967773\n",
            "2.5484001636505127\n",
            "2.5077805519104004\n",
            "2.5066165924072266\n",
            "2.376067876815796\n",
            "2.3891401290893555\n",
            "2.523977041244507\n",
            "2.384122371673584\n",
            "2.491986036300659\n",
            "2.397298812866211\n",
            "2.3886194229125977\n",
            "2.357224941253662\n",
            "2.366286039352417\n",
            "2.4414072036743164\n",
            "2.4428014755249023\n",
            "2.4823873043060303\n",
            "2.37825345993042\n",
            "2.3854854106903076\n",
            "2.3560211658477783\n",
            "2.470796823501587\n",
            "2.3204092979431152\n",
            "2.458916664123535\n",
            "2.5178561210632324\n",
            "2.4384243488311768\n",
            "2.362501859664917\n",
            "2.355717420578003\n",
            "2.377725839614868\n",
            "2.4482274055480957\n",
            "2.3314695358276367\n",
            "2.3238525390625\n",
            "2.4716503620147705\n",
            "2.333616256713867\n",
            "2.603069543838501\n",
            "2.4688057899475098\n",
            "2.503608465194702\n",
            "2.4195594787597656\n",
            "2.451511859893799\n",
            "2.301011323928833\n",
            "2.370643377304077\n",
            "2.3506901264190674\n",
            "2.2910971641540527\n",
            "2.4514665603637695\n",
            "2.4463694095611572\n",
            "2.2976157665252686\n",
            "2.3966403007507324\n",
            "2.6097750663757324\n",
            "2.561027765274048\n",
            "2.3719630241394043\n",
            "2.4734745025634766\n",
            "2.4591429233551025\n",
            "2.4749186038970947\n",
            "2.5511529445648193\n",
            "2.5358901023864746\n",
            "2.557873487472534\n",
            "2.561556339263916\n",
            "2.3695454597473145\n",
            "2.4129416942596436\n",
            "2.4264066219329834\n",
            "2.338869094848633\n",
            "2.4490253925323486\n",
            "2.4879496097564697\n",
            "2.4279613494873047\n",
            "2.5723702907562256\n",
            "2.327885389328003\n",
            "2.450615406036377\n",
            "2.454643487930298\n",
            "2.447608232498169\n",
            "2.4544410705566406\n",
            "2.522224187850952\n",
            "2.374225378036499\n",
            "2.4943299293518066\n",
            "2.430513381958008\n",
            "2.474036455154419\n",
            "2.4908030033111572\n",
            "2.3188164234161377\n",
            "2.40059494972229\n",
            "2.5353152751922607\n",
            "2.3764865398406982\n",
            "2.4542839527130127\n",
            "2.6426572799682617\n",
            "2.4251205921173096\n",
            "2.482698917388916\n",
            "2.3181955814361572\n",
            "2.4751455783843994\n",
            "2.3612523078918457\n",
            "2.4134511947631836\n",
            "2.3713037967681885\n",
            "2.489690065383911\n",
            "2.446415662765503\n",
            "2.434044122695923\n",
            "2.3468732833862305\n",
            "2.6285181045532227\n",
            "2.406968593597412\n",
            "2.6310231685638428\n",
            "2.4548470973968506\n",
            "2.4928030967712402\n",
            "2.5119495391845703\n",
            "2.497455596923828\n",
            "2.3907876014709473\n",
            "2.421992063522339\n",
            "2.4398550987243652\n",
            "2.458630084991455\n",
            "2.450481653213501\n",
            "2.365293264389038\n",
            "2.4846928119659424\n",
            "2.3854382038116455\n",
            "2.3860771656036377\n",
            "2.4667131900787354\n",
            "2.5519368648529053\n",
            "2.512038469314575\n",
            "2.331552505493164\n",
            "2.455737352371216\n",
            "2.380016803741455\n",
            "2.5214121341705322\n",
            "2.6048991680145264\n",
            "2.442563056945801\n",
            "2.377636194229126\n",
            "2.4456326961517334\n",
            "2.293217897415161\n",
            "2.4233832359313965\n",
            "2.458674430847168\n",
            "2.4682867527008057\n",
            "2.381531000137329\n",
            "2.34124493598938\n",
            "2.3526086807250977\n",
            "2.513951301574707\n",
            "2.5239486694335938\n",
            "2.3720574378967285\n",
            "2.4203040599823\n",
            "2.4281628131866455\n",
            "2.391328811645508\n",
            "2.4050328731536865\n",
            "2.47347092628479\n",
            "2.459554433822632\n",
            "2.363950729370117\n",
            "2.4724490642547607\n",
            "2.5894229412078857\n",
            "2.4441733360290527\n",
            "2.495058536529541\n",
            "2.5327908992767334\n",
            "2.4076590538024902\n",
            "2.6317903995513916\n",
            "2.3614695072174072\n",
            "2.5937347412109375\n",
            "2.385176181793213\n",
            "2.306774139404297\n",
            "2.404360055923462\n",
            "2.5820631980895996\n",
            "2.373082160949707\n",
            "2.4634344577789307\n",
            "2.4568841457366943\n",
            "2.4827916622161865\n",
            "2.400686740875244\n",
            "2.4453623294830322\n",
            "2.4610185623168945\n",
            "2.50966215133667\n",
            "2.392191171646118\n",
            "2.446939468383789\n",
            "2.4673590660095215\n",
            "2.3652596473693848\n",
            "2.4576892852783203\n",
            "2.39103627204895\n",
            "2.47686505317688\n",
            "2.5777101516723633\n",
            "2.495176076889038\n",
            "2.4788219928741455\n",
            "2.435994863510132\n",
            "2.5276708602905273\n",
            "2.465764045715332\n",
            "2.5580310821533203\n",
            "2.3529300689697266\n",
            "2.3467154502868652\n",
            "2.4337100982666016\n",
            "2.391265392303467\n",
            "2.3992393016815186\n",
            "2.4601528644561768\n",
            "2.3790409564971924\n",
            "2.3589365482330322\n",
            "2.4818108081817627\n",
            "2.555117130279541\n",
            "2.4143049716949463\n",
            "2.4626851081848145\n",
            "2.3295481204986572\n",
            "2.3371803760528564\n",
            "2.307809591293335\n",
            "2.3983333110809326\n",
            "2.3552379608154297\n",
            "2.518766164779663\n",
            "2.556757688522339\n",
            "2.418142795562744\n",
            "2.371347427368164\n",
            "2.514746904373169\n",
            "2.484560489654541\n",
            "2.4687085151672363\n",
            "2.577967643737793\n",
            "2.4559221267700195\n",
            "2.414381265640259\n",
            "2.4469001293182373\n",
            "2.4459948539733887\n",
            "2.3702099323272705\n",
            "2.4982833862304688\n",
            "2.374617338180542\n",
            "2.5500025749206543\n",
            "2.4542629718780518\n",
            "2.498849391937256\n",
            "2.4334733486175537\n",
            "2.4131059646606445\n",
            "2.409557342529297\n",
            "2.4697959423065186\n",
            "2.4282419681549072\n",
            "2.462897300720215\n",
            "2.39625883102417\n",
            "2.417360544204712\n",
            "2.437784433364868\n",
            "2.5443527698516846\n",
            "2.4937098026275635\n",
            "2.3508946895599365\n",
            "2.4510743618011475\n",
            "2.3612496852874756\n",
            "2.436924934387207\n",
            "2.396684408187866\n",
            "2.479956865310669\n",
            "2.500422954559326\n",
            "2.4539268016815186\n",
            "2.4490463733673096\n",
            "2.5162417888641357\n",
            "2.418282985687256\n",
            "2.604398012161255\n",
            "2.43754243850708\n",
            "2.4842722415924072\n",
            "2.4271769523620605\n",
            "2.352480888366699\n",
            "2.498350143432617\n",
            "2.485321521759033\n",
            "2.4359047412872314\n",
            "2.506892442703247\n",
            "2.4441428184509277\n",
            "2.440875768661499\n",
            "2.4373152256011963\n",
            "2.4940006732940674\n",
            "2.256530523300171\n",
            "2.445279359817505\n",
            "2.430042266845703\n",
            "2.313512086868286\n",
            "2.464268684387207\n",
            "2.468193292617798\n",
            "2.437779426574707\n",
            "2.431095838546753\n",
            "2.4488751888275146\n",
            "2.527153491973877\n",
            "2.4170968532562256\n",
            "2.467651605606079\n",
            "2.3785181045532227\n",
            "2.415762186050415\n",
            "2.3540196418762207\n",
            "2.5537428855895996\n",
            "2.309782028198242\n",
            "2.472929000854492\n",
            "2.4627485275268555\n",
            "2.4665651321411133\n",
            "2.4635541439056396\n",
            "2.455026388168335\n",
            "2.4565813541412354\n",
            "2.532222270965576\n",
            "2.336987018585205\n",
            "2.3938589096069336\n",
            "2.4570600986480713\n",
            "2.5282208919525146\n",
            "2.285090446472168\n",
            "2.364443063735962\n",
            "2.484816789627075\n",
            "2.438551902770996\n",
            "2.4862406253814697\n",
            "2.432063579559326\n",
            "2.5141406059265137\n",
            "2.4448180198669434\n",
            "2.395648956298828\n",
            "2.449594497680664\n",
            "2.457719326019287\n",
            "2.4536547660827637\n",
            "2.368025541305542\n",
            "2.3766121864318848\n",
            "2.377148389816284\n",
            "2.4353349208831787\n",
            "2.4825541973114014\n",
            "2.4408953189849854\n",
            "2.3751380443573\n",
            "2.5241551399230957\n",
            "2.3859379291534424\n",
            "2.4328625202178955\n",
            "2.455510377883911\n",
            "2.5007388591766357\n",
            "2.39534330368042\n",
            "2.413571357727051\n",
            "2.4854626655578613\n",
            "2.4168643951416016\n",
            "2.550640821456909\n",
            "2.4723923206329346\n",
            "2.4784369468688965\n",
            "2.4092211723327637\n",
            "2.5878827571868896\n",
            "2.428420066833496\n",
            "2.471879005432129\n",
            "2.448206901550293\n",
            "2.5472588539123535\n",
            "2.438977003097534\n",
            "2.365975856781006\n",
            "2.4288063049316406\n",
            "2.390988349914551\n",
            "2.4774160385131836\n",
            "2.509977340698242\n",
            "2.2834017276763916\n",
            "2.431837797164917\n",
            "2.4954001903533936\n",
            "2.5229241847991943\n",
            "2.389251232147217\n",
            "2.4465653896331787\n",
            "2.4507734775543213\n",
            "2.4102210998535156\n",
            "2.5030126571655273\n",
            "2.5445139408111572\n",
            "2.477651596069336\n",
            "2.347362518310547\n",
            "2.443498373031616\n",
            "2.5078368186950684\n",
            "2.3897252082824707\n",
            "2.4628255367279053\n",
            "2.5265402793884277\n",
            "2.4579570293426514\n",
            "2.3618061542510986\n",
            "2.430194139480591\n",
            "2.3832690715789795\n",
            "2.348062038421631\n",
            "2.4634339809417725\n",
            "2.483124256134033\n",
            "2.4461872577667236\n",
            "2.411562919616699\n",
            "2.459174633026123\n",
            "2.340322494506836\n",
            "2.612638473510742\n",
            "2.4937360286712646\n",
            "2.4301552772521973\n",
            "2.4458582401275635\n",
            "2.4673361778259277\n",
            "2.444121837615967\n",
            "2.4698290824890137\n",
            "2.4729671478271484\n",
            "2.2795815467834473\n",
            "2.3669378757476807\n",
            "2.5048420429229736\n",
            "2.4406111240386963\n",
            "2.454561948776245\n",
            "2.468092679977417\n",
            "2.4686572551727295\n",
            "2.4757399559020996\n",
            "2.4581384658813477\n",
            "2.4378676414489746\n",
            "2.439545154571533\n",
            "2.4428064823150635\n",
            "2.468240737915039\n",
            "2.5152549743652344\n",
            "2.5091893672943115\n",
            "2.3896472454071045\n",
            "2.4031035900115967\n",
            "2.501168727874756\n",
            "2.5375494956970215\n",
            "2.457932710647583\n",
            "2.4095213413238525\n",
            "2.3492648601531982\n",
            "2.442951202392578\n",
            "2.3218345642089844\n",
            "2.489751100540161\n",
            "2.489016056060791\n",
            "2.414900541305542\n",
            "2.4358930587768555\n",
            "2.5471668243408203\n",
            "2.456847667694092\n",
            "2.5236713886260986\n",
            "2.401646375656128\n",
            "2.3436970710754395\n",
            "2.500525951385498\n",
            "2.5862162113189697\n",
            "2.3606650829315186\n",
            "2.4803473949432373\n",
            "2.413944959640503\n",
            "2.450517177581787\n",
            "2.456315755844116\n",
            "2.431623935699463\n",
            "2.402087450027466\n",
            "2.4958508014678955\n",
            "2.567631959915161\n",
            "2.356116771697998\n",
            "2.4278573989868164\n",
            "2.5058677196502686\n",
            "2.473536252975464\n",
            "2.3621389865875244\n",
            "2.493455648422241\n",
            "2.35740065574646\n",
            "2.480614185333252\n",
            "2.397404432296753\n",
            "2.4374358654022217\n",
            "2.4142467975616455\n",
            "2.459520101547241\n",
            "2.6499905586242676\n",
            "2.4723525047302246\n",
            "2.5118114948272705\n",
            "2.444194793701172\n",
            "2.54328989982605\n",
            "2.6051671504974365\n",
            "2.44352126121521\n",
            "2.3808302879333496\n",
            "2.5339953899383545\n",
            "2.4283335208892822\n",
            "2.579692840576172\n",
            "2.346893310546875\n",
            "2.3600993156433105\n",
            "2.569448232650757\n",
            "2.4253640174865723\n",
            "2.4131650924682617\n",
            "2.4515438079833984\n",
            "2.463078737258911\n",
            "2.37943434715271\n",
            "2.2946488857269287\n",
            "2.379509925842285\n",
            "2.300936222076416\n",
            "2.401503086090088\n",
            "2.5058064460754395\n",
            "2.4331140518188477\n",
            "2.353398323059082\n",
            "2.4425415992736816\n",
            "2.4288806915283203\n",
            "2.4604086875915527\n",
            "2.349693536758423\n",
            "2.3872244358062744\n",
            "2.442150354385376\n",
            "2.5843160152435303\n",
            "2.530923843383789\n",
            "2.45747971534729\n",
            "2.460984468460083\n",
            "2.447535991668701\n",
            "2.4947280883789062\n",
            "2.5399396419525146\n",
            "2.5774314403533936\n",
            "2.378779411315918\n",
            "2.51244854927063\n",
            "2.4350790977478027\n",
            "2.585679769515991\n",
            "2.5017921924591064\n",
            "2.4891536235809326\n",
            "2.28524112701416\n",
            "2.420055627822876\n",
            "2.4398088455200195\n",
            "2.5363471508026123\n",
            "2.559004783630371\n",
            "2.4531545639038086\n",
            "2.418635606765747\n",
            "2.3716177940368652\n",
            "2.449888229370117\n",
            "2.4968719482421875\n",
            "2.3873867988586426\n",
            "2.5132532119750977\n",
            "2.481602191925049\n",
            "2.3878989219665527\n",
            "2.400596857070923\n",
            "2.476233720779419\n",
            "2.4949469566345215\n",
            "2.5584652423858643\n",
            "2.5194857120513916\n",
            "2.524872303009033\n",
            "2.4473934173583984\n",
            "2.3255953788757324\n",
            "2.4892518520355225\n",
            "2.4718172550201416\n",
            "2.3974125385284424\n",
            "2.4359536170959473\n",
            "2.3901920318603516\n",
            "2.4359512329101562\n",
            "2.4708664417266846\n",
            "2.4134368896484375\n",
            "2.4272258281707764\n",
            "2.532857656478882\n",
            "2.364964485168457\n",
            "2.437915325164795\n",
            "2.4829483032226562\n",
            "2.4237747192382812\n",
            "2.5705502033233643\n",
            "2.4684629440307617\n",
            "2.3503541946411133\n",
            "2.4130196571350098\n",
            "2.430072069168091\n",
            "2.445035934448242\n",
            "2.4458088874816895\n",
            "2.4416725635528564\n",
            "2.447657585144043\n",
            "2.527430772781372\n",
            "2.338407039642334\n",
            "2.575202226638794\n",
            "2.416051149368286\n",
            "2.3761417865753174\n",
            "2.3693087100982666\n",
            "2.592146158218384\n",
            "2.5478451251983643\n",
            "2.5771431922912598\n",
            "2.4660017490386963\n",
            "2.3660898208618164\n",
            "2.44319224357605\n",
            "2.3066413402557373\n",
            "2.3805267810821533\n",
            "2.3917431831359863\n",
            "2.4437098503112793\n",
            "2.347986936569214\n",
            "2.630845308303833\n",
            "2.493976593017578\n",
            "2.467827558517456\n",
            "2.432713747024536\n",
            "2.441816568374634\n",
            "2.372189998626709\n",
            "2.5339553356170654\n",
            "2.4629313945770264\n",
            "2.530082941055298\n",
            "2.458860397338867\n",
            "2.3628334999084473\n",
            "2.391143798828125\n",
            "2.45058012008667\n",
            "2.451894760131836\n",
            "2.5259358882904053\n",
            "2.525604724884033\n",
            "2.482964038848877\n",
            "2.503607749938965\n",
            "2.378549575805664\n",
            "2.359602212905884\n",
            "2.5082130432128906\n",
            "2.414987087249756\n",
            "2.414355516433716\n",
            "2.5396921634674072\n",
            "2.517169713973999\n",
            "2.361253499984741\n",
            "2.5333008766174316\n",
            "2.474423408508301\n",
            "2.4236092567443848\n",
            "2.425126314163208\n",
            "2.5890116691589355\n",
            "2.4922523498535156\n",
            "2.4287867546081543\n",
            "2.4625606536865234\n",
            "2.4289469718933105\n",
            "2.3885061740875244\n",
            "2.3609790802001953\n",
            "2.399900197982788\n",
            "2.3914175033569336\n",
            "2.5149896144866943\n",
            "2.3216607570648193\n",
            "2.480309009552002\n",
            "2.430476665496826\n",
            "2.392158269882202\n",
            "2.431673526763916\n",
            "2.3869822025299072\n",
            "2.4482691287994385\n",
            "2.434567451477051\n",
            "2.4401016235351562\n",
            "2.462859630584717\n",
            "2.573145627975464\n",
            "2.588047504425049\n",
            "2.3154516220092773\n",
            "2.412364959716797\n",
            "2.404888391494751\n",
            "2.436971664428711\n",
            "2.381204605102539\n",
            "2.435457706451416\n",
            "2.3002676963806152\n",
            "2.474574089050293\n",
            "2.476609230041504\n",
            "2.5516908168792725\n",
            "2.3542017936706543\n",
            "2.4302494525909424\n",
            "2.4485514163970947\n",
            "2.3872430324554443\n",
            "2.3607895374298096\n",
            "2.4802777767181396\n",
            "2.324688673019409\n",
            "2.476412057876587\n",
            "2.564042329788208\n",
            "2.408792734146118\n",
            "2.375178813934326\n",
            "2.364499568939209\n",
            "2.5262959003448486\n",
            "2.54510498046875\n",
            "2.444263458251953\n",
            "2.328556776046753\n",
            "2.4921395778656006\n",
            "2.45621919631958\n",
            "2.559931993484497\n",
            "2.44331431388855\n",
            "2.4753167629241943\n",
            "2.4450886249542236\n",
            "2.4597136974334717\n",
            "2.510042190551758\n",
            "2.472618341445923\n",
            "2.496964931488037\n",
            "2.535682201385498\n",
            "2.422537326812744\n",
            "2.2897372245788574\n",
            "2.5020573139190674\n",
            "2.3776910305023193\n",
            "2.5513291358947754\n",
            "2.4467997550964355\n",
            "2.3429524898529053\n",
            "2.5074636936187744\n",
            "2.5325841903686523\n",
            "2.2407283782958984\n",
            "2.4039628505706787\n",
            "2.471367835998535\n",
            "2.385056734085083\n",
            "2.574213743209839\n",
            "2.4443461894989014\n",
            "2.4210188388824463\n",
            "2.3837227821350098\n",
            "2.457474708557129\n",
            "2.564711332321167\n",
            "2.4177377223968506\n",
            "2.455930709838867\n",
            "2.507430076599121\n",
            "2.6040284633636475\n",
            "2.3553295135498047\n",
            "2.4628095626831055\n",
            "2.3823461532592773\n",
            "2.360931873321533\n",
            "2.4244325160980225\n",
            "2.4148504734039307\n",
            "2.4804794788360596\n",
            "2.4055557250976562\n",
            "2.449129581451416\n",
            "2.3511643409729004\n",
            "2.411008358001709\n",
            "2.474902391433716\n",
            "2.493807315826416\n",
            "2.330366373062134\n",
            "2.376732349395752\n",
            "2.403794288635254\n",
            "2.5359716415405273\n",
            "2.5030226707458496\n",
            "2.4142494201660156\n",
            "2.3801894187927246\n",
            "2.3879241943359375\n",
            "2.4651358127593994\n",
            "2.4046194553375244\n",
            "2.3636701107025146\n",
            "2.5140135288238525\n",
            "2.4912097454071045\n",
            "2.450840473175049\n",
            "2.524763584136963\n",
            "2.3906447887420654\n",
            "2.4735937118530273\n",
            "2.4112300872802734\n",
            "2.4371254444122314\n",
            "2.466860294342041\n",
            "2.4124438762664795\n",
            "2.4162213802337646\n",
            "2.560655117034912\n",
            "2.4433135986328125\n",
            "2.4918863773345947\n",
            "2.337024450302124\n",
            "2.5305116176605225\n",
            "2.367291212081909\n",
            "2.382716178894043\n",
            "2.388287305831909\n",
            "2.462902069091797\n",
            "2.4365925788879395\n",
            "2.463125228881836\n",
            "2.402927875518799\n",
            "2.4193930625915527\n",
            "2.4109222888946533\n",
            "2.5278868675231934\n",
            "2.4767932891845703\n",
            "2.3576443195343018\n",
            "2.497007369995117\n",
            "2.458024501800537\n",
            "2.4505679607391357\n",
            "2.410120964050293\n",
            "2.4700095653533936\n",
            "2.434535026550293\n",
            "2.4345710277557373\n",
            "2.5176029205322266\n",
            "2.4610092639923096\n",
            "2.4781014919281006\n",
            "2.4660112857818604\n",
            "2.4870810508728027\n",
            "2.3893837928771973\n",
            "2.503068447113037\n",
            "2.5005509853363037\n",
            "2.3709781169891357\n",
            "2.452908754348755\n",
            "2.556905508041382\n",
            "2.4793519973754883\n",
            "2.4290661811828613\n",
            "2.4610087871551514\n",
            "2.3876328468322754\n",
            "2.56813383102417\n",
            "2.4758570194244385\n",
            "2.3780341148376465\n",
            "2.517902135848999\n",
            "2.4687976837158203\n",
            "2.4624600410461426\n",
            "2.2562386989593506\n",
            "2.464768886566162\n",
            "2.409619092941284\n",
            "2.2966277599334717\n",
            "2.4155850410461426\n",
            "2.5573337078094482\n",
            "2.5836284160614014\n",
            "2.5622360706329346\n",
            "2.5593810081481934\n",
            "2.535496473312378\n",
            "2.2863810062408447\n",
            "2.4027633666992188\n",
            "2.3887858390808105\n",
            "2.50764536857605\n",
            "2.4033782482147217\n",
            "2.4880247116088867\n",
            "2.380728244781494\n",
            "2.409980058670044\n",
            "2.4902212619781494\n",
            "2.605189800262451\n",
            "2.2807281017303467\n",
            "2.4549691677093506\n",
            "2.3993160724639893\n",
            "2.423919677734375\n",
            "2.4350574016571045\n",
            "2.592369318008423\n",
            "2.53678035736084\n",
            "2.5179443359375\n",
            "2.554034948348999\n",
            "2.472659111022949\n",
            "2.4598474502563477\n",
            "2.566218614578247\n",
            "2.4704666137695312\n",
            "2.413821220397949\n",
            "2.5299718379974365\n",
            "2.3813977241516113\n",
            "2.2870616912841797\n",
            "2.5035197734832764\n",
            "2.3294804096221924\n",
            "2.463695764541626\n",
            "2.4743447303771973\n",
            "2.524247169494629\n",
            "2.5376381874084473\n",
            "2.4610843658447266\n",
            "2.4415316581726074\n",
            "2.497748613357544\n",
            "2.6299891471862793\n",
            "2.3302628993988037\n",
            "2.437962055206299\n",
            "2.3438944816589355\n",
            "2.4959919452667236\n",
            "2.5618789196014404\n",
            "2.481245517730713\n",
            "2.611257314682007\n",
            "2.5417749881744385\n",
            "2.5429720878601074\n",
            "2.4553632736206055\n",
            "2.5384485721588135\n",
            "2.34602689743042\n",
            "2.4138524532318115\n",
            "2.3973917961120605\n",
            "2.384248733520508\n",
            "2.506530284881592\n",
            "2.5505521297454834\n",
            "2.4623568058013916\n",
            "2.379654884338379\n",
            "2.4828977584838867\n",
            "2.4339375495910645\n",
            "2.5882389545440674\n",
            "2.578855514526367\n",
            "2.524836778640747\n",
            "2.4656758308410645\n",
            "2.45046067237854\n",
            "2.452366352081299\n",
            "2.487468719482422\n",
            "2.518746852874756\n",
            "2.410670757293701\n",
            "2.4617488384246826\n",
            "2.3386456966400146\n",
            "2.5407962799072266\n",
            "2.4814653396606445\n",
            "2.4699456691741943\n",
            "2.488886594772339\n",
            "2.421945095062256\n",
            "2.5183944702148438\n",
            "2.5862772464752197\n",
            "2.5126914978027344\n",
            "2.507887840270996\n",
            "2.4918227195739746\n",
            "2.4786930084228516\n",
            "2.4756762981414795\n",
            "2.4348573684692383\n",
            "2.4860894680023193\n",
            "2.4512507915496826\n",
            "2.568288803100586\n",
            "2.405005931854248\n",
            "2.385348320007324\n",
            "2.4721693992614746\n",
            "2.521338701248169\n",
            "2.3228094577789307\n",
            "2.3599464893341064\n",
            "2.3987715244293213\n",
            "2.448167562484741\n",
            "2.4908447265625\n",
            "2.4445555210113525\n",
            "2.3739583492279053\n",
            "2.5429751873016357\n",
            "2.5121428966522217\n",
            "2.600187301635742\n",
            "2.4223246574401855\n",
            "2.4535841941833496\n",
            "2.5084941387176514\n",
            "2.441053867340088\n",
            "2.4111568927764893\n",
            "2.515599250793457\n",
            "2.523956775665283\n",
            "2.459066867828369\n",
            "2.5180411338806152\n",
            "2.4101030826568604\n",
            "2.4665677547454834\n",
            "2.435558319091797\n",
            "2.458143472671509\n",
            "2.342395067214966\n",
            "2.389286518096924\n",
            "2.493511438369751\n",
            "2.4859585762023926\n",
            "2.4293713569641113\n",
            "2.4700145721435547\n",
            "2.4630379676818848\n",
            "2.400388479232788\n",
            "2.471123218536377\n",
            "2.448193073272705\n",
            "2.444417715072632\n",
            "2.4466171264648438\n",
            "2.387758731842041\n",
            "2.4955482482910156\n",
            "2.45674204826355\n",
            "2.49304461479187\n",
            "2.4353787899017334\n",
            "2.467721700668335\n",
            "2.303461790084839\n",
            "2.4587695598602295\n",
            "2.4455811977386475\n",
            "2.544443130493164\n",
            "2.496630907058716\n",
            "2.458601236343384\n",
            "2.4781479835510254\n",
            "2.4657375812530518\n",
            "2.528533697128296\n",
            "2.3707895278930664\n",
            "2.4566991329193115\n",
            "2.50787091255188\n",
            "2.5819575786590576\n",
            "2.4945268630981445\n",
            "2.4514248371124268\n",
            "2.3986380100250244\n",
            "2.4576828479766846\n",
            "2.2451794147491455\n",
            "2.420041084289551\n",
            "2.4115824699401855\n",
            "2.477094888687134\n",
            "2.5477962493896484\n",
            "2.4698643684387207\n",
            "2.4551539421081543\n",
            "2.520998954772949\n",
            "2.5551412105560303\n",
            "2.4175796508789062\n",
            "2.3758506774902344\n",
            "2.407583236694336\n",
            "2.489161968231201\n",
            "2.49703311920166\n",
            "2.4472076892852783\n",
            "2.412027359008789\n",
            "2.354605197906494\n",
            "2.383587121963501\n",
            "2.493507146835327\n",
            "2.4933745861053467\n",
            "2.565852642059326\n",
            "2.569446325302124\n",
            "2.4659922122955322\n",
            "2.418459177017212\n",
            "2.4577295780181885\n",
            "2.494480848312378\n",
            "2.4296681880950928\n",
            "2.497958183288574\n",
            "2.4640400409698486\n",
            "2.5619380474090576\n",
            "2.5064072608947754\n",
            "2.4396798610687256\n",
            "2.5485503673553467\n",
            "2.516167640686035\n",
            "2.433809280395508\n",
            "2.526728391647339\n",
            "2.5458016395568848\n",
            "2.52120304107666\n",
            "2.4012632369995117\n",
            "2.365030288696289\n",
            "2.4132778644561768\n",
            "2.4547486305236816\n",
            "2.3882720470428467\n",
            "2.4165027141571045\n",
            "2.34409761428833\n",
            "2.333486557006836\n",
            "2.4585602283477783\n",
            "2.542541742324829\n",
            "2.3970630168914795\n",
            "2.442939043045044\n",
            "2.461698532104492\n",
            "2.426957368850708\n",
            "2.4435653686523438\n",
            "2.6272294521331787\n",
            "2.3362464904785156\n",
            "2.4780993461608887\n",
            "2.410487174987793\n",
            "2.3660385608673096\n",
            "2.4891304969787598\n",
            "2.478571653366089\n",
            "2.4837796688079834\n",
            "2.453993797302246\n",
            "2.414290189743042\n",
            "2.454529047012329\n",
            "2.4167284965515137\n",
            "2.522974967956543\n",
            "2.464721441268921\n",
            "2.4209678173065186\n",
            "2.3802669048309326\n",
            "2.46225643157959\n",
            "2.456634521484375\n",
            "2.393620252609253\n",
            "2.2931954860687256\n",
            "2.450505495071411\n",
            "2.4276387691497803\n",
            "2.4949307441711426\n",
            "2.400224208831787\n",
            "2.3568694591522217\n",
            "2.432340621948242\n",
            "2.448517084121704\n",
            "2.476515531539917\n",
            "2.3921260833740234\n",
            "2.451810359954834\n",
            "2.5251708030700684\n",
            "2.4742138385772705\n",
            "2.459939956665039\n",
            "2.438408374786377\n",
            "2.510507583618164\n",
            "2.403651237487793\n",
            "2.3798418045043945\n",
            "2.396973133087158\n",
            "2.3594233989715576\n",
            "2.5519859790802\n",
            "2.570669174194336\n",
            "2.485398530960083\n",
            "2.6080946922302246\n",
            "2.4862914085388184\n",
            "2.551258087158203\n",
            "2.4921140670776367\n",
            "2.463153600692749\n",
            "2.5250682830810547\n",
            "2.4532761573791504\n",
            "2.4037652015686035\n",
            "2.454010486602783\n",
            "2.5046825408935547\n",
            "2.46028733253479\n",
            "2.4418866634368896\n",
            "2.5132381916046143\n",
            "2.4053702354431152\n",
            "2.5378804206848145\n",
            "2.3954498767852783\n",
            "2.4686989784240723\n",
            "2.35089111328125\n",
            "2.462777853012085\n",
            "2.609283208847046\n",
            "2.403482675552368\n",
            "2.3487226963043213\n",
            "2.528980255126953\n",
            "2.4262194633483887\n",
            "2.3718934059143066\n",
            "2.4083070755004883\n",
            "2.5944597721099854\n",
            "2.428967237472534\n",
            "2.4252004623413086\n",
            "2.3250489234924316\n",
            "2.5613210201263428\n",
            "2.5221407413482666\n",
            "2.4857866764068604\n",
            "2.4359841346740723\n",
            "2.2816407680511475\n",
            "2.431037187576294\n",
            "2.432413101196289\n",
            "2.5033113956451416\n",
            "2.3806300163269043\n",
            "2.5048272609710693\n",
            "2.3825929164886475\n",
            "2.5096828937530518\n",
            "2.406228542327881\n",
            "2.5942749977111816\n",
            "2.283177614212036\n",
            "2.3737120628356934\n",
            "2.4516000747680664\n",
            "2.439959764480591\n",
            "2.4705679416656494\n",
            "2.439258337020874\n",
            "2.5863821506500244\n",
            "2.4787683486938477\n",
            "2.3730616569519043\n",
            "2.370168685913086\n",
            "2.525965929031372\n",
            "2.5041565895080566\n",
            "2.3285677433013916\n",
            "2.432624101638794\n",
            "2.3830065727233887\n",
            "2.492435932159424\n",
            "2.4638571739196777\n",
            "2.4045753479003906\n",
            "2.4236297607421875\n",
            "2.5135819911956787\n",
            "2.4152719974517822\n",
            "2.518549680709839\n",
            "2.444284200668335\n",
            "2.377782106399536\n",
            "2.676150321960449\n",
            "2.5425374507904053\n",
            "2.5520620346069336\n",
            "2.373263120651245\n",
            "2.3138015270233154\n",
            "2.490675687789917\n",
            "2.4888381958007812\n",
            "2.5229480266571045\n",
            "2.4884192943573\n",
            "2.548112392425537\n",
            "2.420128583908081\n",
            "2.4499757289886475\n",
            "2.4185330867767334\n",
            "2.4087038040161133\n",
            "2.479121208190918\n",
            "2.4940671920776367\n",
            "2.4905478954315186\n",
            "2.4758777618408203\n",
            "2.3879027366638184\n",
            "2.3712925910949707\n",
            "2.5380537509918213\n",
            "2.4788899421691895\n",
            "2.410418748855591\n",
            "2.541861057281494\n",
            "2.4181604385375977\n",
            "2.446823835372925\n",
            "2.5030274391174316\n",
            "2.2825515270233154\n",
            "2.4283149242401123\n",
            "2.5949907302856445\n",
            "2.363771915435791\n",
            "2.400157928466797\n",
            "2.6322529315948486\n",
            "2.5504157543182373\n",
            "2.417691946029663\n",
            "2.410900354385376\n",
            "2.532409906387329\n",
            "2.3199875354766846\n",
            "2.4533305168151855\n",
            "2.452446222305298\n",
            "2.474569082260132\n",
            "2.5571932792663574\n",
            "2.4412288665771484\n",
            "2.5335116386413574\n",
            "2.4219021797180176\n",
            "2.432277202606201\n",
            "2.466632127761841\n",
            "2.4657251834869385\n",
            "2.4418325424194336\n",
            "2.3700156211853027\n",
            "2.4663338661193848\n",
            "2.4452052116394043\n",
            "2.3915069103240967\n",
            "2.5830132961273193\n",
            "2.4303319454193115\n",
            "2.4760079383850098\n",
            "2.3952603340148926\n",
            "2.455470323562622\n",
            "2.4226107597351074\n",
            "2.3986921310424805\n",
            "2.314251184463501\n",
            "2.5398740768432617\n",
            "2.478973388671875\n",
            "2.504086494445801\n",
            "2.510913610458374\n",
            "2.4084866046905518\n",
            "2.3861985206604004\n",
            "2.592323064804077\n",
            "2.530790090560913\n",
            "2.4214396476745605\n",
            "2.4673545360565186\n",
            "2.419346332550049\n",
            "2.510158061981201\n",
            "2.428962469100952\n",
            "2.528851270675659\n",
            "2.2927911281585693\n",
            "2.463803291320801\n",
            "2.657971143722534\n",
            "2.507718801498413\n",
            "2.3462092876434326\n",
            "2.4851083755493164\n",
            "2.462916612625122\n",
            "2.4281396865844727\n",
            "2.4748361110687256\n",
            "2.4586539268493652\n",
            "2.3808000087738037\n",
            "2.3916993141174316\n",
            "2.385236978530884\n",
            "2.282350540161133\n",
            "2.4027724266052246\n",
            "2.4739184379577637\n",
            "2.454972982406616\n",
            "2.66019344329834\n",
            "2.4988911151885986\n",
            "2.4473824501037598\n",
            "2.40368914604187\n",
            "2.378079652786255\n",
            "2.476837158203125\n",
            "2.335430145263672\n",
            "2.377162456512451\n",
            "2.4115939140319824\n",
            "2.5457117557525635\n",
            "2.4205851554870605\n",
            "2.575138568878174\n",
            "2.466151237487793\n",
            "2.4539902210235596\n",
            "2.358663320541382\n",
            "2.511240005493164\n",
            "2.4521303176879883\n",
            "2.372994899749756\n",
            "2.4450180530548096\n",
            "2.2821106910705566\n",
            "2.480858564376831\n",
            "2.4991393089294434\n",
            "2.4367780685424805\n",
            "2.45924711227417\n",
            "2.481304883956909\n",
            "2.5770227909088135\n",
            "2.5880959033966064\n",
            "2.4393081665039062\n",
            "2.4488534927368164\n",
            "2.475031852722168\n",
            "2.522523880004883\n",
            "2.464627981185913\n",
            "2.3489327430725098\n",
            "2.410024404525757\n",
            "2.4399893283843994\n",
            "2.410778045654297\n",
            "2.4165942668914795\n",
            "2.494971752166748\n",
            "2.3194432258605957\n",
            "2.4161276817321777\n",
            "2.609205961227417\n",
            "2.4439010620117188\n",
            "2.4285664558410645\n",
            "2.4800326824188232\n",
            "2.4564507007598877\n",
            "2.5016348361968994\n",
            "2.4839892387390137\n",
            "2.324732780456543\n",
            "2.4853270053863525\n",
            "2.3442952632904053\n",
            "2.4414589405059814\n",
            "2.4824886322021484\n",
            "2.401643991470337\n",
            "2.51454496383667\n",
            "2.501171350479126\n",
            "2.5973117351531982\n",
            "2.6000726222991943\n",
            "2.4814841747283936\n",
            "2.4791109561920166\n",
            "2.398918628692627\n",
            "2.4453892707824707\n",
            "2.42972731590271\n",
            "2.501011848449707\n",
            "2.412055492401123\n",
            "2.509469747543335\n",
            "2.447951555252075\n",
            "2.4603166580200195\n",
            "2.4587619304656982\n",
            "2.4550788402557373\n",
            "2.408925771713257\n",
            "2.3613758087158203\n",
            "2.4767391681671143\n",
            "2.4283976554870605\n",
            "2.5085296630859375\n",
            "2.3752830028533936\n",
            "2.389272689819336\n",
            "2.2844221591949463\n",
            "2.440579652786255\n",
            "2.5247464179992676\n",
            "2.332453489303589\n",
            "2.435635805130005\n",
            "2.3935067653656006\n",
            "2.611924409866333\n",
            "2.421809673309326\n",
            "2.4925501346588135\n",
            "2.4417998790740967\n",
            "2.3032407760620117\n",
            "2.4413740634918213\n",
            "2.461487293243408\n",
            "2.4611892700195312\n",
            "2.46044921875\n",
            "2.3423068523406982\n",
            "2.3410885334014893\n",
            "2.434748888015747\n",
            "2.3694989681243896\n",
            "2.4323344230651855\n",
            "2.565006732940674\n",
            "2.4415395259857178\n",
            "2.454934597015381\n",
            "2.495732069015503\n",
            "2.4488532543182373\n",
            "2.399583339691162\n",
            "2.5021860599517822\n",
            "2.4415857791900635\n",
            "2.5061659812927246\n",
            "2.459733247756958\n",
            "2.456233501434326\n",
            "2.496882677078247\n",
            "2.2425546646118164\n",
            "2.483189821243286\n",
            "2.4262914657592773\n",
            "2.4306793212890625\n",
            "2.3289992809295654\n",
            "2.477118492126465\n",
            "2.434469699859619\n",
            "2.5031869411468506\n",
            "2.5345468521118164\n",
            "2.516434907913208\n",
            "2.4438884258270264\n",
            "2.478633165359497\n",
            "2.4838976860046387\n",
            "2.4230525493621826\n",
            "2.611260175704956\n",
            "2.4847500324249268\n",
            "2.4322400093078613\n",
            "2.4996321201324463\n",
            "2.481264114379883\n",
            "2.4420456886291504\n",
            "2.4418439865112305\n",
            "2.4688520431518555\n",
            "2.4132132530212402\n",
            "2.5478768348693848\n",
            "2.3987925052642822\n",
            "2.427992582321167\n",
            "2.3996286392211914\n",
            "2.4037883281707764\n",
            "2.472698926925659\n",
            "2.4103565216064453\n",
            "2.4676527976989746\n",
            "2.5473272800445557\n",
            "2.40055251121521\n",
            "2.3998777866363525\n",
            "2.3837618827819824\n",
            "2.372926950454712\n",
            "2.4989376068115234\n",
            "2.4569015502929688\n",
            "2.5485410690307617\n",
            "2.3579773902893066\n",
            "2.4940595626831055\n",
            "2.402859926223755\n",
            "2.420910120010376\n",
            "2.443106174468994\n",
            "2.4135067462921143\n",
            "2.443999767303467\n",
            "2.4701223373413086\n",
            "2.3915369510650635\n",
            "2.5396676063537598\n",
            "2.53066086769104\n",
            "2.45986270904541\n",
            "2.550588846206665\n",
            "2.522101640701294\n",
            "2.531656265258789\n",
            "2.3823540210723877\n",
            "2.428651809692383\n",
            "2.5020463466644287\n",
            "2.5239145755767822\n",
            "2.4189515113830566\n",
            "2.487840414047241\n",
            "2.5878398418426514\n",
            "2.4871103763580322\n",
            "2.2544302940368652\n",
            "2.3568332195281982\n",
            "2.6109936237335205\n",
            "2.5779342651367188\n",
            "2.484395980834961\n",
            "2.519148349761963\n",
            "2.5446064472198486\n",
            "2.4441874027252197\n",
            "2.4239020347595215\n",
            "2.4551773071289062\n",
            "2.476755380630493\n",
            "2.6339359283447266\n",
            "2.447613477706909\n",
            "2.32906174659729\n",
            "2.4787473678588867\n",
            "2.4130523204803467\n",
            "2.3289456367492676\n",
            "2.4042129516601562\n",
            "2.3457093238830566\n",
            "2.4291210174560547\n",
            "2.476377010345459\n",
            "2.4819529056549072\n",
            "2.443521738052368\n",
            "2.3156840801239014\n",
            "2.44297456741333\n",
            "2.5920631885528564\n",
            "2.4341049194335938\n",
            "2.4191977977752686\n",
            "2.5007846355438232\n",
            "2.4502389430999756\n",
            "2.422367811203003\n",
            "2.497253179550171\n",
            "2.5169389247894287\n",
            "2.5087814331054688\n",
            "2.5311825275421143\n",
            "2.4394960403442383\n",
            "2.4378294944763184\n",
            "2.476983070373535\n",
            "2.3922200202941895\n",
            "2.4914629459381104\n",
            "2.494502067565918\n",
            "2.4542579650878906\n",
            "2.5529520511627197\n",
            "2.4071640968322754\n",
            "2.4321930408477783\n",
            "2.3558766841888428\n",
            "2.4736969470977783\n",
            "2.5083694458007812\n",
            "2.414167881011963\n",
            "2.447289228439331\n",
            "2.4183762073516846\n",
            "2.5232315063476562\n",
            "2.5071074962615967\n",
            "2.4404540061950684\n",
            "2.419674873352051\n",
            "2.4822657108306885\n",
            "2.441403865814209\n",
            "2.4303789138793945\n",
            "2.432671308517456\n",
            "2.4717390537261963\n",
            "2.3358449935913086\n",
            "2.45013689994812\n",
            "2.416867733001709\n",
            "2.525245428085327\n",
            "2.6552746295928955\n",
            "2.46290922164917\n",
            "2.421334743499756\n",
            "2.4002277851104736\n",
            "2.4650161266326904\n",
            "2.582193613052368\n",
            "2.422830820083618\n",
            "2.459000587463379\n",
            "2.3970565795898438\n",
            "2.4474334716796875\n",
            "2.5801808834075928\n",
            "2.4974894523620605\n",
            "2.4341840744018555\n",
            "2.4974751472473145\n",
            "2.4793617725372314\n",
            "2.4197945594787598\n",
            "2.597254991531372\n",
            "2.3165531158447266\n",
            "2.49576997756958\n",
            "2.5600228309631348\n",
            "2.338160514831543\n",
            "2.4763545989990234\n",
            "2.4772465229034424\n",
            "2.379385232925415\n",
            "2.433892011642456\n",
            "2.4371907711029053\n",
            "2.548593044281006\n",
            "2.4915952682495117\n",
            "2.3994052410125732\n",
            "2.5333199501037598\n",
            "2.5451154708862305\n",
            "2.517707109451294\n",
            "2.459693431854248\n",
            "2.3708958625793457\n",
            "2.4420483112335205\n",
            "2.5066726207733154\n",
            "2.508777379989624\n",
            "2.478572130203247\n",
            "2.4430296421051025\n",
            "2.318586826324463\n",
            "2.5202736854553223\n",
            "2.4990811347961426\n",
            "2.2788472175598145\n",
            "2.455972194671631\n",
            "2.509406328201294\n",
            "2.5456483364105225\n",
            "2.394587755203247\n",
            "2.4502041339874268\n",
            "2.529144287109375\n",
            "2.543137311935425\n",
            "2.3551666736602783\n",
            "2.426717519760132\n",
            "2.3636865615844727\n",
            "2.361185073852539\n",
            "2.474769353866577\n",
            "2.5178329944610596\n",
            "2.4831454753875732\n",
            "2.4426393508911133\n",
            "2.521362543106079\n",
            "2.423926591873169\n",
            "2.425614595413208\n",
            "2.465388774871826\n",
            "2.4477100372314453\n",
            "2.537034749984741\n",
            "2.493922233581543\n",
            "2.3346481323242188\n",
            "2.491145610809326\n",
            "2.454545021057129\n",
            "2.4738106727600098\n",
            "2.3820009231567383\n",
            "2.504854917526245\n",
            "2.468510150909424\n",
            "2.413649320602417\n",
            "2.41188383102417\n",
            "2.487243413925171\n",
            "2.621582269668579\n",
            "2.568572521209717\n",
            "2.4566125869750977\n",
            "2.383805274963379\n",
            "2.4087741374969482\n",
            "2.4004077911376953\n",
            "2.4966487884521484\n",
            "2.479435443878174\n",
            "2.486691474914551\n",
            "2.3316447734832764\n",
            "2.459430694580078\n",
            "2.442002773284912\n",
            "2.3671905994415283\n",
            "2.5083541870117188\n",
            "2.3680813312530518\n",
            "2.3790712356567383\n",
            "2.471200466156006\n",
            "2.4654202461242676\n",
            "2.517591714859009\n",
            "2.3616178035736084\n",
            "2.424699544906616\n",
            "2.4873251914978027\n",
            "2.462672233581543\n",
            "2.5377085208892822\n",
            "2.467630386352539\n",
            "2.393695831298828\n",
            "2.4711060523986816\n",
            "2.461448907852173\n",
            "2.421888828277588\n",
            "2.485557794570923\n",
            "2.475179433822632\n",
            "2.4312143325805664\n",
            "2.5464935302734375\n",
            "2.417173385620117\n",
            "2.3668019771575928\n",
            "2.4999594688415527\n",
            "2.478304862976074\n",
            "2.381725788116455\n",
            "2.4888699054718018\n",
            "2.5667247772216797\n",
            "2.445394515991211\n",
            "2.4162960052490234\n",
            "2.451939105987549\n",
            "2.553117513656616\n",
            "2.6436898708343506\n",
            "2.5380072593688965\n",
            "2.374267578125\n",
            "2.336636781692505\n",
            "2.4786553382873535\n",
            "2.403883934020996\n",
            "2.440061092376709\n",
            "2.436598300933838\n",
            "2.4880213737487793\n",
            "2.3678321838378906\n",
            "2.450007915496826\n",
            "2.3967957496643066\n",
            "2.557020425796509\n",
            "2.5189223289489746\n",
            "2.4435253143310547\n",
            "2.366030216217041\n",
            "2.4273715019226074\n",
            "2.4944515228271484\n",
            "2.5808372497558594\n",
            "2.553250551223755\n",
            "2.435340404510498\n",
            "2.424008369445801\n",
            "2.4038279056549072\n",
            "2.541043758392334\n",
            "2.459249258041382\n",
            "2.4835174083709717\n",
            "2.3867266178131104\n",
            "2.561972141265869\n",
            "2.4906883239746094\n",
            "2.538027286529541\n",
            "2.482574939727783\n",
            "2.4215176105499268\n",
            "2.3910319805145264\n",
            "2.407400608062744\n",
            "2.433075428009033\n",
            "2.475726842880249\n",
            "2.594724416732788\n",
            "2.453768014907837\n",
            "2.4546868801116943\n",
            "2.3586385250091553\n",
            "2.3311009407043457\n",
            "2.5358033180236816\n",
            "2.4851012229919434\n",
            "2.3920581340789795\n",
            "2.5031332969665527\n",
            "2.4194319248199463\n",
            "2.5444066524505615\n",
            "2.363961935043335\n",
            "2.428422212600708\n",
            "2.5512280464172363\n",
            "2.3961257934570312\n",
            "2.37532377243042\n",
            "2.641140937805176\n",
            "2.425407886505127\n",
            "2.4630050659179688\n",
            "2.4898178577423096\n",
            "2.4528415203094482\n",
            "2.382913112640381\n",
            "2.500912666320801\n",
            "2.45937180519104\n",
            "2.4749057292938232\n",
            "2.4568002223968506\n",
            "2.530141592025757\n",
            "2.4379725456237793\n",
            "2.457545280456543\n",
            "2.414135694503784\n",
            "2.4525859355926514\n",
            "2.48742938041687\n",
            "2.4226391315460205\n",
            "2.4783709049224854\n",
            "2.466761827468872\n",
            "2.5146472454071045\n",
            "2.4179537296295166\n",
            "2.587301254272461\n",
            "2.5074219703674316\n",
            "2.3795008659362793\n",
            "2.570115089416504\n",
            "2.484417676925659\n",
            "2.3367300033569336\n",
            "2.5046918392181396\n",
            "2.428837299346924\n",
            "2.4486961364746094\n",
            "2.454464912414551\n",
            "2.430584669113159\n",
            "2.2978062629699707\n",
            "2.3369133472442627\n",
            "2.4159960746765137\n",
            "2.524495840072632\n",
            "2.4399561882019043\n",
            "2.4194140434265137\n",
            "2.5323917865753174\n",
            "2.413182497024536\n",
            "2.406582832336426\n",
            "2.584064483642578\n",
            "2.5089244842529297\n",
            "2.5382063388824463\n",
            "2.505143165588379\n",
            "2.499300003051758\n",
            "2.471604108810425\n",
            "2.34858775138855\n",
            "2.454084873199463\n",
            "2.4186642169952393\n",
            "2.498692035675049\n",
            "2.3363115787506104\n",
            "2.5161893367767334\n",
            "2.469568967819214\n",
            "2.427931308746338\n",
            "2.375486135482788\n",
            "2.4115524291992188\n",
            "2.4302003383636475\n",
            "2.4179446697235107\n",
            "2.421304941177368\n",
            "2.523364543914795\n",
            "2.4347219467163086\n",
            "2.462285280227661\n",
            "2.4729743003845215\n",
            "2.5668609142303467\n",
            "2.4134557247161865\n",
            "2.4704856872558594\n",
            "2.5775039196014404\n",
            "2.55088210105896\n",
            "2.4662668704986572\n",
            "2.5230770111083984\n",
            "2.4136505126953125\n",
            "2.5166943073272705\n",
            "2.3837194442749023\n",
            "2.4623498916625977\n",
            "2.6070287227630615\n",
            "2.54909610748291\n",
            "2.483682870864868\n",
            "2.4707109928131104\n",
            "2.54122257232666\n",
            "2.4447734355926514\n",
            "2.4953386783599854\n",
            "2.4238953590393066\n",
            "2.423956871032715\n",
            "2.3469605445861816\n",
            "2.3459713459014893\n",
            "2.432438850402832\n",
            "2.5885212421417236\n",
            "2.4195470809936523\n",
            "2.4858903884887695\n",
            "2.5254666805267334\n",
            "2.394228219985962\n",
            "2.524461269378662\n",
            "2.429551124572754\n",
            "2.4399490356445312\n",
            "2.437912940979004\n",
            "2.3584420680999756\n",
            "2.3493049144744873\n",
            "2.351555824279785\n",
            "2.5293474197387695\n",
            "2.50177264213562\n",
            "2.3202433586120605\n",
            "2.395961284637451\n",
            "2.4897141456604004\n",
            "2.4889731407165527\n",
            "2.496591567993164\n",
            "2.5685675144195557\n",
            "2.4129083156585693\n",
            "2.354203462600708\n",
            "2.3897297382354736\n",
            "2.41343092918396\n",
            "2.5800621509552\n",
            "2.5090761184692383\n",
            "2.425344944000244\n",
            "2.650240659713745\n",
            "2.3877198696136475\n",
            "2.465430498123169\n",
            "2.4451773166656494\n",
            "2.4201667308807373\n",
            "2.5315539836883545\n",
            "2.5003151893615723\n",
            "2.5412778854370117\n",
            "2.4787757396698\n",
            "2.5304203033447266\n",
            "2.4706666469573975\n",
            "2.47015118598938\n",
            "2.5178093910217285\n",
            "2.426950454711914\n",
            "2.401353597640991\n",
            "2.302114725112915\n",
            "2.4582388401031494\n",
            "2.4696879386901855\n",
            "2.499821186065674\n",
            "2.3809211254119873\n",
            "2.5334665775299072\n",
            "2.51982045173645\n",
            "2.4253907203674316\n",
            "2.3782236576080322\n",
            "2.5424177646636963\n",
            "2.5627338886260986\n",
            "2.4444282054901123\n",
            "2.5390357971191406\n",
            "2.3784782886505127\n",
            "2.4984123706817627\n",
            "2.405460834503174\n",
            "2.5226569175720215\n",
            "2.572422981262207\n",
            "2.4969542026519775\n",
            "2.481496810913086\n",
            "2.3962252140045166\n",
            "2.5032217502593994\n",
            "2.333876609802246\n",
            "2.3895368576049805\n",
            "2.4999794960021973\n",
            "2.3878188133239746\n",
            "2.3934450149536133\n",
            "2.5442306995391846\n",
            "2.378873825073242\n",
            "2.4611682891845703\n",
            "2.3043432235717773\n",
            "2.4257113933563232\n",
            "2.4359259605407715\n",
            "2.434769630432129\n",
            "2.321486473083496\n",
            "2.4180314540863037\n",
            "2.433199167251587\n",
            "2.5321004390716553\n",
            "2.3965437412261963\n",
            "2.487886905670166\n",
            "2.400310516357422\n",
            "2.4597742557525635\n",
            "2.4516947269439697\n",
            "2.3470098972320557\n",
            "2.502434253692627\n",
            "2.339723825454712\n",
            "2.466491460800171\n",
            "2.491907835006714\n",
            "2.352293014526367\n",
            "2.506291389465332\n",
            "2.401660442352295\n",
            "2.309687614440918\n",
            "2.5306878089904785\n",
            "2.5075409412384033\n",
            "2.4635026454925537\n",
            "2.5099880695343018\n",
            "2.362835168838501\n",
            "2.471039295196533\n",
            "2.473994255065918\n",
            "2.510449171066284\n",
            "2.429798126220703\n",
            "2.441460609436035\n",
            "2.5440833568573\n",
            "2.486921787261963\n",
            "2.5356223583221436\n",
            "2.4668822288513184\n",
            "2.5612621307373047\n",
            "2.379244327545166\n",
            "2.2565577030181885\n",
            "2.400822401046753\n",
            "2.447232484817505\n",
            "2.5895955562591553\n",
            "2.431795358657837\n",
            "2.558264970779419\n",
            "2.4270386695861816\n",
            "2.501873731613159\n",
            "2.4702632427215576\n",
            "2.4195621013641357\n",
            "2.630246639251709\n",
            "2.333712339401245\n",
            "2.3633480072021484\n",
            "2.5070624351501465\n",
            "2.351395845413208\n",
            "2.4737894535064697\n",
            "2.3467931747436523\n",
            "2.45719838142395\n",
            "2.5111289024353027\n",
            "2.491590738296509\n",
            "2.6239917278289795\n",
            "2.237722396850586\n",
            "2.4801721572875977\n",
            "2.4271538257598877\n",
            "2.4543354511260986\n",
            "2.5162854194641113\n",
            "2.467292547225952\n",
            "2.4337503910064697\n",
            "2.5021169185638428\n",
            "2.6202898025512695\n",
            "2.4625744819641113\n",
            "2.3221828937530518\n",
            "2.4045746326446533\n",
            "2.430978775024414\n",
            "2.4703989028930664\n",
            "2.3589916229248047\n",
            "2.2994179725646973\n",
            "2.445199966430664\n",
            "2.440354824066162\n",
            "2.3919827938079834\n",
            "2.4744551181793213\n",
            "2.4390175342559814\n",
            "2.5060949325561523\n",
            "2.6561214923858643\n",
            "2.402895450592041\n",
            "2.4643003940582275\n",
            "2.4837019443511963\n",
            "2.5124223232269287\n",
            "2.434473991394043\n",
            "2.492441415786743\n",
            "2.3873825073242188\n",
            "2.3387107849121094\n",
            "2.624410390853882\n",
            "2.3850905895233154\n",
            "2.4773759841918945\n",
            "2.3975374698638916\n",
            "2.436901569366455\n",
            "2.4868690967559814\n",
            "2.351710319519043\n",
            "2.5126287937164307\n",
            "2.5432376861572266\n",
            "2.4469072818756104\n",
            "2.4705710411071777\n",
            "2.301988124847412\n",
            "2.5242414474487305\n",
            "2.505403757095337\n",
            "2.5031633377075195\n",
            "2.460036277770996\n",
            "2.4595096111297607\n",
            "2.431009292602539\n",
            "2.485805034637451\n",
            "2.3514163494110107\n",
            "2.33979868888855\n",
            "2.234997034072876\n",
            "2.5056192874908447\n",
            "2.3786134719848633\n",
            "2.3453547954559326\n",
            "2.4990384578704834\n",
            "2.3775932788848877\n",
            "2.368706226348877\n",
            "2.482823371887207\n",
            "2.438034772872925\n",
            "2.3900012969970703\n",
            "2.4711546897888184\n",
            "2.409998655319214\n",
            "2.3931334018707275\n",
            "2.6081764698028564\n",
            "2.4293880462646484\n",
            "2.4775381088256836\n",
            "2.515634775161743\n",
            "2.496717929840088\n",
            "2.448751211166382\n",
            "2.366572618484497\n",
            "2.411968231201172\n",
            "2.443443775177002\n",
            "2.416825771331787\n",
            "2.6115036010742188\n",
            "2.3678767681121826\n",
            "2.4348084926605225\n",
            "2.4090280532836914\n",
            "2.567711114883423\n",
            "2.397963523864746\n",
            "2.437445878982544\n",
            "2.4149534702301025\n",
            "2.479255437850952\n",
            "2.396179676055908\n",
            "2.4920694828033447\n",
            "2.3886663913726807\n",
            "2.4631693363189697\n",
            "2.4439475536346436\n",
            "2.3680310249328613\n",
            "2.410875082015991\n",
            "2.384859800338745\n",
            "2.529088020324707\n",
            "2.4605154991149902\n",
            "2.428377628326416\n",
            "2.390857458114624\n",
            "2.4120395183563232\n",
            "2.4367058277130127\n",
            "2.3968801498413086\n",
            "2.427016019821167\n",
            "2.385026693344116\n",
            "2.4268152713775635\n",
            "2.495612859725952\n",
            "2.4001379013061523\n",
            "2.334362506866455\n",
            "2.4585657119750977\n",
            "2.3473405838012695\n",
            "2.471163511276245\n",
            "2.3185200691223145\n",
            "2.389704465866089\n",
            "2.4938409328460693\n",
            "2.3964545726776123\n",
            "2.4500436782836914\n",
            "2.3684566020965576\n",
            "2.406773567199707\n",
            "2.2886416912078857\n",
            "2.4510154724121094\n",
            "2.407632350921631\n",
            "2.408719778060913\n",
            "2.5002048015594482\n",
            "2.43123197555542\n",
            "2.5080182552337646\n",
            "2.4574387073516846\n",
            "2.399873733520508\n",
            "2.3366754055023193\n",
            "2.4801716804504395\n",
            "2.507995367050171\n",
            "2.343162775039673\n",
            "2.3741371631622314\n",
            "2.4357314109802246\n",
            "2.450206756591797\n",
            "2.4977033138275146\n",
            "2.444214344024658\n",
            "2.446091413497925\n",
            "2.508195638656616\n",
            "2.406430244445801\n",
            "2.461071491241455\n",
            "2.582824230194092\n",
            "2.4672811031341553\n",
            "2.4827070236206055\n",
            "2.426039934158325\n",
            "2.464867353439331\n",
            "2.4737699031829834\n",
            "2.450770139694214\n",
            "2.457686424255371\n",
            "2.414752721786499\n",
            "2.52345609664917\n",
            "2.484901189804077\n",
            "2.41990065574646\n",
            "2.5719525814056396\n",
            "2.5300674438476562\n",
            "2.3940911293029785\n",
            "2.474560260772705\n",
            "2.4514834880828857\n",
            "2.410182237625122\n",
            "2.3645355701446533\n",
            "2.3695502281188965\n",
            "2.490908622741699\n",
            "2.3834116458892822\n",
            "2.514333724975586\n",
            "2.3103816509246826\n",
            "2.386918783187866\n",
            "2.5863842964172363\n",
            "2.417388677597046\n",
            "2.472470760345459\n",
            "2.4883835315704346\n",
            "2.3894548416137695\n",
            "2.3203799724578857\n",
            "2.398197889328003\n",
            "2.5387179851531982\n",
            "2.462783098220825\n",
            "2.4577555656433105\n",
            "2.4427103996276855\n",
            "2.4433045387268066\n",
            "2.436673641204834\n",
            "2.4321558475494385\n",
            "2.4123222827911377\n",
            "2.4021847248077393\n",
            "2.4608302116394043\n",
            "2.4090416431427\n",
            "2.419666290283203\n",
            "2.474661111831665\n",
            "2.6282103061676025\n",
            "2.4266281127929688\n",
            "2.3967795372009277\n",
            "2.5296432971954346\n",
            "2.451728105545044\n",
            "2.413590431213379\n",
            "2.4039456844329834\n",
            "2.3895978927612305\n",
            "2.5426290035247803\n",
            "2.4717109203338623\n",
            "2.5355584621429443\n",
            "2.4794578552246094\n",
            "2.4514944553375244\n",
            "2.5266435146331787\n",
            "2.4307219982147217\n",
            "2.4777398109436035\n",
            "2.2987918853759766\n",
            "2.4633190631866455\n",
            "2.535767078399658\n",
            "2.4629151821136475\n",
            "2.549049139022827\n",
            "2.4141783714294434\n",
            "2.341658353805542\n",
            "2.419529438018799\n",
            "2.456606149673462\n",
            "2.4699838161468506\n",
            "2.377821207046509\n",
            "2.4799957275390625\n",
            "2.4783334732055664\n",
            "2.498861074447632\n",
            "2.4339520931243896\n",
            "2.4023854732513428\n",
            "2.5368292331695557\n",
            "2.514946937561035\n",
            "2.398627996444702\n",
            "2.4202053546905518\n",
            "2.4388670921325684\n",
            "2.356308937072754\n",
            "2.537151336669922\n",
            "2.4131298065185547\n",
            "2.491812229156494\n",
            "2.562317132949829\n",
            "2.448223352432251\n",
            "2.3496007919311523\n",
            "2.455042839050293\n",
            "2.519326686859131\n",
            "2.61076283454895\n",
            "2.5190811157226562\n",
            "2.3848302364349365\n",
            "2.458245277404785\n",
            "2.4774601459503174\n",
            "2.2505736351013184\n",
            "2.4163143634796143\n",
            "2.507324457168579\n",
            "2.3569748401641846\n",
            "2.3958778381347656\n",
            "2.381575584411621\n",
            "2.4194247722625732\n",
            "2.5685195922851562\n",
            "2.5209274291992188\n",
            "2.40214204788208\n",
            "2.353295087814331\n",
            "2.4282071590423584\n",
            "2.350654125213623\n",
            "2.413451671600342\n",
            "2.4949774742126465\n",
            "2.326599359512329\n",
            "2.3523037433624268\n",
            "2.3674957752227783\n",
            "2.4960479736328125\n",
            "2.442481756210327\n",
            "2.472080707550049\n",
            "2.509293794631958\n",
            "2.46490216255188\n",
            "2.520963430404663\n",
            "2.3563456535339355\n",
            "2.588306427001953\n",
            "2.456859827041626\n",
            "2.441159725189209\n",
            "2.441995620727539\n",
            "2.3875300884246826\n",
            "2.414520263671875\n",
            "2.36495041847229\n",
            "2.390687942504883\n",
            "2.3814609050750732\n",
            "2.419597625732422\n",
            "2.4563236236572266\n",
            "2.5061419010162354\n",
            "2.3657073974609375\n",
            "2.5410261154174805\n",
            "2.5497636795043945\n",
            "2.377326488494873\n",
            "2.4507060050964355\n",
            "2.4488351345062256\n",
            "2.391683340072632\n",
            "2.286362409591675\n",
            "2.4760119915008545\n",
            "2.3414065837860107\n",
            "2.54312801361084\n",
            "2.4565346240997314\n",
            "2.3266615867614746\n",
            "2.4474456310272217\n",
            "2.4700183868408203\n",
            "2.536458969116211\n",
            "2.4535725116729736\n",
            "2.4136428833007812\n",
            "2.537679672241211\n",
            "2.426412343978882\n",
            "2.4405465126037598\n",
            "2.5086288452148438\n",
            "2.5053374767303467\n",
            "2.454897403717041\n",
            "2.4910426139831543\n",
            "2.5175013542175293\n",
            "2.547800302505493\n",
            "2.4722347259521484\n",
            "2.417524576187134\n",
            "2.4597725868225098\n",
            "2.423957109451294\n",
            "2.456752061843872\n",
            "2.500704526901245\n",
            "2.3542397022247314\n",
            "2.353283643722534\n",
            "2.4191477298736572\n",
            "2.5565683841705322\n",
            "2.491217851638794\n",
            "2.4567816257476807\n",
            "2.4412028789520264\n",
            "2.5281853675842285\n",
            "2.5239710807800293\n",
            "2.51298451423645\n",
            "2.4231953620910645\n",
            "2.31649112701416\n",
            "2.5200741291046143\n",
            "2.4233686923980713\n",
            "2.4887211322784424\n",
            "2.3861544132232666\n",
            "2.5329885482788086\n",
            "2.439542770385742\n",
            "2.4302103519439697\n",
            "2.503364324569702\n",
            "2.417508602142334\n",
            "2.537247896194458\n",
            "2.4445459842681885\n",
            "2.5625994205474854\n",
            "2.4295318126678467\n",
            "2.4510321617126465\n",
            "2.462968587875366\n",
            "2.5166263580322266\n",
            "2.370161294937134\n",
            "2.3417742252349854\n",
            "2.5544965267181396\n",
            "2.562082529067993\n",
            "2.403625965118408\n",
            "2.462740182876587\n",
            "2.458336353302002\n",
            "2.41363525390625\n",
            "2.4417917728424072\n",
            "2.439174175262451\n",
            "2.4474196434020996\n",
            "2.3718318939208984\n",
            "2.411611318588257\n",
            "2.404651165008545\n",
            "2.46126127243042\n",
            "2.4350433349609375\n",
            "2.4347481727600098\n",
            "2.308286428451538\n",
            "2.374560594558716\n",
            "2.5009965896606445\n",
            "2.5728132724761963\n",
            "2.4693148136138916\n",
            "2.430286407470703\n",
            "2.483532667160034\n",
            "2.50215744972229\n",
            "2.492992401123047\n",
            "2.4469432830810547\n",
            "2.440812349319458\n",
            "2.522486925125122\n",
            "2.603201389312744\n",
            "2.4984869956970215\n",
            "2.467115879058838\n",
            "2.414503574371338\n",
            "2.377983570098877\n",
            "2.3613297939300537\n",
            "2.434617519378662\n",
            "2.527785062789917\n",
            "2.58880615234375\n",
            "2.32889461517334\n",
            "2.571129083633423\n",
            "2.498826026916504\n",
            "2.439577341079712\n",
            "2.428838014602661\n",
            "2.4163122177124023\n",
            "2.373061180114746\n",
            "2.507286310195923\n",
            "2.557966470718384\n",
            "2.3915703296661377\n",
            "2.410409450531006\n",
            "2.438610792160034\n",
            "2.456397771835327\n",
            "2.445098638534546\n",
            "2.6060969829559326\n",
            "2.4939820766448975\n",
            "2.3454079627990723\n",
            "2.5852673053741455\n",
            "2.2825186252593994\n",
            "2.4090840816497803\n",
            "2.435316562652588\n",
            "2.5428075790405273\n",
            "2.4791035652160645\n",
            "2.446753740310669\n",
            "2.532027006149292\n",
            "2.255446434020996\n",
            "2.359306812286377\n",
            "2.382568836212158\n",
            "2.477452039718628\n",
            "2.6750500202178955\n",
            "2.527468204498291\n",
            "2.4839224815368652\n",
            "2.3048524856567383\n",
            "2.5178024768829346\n",
            "2.4227590560913086\n",
            "2.4173035621643066\n",
            "2.383700370788574\n",
            "2.488640785217285\n",
            "2.5276756286621094\n",
            "2.527892589569092\n",
            "2.370588541030884\n",
            "2.3109400272369385\n",
            "2.438347816467285\n",
            "2.476348876953125\n",
            "2.40351939201355\n",
            "2.4405529499053955\n",
            "2.4720499515533447\n",
            "2.4608869552612305\n",
            "2.3630266189575195\n",
            "2.476245164871216\n",
            "2.5290746688842773\n",
            "2.45253849029541\n",
            "2.434054136276245\n",
            "2.4523282051086426\n",
            "2.331085443496704\n",
            "2.403877019882202\n",
            "2.3695948123931885\n",
            "2.43955397605896\n",
            "2.4439611434936523\n",
            "2.4515879154205322\n",
            "2.4243814945220947\n",
            "2.4840054512023926\n",
            "2.5080039501190186\n",
            "2.4402999877929688\n",
            "2.4129488468170166\n",
            "2.3442065715789795\n",
            "2.3862462043762207\n",
            "2.470355987548828\n",
            "2.4429144859313965\n",
            "2.3571746349334717\n",
            "2.489373207092285\n",
            "2.3939027786254883\n",
            "2.3660831451416016\n",
            "2.2682642936706543\n",
            "2.399407148361206\n",
            "2.3707616329193115\n",
            "2.4773776531219482\n",
            "2.389418363571167\n",
            "2.5175557136535645\n",
            "2.4870471954345703\n",
            "2.416994571685791\n",
            "2.5114376544952393\n",
            "2.4047629833221436\n",
            "2.4327309131622314\n",
            "2.5633997917175293\n",
            "2.3235538005828857\n",
            "2.5993850231170654\n",
            "2.4388251304626465\n",
            "2.410590887069702\n",
            "2.3924901485443115\n",
            "2.4255645275115967\n",
            "2.4921107292175293\n",
            "2.403095245361328\n",
            "2.461716413497925\n",
            "2.4714882373809814\n",
            "2.44620680809021\n",
            "2.4708802700042725\n",
            "2.4626874923706055\n",
            "2.443821668624878\n",
            "2.4579415321350098\n",
            "2.464726448059082\n",
            "2.5112290382385254\n",
            "2.3680269718170166\n",
            "2.4219601154327393\n",
            "2.552119493484497\n",
            "2.6014132499694824\n",
            "2.3838467597961426\n",
            "2.4962871074676514\n",
            "2.367537498474121\n",
            "2.4588639736175537\n",
            "2.4386236667633057\n",
            "2.4821581840515137\n",
            "2.506805181503296\n",
            "2.489013671875\n",
            "2.5049359798431396\n",
            "2.5938732624053955\n",
            "2.469869613647461\n",
            "2.494067668914795\n",
            "2.4957408905029297\n",
            "2.6018226146698\n",
            "2.5775222778320312\n",
            "2.4517464637756348\n",
            "2.3722941875457764\n",
            "2.418318271636963\n",
            "2.4099502563476562\n",
            "2.4745795726776123\n",
            "2.3707168102264404\n",
            "2.4840588569641113\n",
            "2.3907968997955322\n",
            "2.3684394359588623\n",
            "2.5497219562530518\n",
            "2.4219274520874023\n",
            "2.527400016784668\n",
            "2.4866716861724854\n",
            "2.502056121826172\n",
            "2.490511417388916\n",
            "2.5666770935058594\n",
            "2.502082586288452\n",
            "2.368811845779419\n",
            "2.39249587059021\n",
            "2.4214324951171875\n",
            "2.439807176589966\n",
            "2.422901153564453\n",
            "2.504754066467285\n",
            "2.3899712562561035\n",
            "2.4142110347747803\n",
            "2.5140817165374756\n",
            "2.4428083896636963\n",
            "2.3507285118103027\n",
            "2.471717119216919\n",
            "2.425388813018799\n",
            "2.5490522384643555\n",
            "2.3500707149505615\n",
            "2.4932916164398193\n",
            "2.4873738288879395\n",
            "2.528108596801758\n",
            "2.478400945663452\n",
            "2.4200594425201416\n",
            "2.4363179206848145\n",
            "2.385038137435913\n",
            "2.3964710235595703\n",
            "2.4622459411621094\n",
            "2.3696448802948\n",
            "2.3310132026672363\n",
            "2.485964775085449\n",
            "2.4617481231689453\n",
            "2.4621174335479736\n",
            "2.524839401245117\n",
            "2.3751697540283203\n",
            "2.3845298290252686\n",
            "2.552006959915161\n",
            "2.405775308609009\n",
            "2.524503707885742\n",
            "2.3418307304382324\n",
            "2.3572161197662354\n",
            "2.6004767417907715\n",
            "2.430875301361084\n",
            "2.329427480697632\n",
            "2.3882429599761963\n",
            "2.4181125164031982\n",
            "2.4122202396392822\n",
            "2.48209285736084\n",
            "2.5261619091033936\n",
            "2.455995559692383\n",
            "2.419140338897705\n",
            "2.3465330600738525\n",
            "2.614565372467041\n",
            "2.3791275024414062\n",
            "2.4366676807403564\n",
            "2.340322256088257\n",
            "2.5039613246917725\n",
            "2.3123960494995117\n",
            "2.4886093139648438\n",
            "2.4095003604888916\n",
            "2.3829116821289062\n",
            "2.4517812728881836\n",
            "2.5195770263671875\n",
            "2.464416027069092\n",
            "2.501222848892212\n",
            "2.489189863204956\n",
            "2.4664387702941895\n",
            "2.3904378414154053\n",
            "2.400268077850342\n",
            "2.5949063301086426\n",
            "2.352964162826538\n",
            "2.587310314178467\n",
            "2.4730355739593506\n",
            "2.4832675457000732\n",
            "2.4712002277374268\n",
            "2.5845789909362793\n",
            "2.3817617893218994\n",
            "2.4697556495666504\n",
            "2.4112510681152344\n",
            "2.504727602005005\n",
            "2.397888422012329\n",
            "2.4050655364990234\n",
            "2.5525014400482178\n",
            "2.3928327560424805\n",
            "2.5427427291870117\n",
            "2.4635555744171143\n",
            "2.5091640949249268\n",
            "2.376549005508423\n",
            "2.459620237350464\n",
            "2.3728017807006836\n",
            "2.365907907485962\n",
            "2.4592342376708984\n",
            "2.4124248027801514\n",
            "2.311769962310791\n",
            "2.486081600189209\n",
            "2.438814163208008\n",
            "2.4565625190734863\n",
            "2.4796245098114014\n",
            "2.394314765930176\n",
            "2.423903703689575\n",
            "2.513803243637085\n",
            "2.5423154830932617\n",
            "2.494433641433716\n",
            "2.425297498703003\n",
            "2.50573992729187\n",
            "2.470501184463501\n",
            "2.443763256072998\n",
            "2.48826265335083\n",
            "2.5495967864990234\n",
            "2.5052926540374756\n",
            "2.388796091079712\n",
            "2.566263437271118\n",
            "2.4300880432128906\n",
            "2.538701295852661\n",
            "2.4124813079833984\n",
            "2.33725905418396\n",
            "2.4521119594573975\n",
            "2.49421763420105\n",
            "2.4694266319274902\n",
            "2.40905499458313\n",
            "2.54372239112854\n",
            "2.5646374225616455\n",
            "2.463486909866333\n",
            "2.277273178100586\n",
            "2.5924429893493652\n",
            "2.5084025859832764\n",
            "2.384124994277954\n",
            "2.382246255874634\n",
            "2.3658246994018555\n",
            "2.5265872478485107\n",
            "2.351012706756592\n",
            "2.4522359371185303\n",
            "2.389014720916748\n",
            "2.4630701541900635\n",
            "2.4258627891540527\n",
            "2.4111366271972656\n",
            "2.4016971588134766\n",
            "2.4281458854675293\n",
            "2.3824923038482666\n",
            "2.3368616104125977\n",
            "2.567906379699707\n",
            "2.448911190032959\n",
            "2.3536930084228516\n",
            "2.4137959480285645\n",
            "2.384976625442505\n",
            "2.480994939804077\n",
            "2.36558198928833\n",
            "2.4664323329925537\n",
            "2.4336559772491455\n",
            "2.51298189163208\n",
            "2.364091157913208\n",
            "2.4147050380706787\n",
            "2.4779458045959473\n",
            "2.41740083694458\n",
            "2.426666021347046\n",
            "2.4491677284240723\n",
            "2.483607292175293\n",
            "2.457707405090332\n",
            "2.546762704849243\n",
            "2.5337157249450684\n",
            "2.5625360012054443\n",
            "2.432628870010376\n",
            "2.53694486618042\n",
            "2.428197145462036\n",
            "2.329699993133545\n",
            "2.5711545944213867\n",
            "2.3868021965026855\n",
            "2.4225144386291504\n",
            "2.4488418102264404\n",
            "2.468348979949951\n",
            "2.4696078300476074\n",
            "2.461233377456665\n",
            "2.500929117202759\n",
            "2.490482807159424\n",
            "2.379833459854126\n",
            "2.385205030441284\n",
            "2.385471820831299\n",
            "2.500319242477417\n",
            "2.456946611404419\n",
            "2.3984439373016357\n",
            "2.510070562362671\n",
            "2.4419972896575928\n",
            "2.352837562561035\n",
            "2.4503684043884277\n",
            "2.4032487869262695\n",
            "2.5032520294189453\n",
            "2.477077007293701\n",
            "2.3946335315704346\n",
            "2.3790199756622314\n",
            "2.3771119117736816\n",
            "2.373356819152832\n",
            "2.3785135746002197\n",
            "2.4973747730255127\n",
            "2.4106478691101074\n",
            "2.4404358863830566\n",
            "2.509068012237549\n",
            "2.339134693145752\n",
            "2.414766311645508\n",
            "2.4809603691101074\n",
            "2.6071202754974365\n",
            "2.4420173168182373\n",
            "2.417299509048462\n",
            "2.508392095565796\n",
            "2.3860838413238525\n",
            "2.3131308555603027\n",
            "2.4717817306518555\n",
            "2.384535789489746\n",
            "2.4391674995422363\n",
            "2.4584531784057617\n",
            "2.473283052444458\n",
            "2.3742265701293945\n",
            "2.4852778911590576\n",
            "2.381150007247925\n",
            "2.3687021732330322\n",
            "2.408975601196289\n",
            "2.4552106857299805\n",
            "2.507040500640869\n",
            "2.469444751739502\n",
            "2.604144811630249\n",
            "2.3719000816345215\n",
            "2.5087838172912598\n",
            "2.6008718013763428\n",
            "2.5279266834259033\n",
            "2.455415725708008\n",
            "2.4620425701141357\n",
            "2.673994302749634\n",
            "2.611832857131958\n",
            "2.4449496269226074\n",
            "2.4673261642456055\n",
            "2.532562255859375\n",
            "2.4834253787994385\n",
            "2.4519896507263184\n",
            "2.4950075149536133\n",
            "2.445657968521118\n",
            "2.5007596015930176\n",
            "2.351940393447876\n",
            "2.468245267868042\n",
            "2.5227696895599365\n",
            "2.6003315448760986\n",
            "2.3344380855560303\n",
            "2.415306806564331\n",
            "2.637274980545044\n",
            "2.3987817764282227\n",
            "2.3330304622650146\n",
            "2.4805753231048584\n",
            "2.383638620376587\n",
            "2.564002752304077\n",
            "2.504579544067383\n",
            "2.4724645614624023\n",
            "2.4409592151641846\n",
            "2.4532663822174072\n",
            "2.585049629211426\n",
            "2.485057830810547\n",
            "2.547305107116699\n",
            "2.5163867473602295\n",
            "2.34334135055542\n",
            "2.414337158203125\n",
            "2.4776272773742676\n",
            "2.50791597366333\n",
            "2.423264980316162\n",
            "2.4240806102752686\n",
            "2.434075355529785\n",
            "2.450516939163208\n",
            "2.3084144592285156\n",
            "2.3277511596679688\n",
            "2.648513078689575\n",
            "2.486693859100342\n",
            "2.4613704681396484\n",
            "2.5088257789611816\n",
            "2.5744497776031494\n",
            "2.4285898208618164\n",
            "2.450423240661621\n",
            "2.4390738010406494\n",
            "2.4075942039489746\n",
            "2.5860564708709717\n",
            "2.4332854747772217\n",
            "2.5574328899383545\n",
            "2.469116687774658\n",
            "2.4511046409606934\n",
            "2.485708713531494\n",
            "2.50248122215271\n",
            "2.4692280292510986\n",
            "2.4712538719177246\n",
            "2.355067491531372\n",
            "2.445772886276245\n",
            "2.3285861015319824\n",
            "2.5879223346710205\n",
            "2.42452073097229\n",
            "2.453007698059082\n",
            "2.400320291519165\n",
            "2.3322970867156982\n",
            "2.411594867706299\n",
            "2.48119854927063\n",
            "2.442559003829956\n",
            "2.4691710472106934\n",
            "2.5864875316619873\n",
            "2.4356157779693604\n",
            "2.5036027431488037\n",
            "2.466062545776367\n",
            "2.490114450454712\n",
            "2.5374996662139893\n",
            "2.5027823448181152\n",
            "2.4739086627960205\n",
            "2.5577855110168457\n",
            "2.417752265930176\n",
            "2.3883326053619385\n",
            "2.4615774154663086\n",
            "2.3875160217285156\n",
            "2.5005710124969482\n",
            "2.383368730545044\n",
            "2.4245357513427734\n",
            "2.4864861965179443\n",
            "2.4446303844451904\n",
            "2.5397756099700928\n",
            "2.5199973583221436\n",
            "2.4998650550842285\n",
            "2.4227685928344727\n",
            "2.4965193271636963\n",
            "2.5270116329193115\n",
            "2.5259952545166016\n",
            "2.44437837600708\n",
            "2.5698776245117188\n",
            "2.5234503746032715\n",
            "2.4273502826690674\n",
            "2.511289358139038\n",
            "2.4349944591522217\n",
            "2.532172918319702\n",
            "2.424893856048584\n",
            "2.336850643157959\n",
            "2.2615277767181396\n",
            "2.4206838607788086\n",
            "2.483543634414673\n",
            "2.3303544521331787\n",
            "2.659142017364502\n",
            "2.395379066467285\n",
            "2.370673418045044\n",
            "2.4405596256256104\n",
            "2.4361331462860107\n",
            "2.4288933277130127\n",
            "2.5035481452941895\n",
            "2.541882276535034\n",
            "2.4896621704101562\n",
            "2.5485758781433105\n",
            "2.4601473808288574\n",
            "2.3898301124572754\n",
            "2.503061532974243\n",
            "2.4040257930755615\n",
            "2.37707781791687\n",
            "2.4671449661254883\n",
            "2.4785940647125244\n",
            "2.489677667617798\n",
            "2.4117212295532227\n",
            "2.608701467514038\n",
            "2.475566864013672\n",
            "2.4661874771118164\n",
            "2.478334665298462\n",
            "2.5040090084075928\n",
            "2.5148775577545166\n",
            "2.4405357837677\n",
            "2.3977115154266357\n",
            "2.487943410873413\n",
            "2.562488079071045\n",
            "2.538562536239624\n",
            "2.3676865100860596\n",
            "2.457549571990967\n",
            "2.4112894535064697\n",
            "2.5992870330810547\n",
            "2.3815767765045166\n",
            "2.3933002948760986\n",
            "2.413682699203491\n",
            "2.474918842315674\n",
            "2.3652374744415283\n",
            "2.3695805072784424\n",
            "2.472914218902588\n",
            "2.3394973278045654\n",
            "2.500464916229248\n",
            "2.48844575881958\n",
            "2.4743869304656982\n",
            "2.4551377296447754\n",
            "2.4629054069519043\n",
            "2.52980899810791\n",
            "2.473625421524048\n",
            "2.4218521118164062\n",
            "2.518522024154663\n",
            "2.4503085613250732\n",
            "2.434636116027832\n",
            "2.4411399364471436\n",
            "2.505145788192749\n",
            "2.4800491333007812\n",
            "2.595618963241577\n",
            "2.4069769382476807\n",
            "2.5033175945281982\n",
            "2.467052936553955\n",
            "2.452444553375244\n",
            "2.376046895980835\n",
            "2.4674551486968994\n",
            "2.443667411804199\n",
            "2.3704447746276855\n",
            "2.40873646736145\n",
            "2.445103645324707\n",
            "2.4122700691223145\n",
            "2.5234336853027344\n",
            "2.4883997440338135\n",
            "2.2376394271850586\n",
            "2.457489013671875\n",
            "2.4774763584136963\n",
            "2.545945167541504\n",
            "2.4875669479370117\n",
            "2.37776780128479\n",
            "2.495387315750122\n",
            "2.5429322719573975\n",
            "2.45827317237854\n",
            "2.3736987113952637\n",
            "2.464473009109497\n",
            "2.39780855178833\n",
            "2.4374849796295166\n",
            "2.3930304050445557\n",
            "2.309046506881714\n",
            "2.36259388923645\n",
            "2.4445641040802\n",
            "2.4761343002319336\n",
            "2.549790382385254\n",
            "2.433321952819824\n",
            "2.519366502761841\n",
            "2.399071216583252\n",
            "2.346419334411621\n",
            "2.5408272743225098\n",
            "2.354365110397339\n",
            "2.5064046382904053\n",
            "2.547220230102539\n",
            "2.5260815620422363\n",
            "2.5167396068573\n",
            "2.5161259174346924\n",
            "2.5336523056030273\n",
            "2.4732656478881836\n",
            "2.4374773502349854\n",
            "2.475703716278076\n",
            "2.5450186729431152\n",
            "2.4211645126342773\n",
            "2.4651689529418945\n",
            "2.5065085887908936\n",
            "2.495389223098755\n",
            "2.458939790725708\n",
            "2.4883062839508057\n",
            "2.4776663780212402\n",
            "2.4261105060577393\n",
            "2.4287989139556885\n",
            "2.520005702972412\n",
            "2.3969156742095947\n",
            "2.573650360107422\n",
            "2.5515332221984863\n",
            "2.5410783290863037\n",
            "2.4340245723724365\n",
            "2.4016458988189697\n",
            "2.476388454437256\n",
            "2.5291104316711426\n",
            "2.4146568775177\n",
            "2.577970266342163\n",
            "2.466212749481201\n",
            "2.5622804164886475\n",
            "2.41579532623291\n",
            "2.539804697036743\n",
            "2.4782159328460693\n",
            "2.364351749420166\n",
            "2.3417716026306152\n",
            "2.4001288414001465\n",
            "2.467170476913452\n",
            "2.466381311416626\n",
            "2.470095157623291\n",
            "2.436006546020508\n",
            "2.505096673965454\n",
            "2.4338996410369873\n",
            "2.4879393577575684\n",
            "2.4347591400146484\n",
            "2.3610305786132812\n",
            "2.4681015014648438\n",
            "2.452362298965454\n",
            "2.4950673580169678\n",
            "2.432912826538086\n",
            "2.4389209747314453\n",
            "2.5556130409240723\n",
            "2.461517333984375\n",
            "2.5426459312438965\n",
            "2.454693555831909\n",
            "2.523285388946533\n",
            "2.555788278579712\n",
            "2.531968832015991\n",
            "2.473097324371338\n",
            "2.4371659755706787\n",
            "2.4295458793640137\n",
            "2.4561166763305664\n",
            "2.396390676498413\n",
            "2.5984842777252197\n",
            "2.3112974166870117\n",
            "2.4336721897125244\n",
            "2.3819918632507324\n",
            "2.430405378341675\n",
            "2.381272792816162\n",
            "2.4130070209503174\n",
            "2.416194200515747\n",
            "2.3866958618164062\n",
            "2.4858298301696777\n",
            "2.3096706867218018\n",
            "2.513871192932129\n",
            "2.501126527786255\n",
            "2.3981592655181885\n",
            "2.5394375324249268\n",
            "2.588932752609253\n",
            "2.4237194061279297\n",
            "2.507667303085327\n",
            "2.490774393081665\n",
            "2.3769826889038086\n",
            "2.46419358253479\n",
            "2.440394163131714\n",
            "2.263026237487793\n",
            "2.4084603786468506\n",
            "2.504976749420166\n",
            "2.471101999282837\n",
            "2.519061326980591\n",
            "2.5153534412384033\n",
            "2.43190336227417\n",
            "2.3858561515808105\n",
            "2.5424017906188965\n",
            "2.5525271892547607\n",
            "2.58612322807312\n",
            "2.4966351985931396\n",
            "2.479174852371216\n",
            "2.401108980178833\n",
            "2.478156089782715\n",
            "2.4805006980895996\n",
            "2.524043321609497\n",
            "2.3677031993865967\n",
            "2.3720626831054688\n",
            "2.502579927444458\n",
            "2.511831760406494\n",
            "2.4806597232818604\n",
            "2.4854307174682617\n",
            "2.461574077606201\n",
            "2.4861578941345215\n",
            "2.5161330699920654\n",
            "2.4362597465515137\n",
            "2.4549856185913086\n",
            "2.472210168838501\n",
            "2.40803599357605\n",
            "2.535900115966797\n",
            "2.3333044052124023\n",
            "2.4162051677703857\n",
            "2.3449161052703857\n",
            "2.492147445678711\n",
            "2.513706922531128\n",
            "2.4945244789123535\n",
            "2.6514151096343994\n",
            "2.3740646839141846\n",
            "2.4912686347961426\n",
            "2.4740548133850098\n",
            "2.4191102981567383\n",
            "2.252958297729492\n",
            "2.402878999710083\n",
            "2.4694507122039795\n",
            "2.5498106479644775\n",
            "2.5040102005004883\n",
            "2.438105821609497\n",
            "2.6141228675842285\n",
            "2.468163251876831\n",
            "2.4305808544158936\n",
            "2.3842520713806152\n",
            "2.385382890701294\n",
            "2.559713363647461\n",
            "2.381133556365967\n",
            "2.4414849281311035\n",
            "2.405851364135742\n",
            "2.434502124786377\n",
            "2.495988607406616\n",
            "2.40372896194458\n",
            "2.398043632507324\n",
            "2.48451566696167\n",
            "2.5265955924987793\n",
            "2.418627977371216\n",
            "2.5014209747314453\n",
            "2.360410690307617\n",
            "2.366774797439575\n",
            "2.5579442977905273\n",
            "2.3535215854644775\n",
            "2.4689135551452637\n",
            "2.510181427001953\n",
            "2.456489086151123\n",
            "2.34238338470459\n",
            "2.412400960922241\n",
            "2.375378370285034\n",
            "2.428825855255127\n",
            "2.445120096206665\n",
            "2.480607271194458\n",
            "2.3966267108917236\n",
            "2.40822434425354\n",
            "2.468374013900757\n",
            "2.3529274463653564\n",
            "2.4799082279205322\n",
            "2.383523941040039\n",
            "2.4749741554260254\n",
            "2.5196080207824707\n",
            "2.4544553756713867\n",
            "2.477116346359253\n",
            "2.5472381114959717\n",
            "2.3956265449523926\n",
            "2.3582606315612793\n",
            "2.502984046936035\n",
            "2.4187746047973633\n",
            "2.3805298805236816\n",
            "2.320451498031616\n",
            "2.306152820587158\n",
            "2.3707964420318604\n",
            "2.4192516803741455\n",
            "2.4466471672058105\n",
            "2.6293575763702393\n",
            "2.3992555141448975\n",
            "2.4902801513671875\n",
            "2.6092612743377686\n",
            "2.501349449157715\n",
            "2.5214314460754395\n",
            "2.5245909690856934\n",
            "2.2956931591033936\n",
            "2.4458160400390625\n",
            "2.4392991065979004\n",
            "2.3582897186279297\n",
            "2.4791271686553955\n",
            "2.3778610229492188\n",
            "2.363434314727783\n",
            "2.539367198944092\n",
            "2.3790013790130615\n",
            "2.4666552543640137\n",
            "2.633697748184204\n",
            "2.5393970012664795\n",
            "2.4455628395080566\n",
            "2.5582430362701416\n",
            "2.4566171169281006\n",
            "2.5962073802948\n",
            "2.4710073471069336\n",
            "2.473527431488037\n",
            "2.5379951000213623\n",
            "2.4879579544067383\n",
            "2.42050838470459\n",
            "2.4218204021453857\n",
            "2.474780559539795\n",
            "2.5125720500946045\n",
            "2.4882991313934326\n",
            "2.458294153213501\n",
            "2.487257242202759\n",
            "2.4005308151245117\n",
            "2.4749207496643066\n",
            "2.2782363891601562\n",
            "2.389531135559082\n",
            "2.4107656478881836\n",
            "2.561525583267212\n",
            "2.51475191116333\n",
            "2.455258369445801\n",
            "2.426439046859741\n",
            "2.476484537124634\n",
            "2.410538911819458\n",
            "2.616792678833008\n",
            "2.455015182495117\n",
            "2.391735792160034\n",
            "2.4638400077819824\n",
            "2.4850809574127197\n",
            "2.4372668266296387\n",
            "2.450198173522949\n",
            "2.3930304050445557\n",
            "2.556978464126587\n",
            "2.5073952674865723\n",
            "2.4994308948516846\n",
            "2.454707622528076\n",
            "2.3625540733337402\n",
            "2.442758798599243\n",
            "2.3966569900512695\n",
            "2.4083759784698486\n",
            "2.3488597869873047\n",
            "2.4020886421203613\n",
            "2.473524808883667\n",
            "2.44946551322937\n",
            "2.41378116607666\n",
            "2.3702573776245117\n",
            "2.5588538646698\n",
            "2.3216471672058105\n",
            "2.3883678913116455\n",
            "2.562589406967163\n",
            "2.426974058151245\n",
            "2.4744341373443604\n",
            "2.3867697715759277\n",
            "2.363344669342041\n",
            "2.4913721084594727\n",
            "2.394000768661499\n",
            "2.464106559753418\n",
            "2.5114235877990723\n",
            "2.543386697769165\n",
            "2.2957394123077393\n",
            "2.445530891418457\n",
            "2.4306578636169434\n",
            "2.51299786567688\n",
            "2.431011199951172\n",
            "2.3302249908447266\n",
            "2.414715528488159\n",
            "2.3309109210968018\n",
            "2.289018154144287\n",
            "2.3041622638702393\n",
            "2.4574930667877197\n",
            "2.424968957901001\n",
            "2.4422807693481445\n",
            "2.5715503692626953\n",
            "2.410959482192993\n",
            "2.280689001083374\n",
            "2.453464984893799\n",
            "2.3956549167633057\n",
            "2.6079025268554688\n",
            "2.4531712532043457\n",
            "2.3378524780273438\n",
            "2.4447293281555176\n",
            "2.5018529891967773\n",
            "2.381089925765991\n",
            "2.4611306190490723\n",
            "2.5281896591186523\n",
            "2.4262759685516357\n",
            "2.4753754138946533\n",
            "2.4988555908203125\n",
            "2.4589433670043945\n",
            "2.506450891494751\n",
            "2.4776217937469482\n",
            "2.4686009883880615\n",
            "2.4321579933166504\n",
            "2.5539069175720215\n",
            "2.498439073562622\n",
            "2.3716063499450684\n",
            "2.508054494857788\n",
            "2.32963228225708\n",
            "2.3353941440582275\n",
            "2.407681465148926\n",
            "2.4622347354888916\n",
            "2.469388723373413\n",
            "2.460667371749878\n",
            "2.3759801387786865\n",
            "2.4228458404541016\n",
            "2.4956703186035156\n",
            "2.4838547706604004\n",
            "2.3083114624023438\n",
            "2.4767494201660156\n",
            "2.4642255306243896\n",
            "2.608635663986206\n",
            "2.5176565647125244\n",
            "2.35771107673645\n",
            "2.4601008892059326\n",
            "2.5053908824920654\n",
            "2.349627733230591\n",
            "2.271469831466675\n",
            "2.3597710132598877\n",
            "2.585444688796997\n",
            "2.3830361366271973\n",
            "2.3643577098846436\n",
            "2.33333683013916\n",
            "2.3787782192230225\n",
            "2.501265525817871\n",
            "2.3509979248046875\n",
            "2.3975632190704346\n",
            "2.580632448196411\n",
            "2.4933812618255615\n",
            "2.4070496559143066\n",
            "2.5111889839172363\n",
            "2.446197032928467\n",
            "2.453238010406494\n",
            "2.5595970153808594\n",
            "2.440173864364624\n",
            "2.4015679359436035\n",
            "2.431819438934326\n",
            "2.4627349376678467\n",
            "2.297797679901123\n",
            "2.448293924331665\n",
            "2.3488547801971436\n",
            "2.393209218978882\n",
            "2.328775405883789\n",
            "2.436049699783325\n",
            "2.4338366985321045\n",
            "2.3496580123901367\n",
            "2.4687998294830322\n",
            "2.366698741912842\n",
            "2.342594861984253\n",
            "2.4729692935943604\n",
            "2.5334153175354004\n",
            "2.474470615386963\n",
            "2.5221967697143555\n",
            "2.5195040702819824\n",
            "2.441427230834961\n",
            "2.4421584606170654\n",
            "2.3827905654907227\n",
            "2.353663682937622\n",
            "2.4204208850860596\n",
            "2.5387394428253174\n",
            "2.504926919937134\n",
            "2.3846240043640137\n",
            "2.5409581661224365\n",
            "2.531355619430542\n",
            "2.4898626804351807\n",
            "2.320692777633667\n",
            "2.6039927005767822\n",
            "2.484905242919922\n",
            "2.484860420227051\n",
            "2.5107696056365967\n",
            "2.3067426681518555\n",
            "2.506639242172241\n",
            "2.519463300704956\n",
            "2.497128963470459\n",
            "2.482105255126953\n",
            "2.425530433654785\n",
            "2.5042543411254883\n",
            "2.4269537925720215\n",
            "2.533200979232788\n",
            "2.4614148139953613\n",
            "2.453890562057495\n",
            "2.4356777667999268\n",
            "2.460742473602295\n",
            "2.578522205352783\n",
            "2.483428955078125\n",
            "2.413005828857422\n",
            "2.480182647705078\n",
            "2.425905466079712\n",
            "2.5229103565216064\n",
            "2.4292192459106445\n",
            "2.4959166049957275\n",
            "2.432873249053955\n",
            "2.472317695617676\n",
            "2.534306049346924\n",
            "2.3925390243530273\n",
            "2.3740928173065186\n",
            "2.4621963500976562\n",
            "2.4598498344421387\n",
            "2.424386501312256\n",
            "2.484240770339966\n",
            "2.3550093173980713\n",
            "2.4164068698883057\n",
            "2.4236044883728027\n",
            "2.421599864959717\n",
            "2.4038009643554688\n",
            "2.3899648189544678\n",
            "2.423293113708496\n",
            "2.3731350898742676\n",
            "2.375594139099121\n",
            "2.5771560668945312\n",
            "2.5622260570526123\n",
            "2.4879446029663086\n",
            "2.3657538890838623\n",
            "2.3065314292907715\n",
            "2.475712537765503\n",
            "2.4133036136627197\n",
            "2.487903594970703\n",
            "2.4714951515197754\n",
            "2.3151068687438965\n",
            "2.3933000564575195\n",
            "2.418335199356079\n",
            "2.352832078933716\n",
            "2.525710105895996\n",
            "2.514979839324951\n",
            "2.3834753036499023\n",
            "2.5512828826904297\n",
            "2.382573366165161\n",
            "2.381035804748535\n",
            "2.2858431339263916\n",
            "2.3949286937713623\n",
            "2.3919904232025146\n",
            "2.4329819679260254\n",
            "2.4058709144592285\n",
            "2.5482802391052246\n",
            "2.2680070400238037\n",
            "2.4052765369415283\n",
            "2.542907238006592\n",
            "2.4730043411254883\n",
            "2.454775094985962\n",
            "2.4608614444732666\n",
            "2.4375126361846924\n",
            "2.4565978050231934\n",
            "2.3877530097961426\n",
            "2.4622139930725098\n",
            "2.528965950012207\n",
            "2.465073585510254\n",
            "2.5448758602142334\n",
            "2.503070831298828\n",
            "2.5441653728485107\n",
            "2.481292247772217\n",
            "2.586251735687256\n",
            "2.3437979221343994\n",
            "2.4039723873138428\n",
            "2.4243476390838623\n",
            "2.347017765045166\n",
            "2.3614614009857178\n",
            "2.4732072353363037\n",
            "2.53076171875\n",
            "2.401484251022339\n",
            "2.545097827911377\n",
            "2.498047113418579\n",
            "2.3119685649871826\n",
            "2.329041004180908\n",
            "2.482680559158325\n",
            "2.3204469680786133\n",
            "2.4559075832366943\n",
            "2.4019176959991455\n",
            "2.448040246963501\n",
            "2.4437105655670166\n",
            "2.4736135005950928\n",
            "2.541815757751465\n",
            "2.5791242122650146\n",
            "2.4448628425598145\n",
            "2.4808337688446045\n",
            "2.499018669128418\n",
            "2.4031825065612793\n",
            "2.4321653842926025\n",
            "2.4935011863708496\n",
            "2.3819026947021484\n",
            "2.60522198677063\n",
            "2.5109305381774902\n",
            "2.5239455699920654\n",
            "2.4732775688171387\n",
            "2.3854751586914062\n",
            "2.5042929649353027\n",
            "2.249419689178467\n",
            "2.3477396965026855\n",
            "2.567531108856201\n",
            "2.559490919113159\n",
            "2.2890124320983887\n",
            "2.3946263790130615\n",
            "2.4328806400299072\n",
            "2.5731050968170166\n",
            "2.386519432067871\n",
            "2.409642457962036\n",
            "2.356219530105591\n",
            "2.446030855178833\n",
            "2.4969966411590576\n",
            "2.553173780441284\n",
            "2.617568254470825\n",
            "2.3770720958709717\n",
            "2.4101057052612305\n",
            "2.4900765419006348\n",
            "2.402305841445923\n",
            "2.5373620986938477\n",
            "2.5752077102661133\n",
            "2.478142738342285\n",
            "2.347649097442627\n",
            "2.597994804382324\n",
            "2.5993432998657227\n",
            "2.5461416244506836\n",
            "2.474956512451172\n",
            "2.4788832664489746\n",
            "2.323542833328247\n",
            "2.547330617904663\n",
            "2.4265146255493164\n",
            "2.4157776832580566\n",
            "2.514230966567993\n",
            "2.4086291790008545\n",
            "2.4548089504241943\n",
            "2.4235310554504395\n",
            "2.436002016067505\n",
            "2.6210825443267822\n",
            "2.489409923553467\n",
            "2.472519874572754\n",
            "2.3673250675201416\n",
            "2.4855403900146484\n",
            "2.4098305702209473\n",
            "2.3863372802734375\n",
            "2.395460367202759\n",
            "2.32515811920166\n",
            "2.4179465770721436\n",
            "2.4638731479644775\n",
            "2.500833511352539\n",
            "2.6095898151397705\n",
            "2.5484328269958496\n",
            "2.4643821716308594\n",
            "2.3983378410339355\n",
            "2.436946392059326\n",
            "2.4503538608551025\n",
            "2.604889392852783\n",
            "2.5805389881134033\n",
            "2.4610469341278076\n",
            "2.3552727699279785\n",
            "2.3162810802459717\n",
            "2.5703558921813965\n",
            "2.3808376789093018\n",
            "2.4345600605010986\n",
            "2.459019422531128\n",
            "2.397897958755493\n",
            "2.4637248516082764\n",
            "2.350407838821411\n",
            "2.43615460395813\n",
            "2.503696918487549\n",
            "2.475782871246338\n",
            "2.5118539333343506\n",
            "2.412307024002075\n",
            "2.498788356781006\n",
            "2.529219150543213\n",
            "2.390237808227539\n",
            "2.437150716781616\n",
            "2.5060861110687256\n",
            "2.479918956756592\n",
            "2.3381519317626953\n",
            "2.4346630573272705\n",
            "2.4994094371795654\n",
            "2.405344009399414\n",
            "2.3792881965637207\n",
            "2.4375786781311035\n",
            "2.4239323139190674\n",
            "2.3973145484924316\n",
            "2.5223448276519775\n",
            "2.3184850215911865\n",
            "2.5320446491241455\n",
            "2.514681577682495\n",
            "2.37756085395813\n",
            "2.4145941734313965\n",
            "2.6065354347229004\n",
            "2.356618642807007\n",
            "2.559141159057617\n",
            "2.4999217987060547\n",
            "2.4298081398010254\n",
            "2.533205986022949\n",
            "2.4005799293518066\n",
            "2.4768126010894775\n",
            "2.452208995819092\n",
            "2.5188233852386475\n",
            "2.416569948196411\n",
            "2.572465658187866\n",
            "2.538983106613159\n",
            "2.4747774600982666\n",
            "2.536264419555664\n",
            "2.4299676418304443\n",
            "2.317511558532715\n",
            "2.3863604068756104\n",
            "2.4525907039642334\n",
            "2.450880765914917\n",
            "2.5457799434661865\n",
            "2.457083225250244\n",
            "2.4637339115142822\n",
            "2.398790121078491\n",
            "2.4493467807769775\n",
            "2.346273183822632\n",
            "2.5469000339508057\n",
            "2.361739158630371\n",
            "2.5030267238616943\n",
            "2.412764310836792\n",
            "2.4395785331726074\n",
            "2.413933753967285\n",
            "2.4744417667388916\n",
            "2.3488686084747314\n",
            "2.45231556892395\n",
            "2.510617971420288\n",
            "2.505157470703125\n",
            "2.4421300888061523\n",
            "2.4927258491516113\n",
            "2.521665334701538\n",
            "2.481565237045288\n",
            "2.4280929565429688\n",
            "2.4288229942321777\n",
            "2.4593029022216797\n",
            "2.4828622341156006\n",
            "2.5086586475372314\n",
            "2.5414419174194336\n",
            "2.4024322032928467\n",
            "2.5978593826293945\n",
            "2.5067825317382812\n",
            "2.5082180500030518\n",
            "2.496882200241089\n",
            "2.4041402339935303\n",
            "2.4794070720672607\n",
            "2.406747341156006\n",
            "2.4358036518096924\n",
            "2.3835811614990234\n",
            "2.552151679992676\n",
            "2.485651731491089\n",
            "2.4690864086151123\n",
            "2.3948137760162354\n",
            "2.504687547683716\n",
            "2.5556678771972656\n",
            "2.5325021743774414\n",
            "2.5556135177612305\n",
            "2.475036144256592\n",
            "2.442094564437866\n",
            "2.4878525733947754\n",
            "2.5322604179382324\n",
            "2.381782293319702\n",
            "2.3578755855560303\n",
            "2.3843374252319336\n",
            "2.4673876762390137\n",
            "2.527162551879883\n",
            "2.509054660797119\n",
            "2.4062082767486572\n",
            "2.5173468589782715\n",
            "2.4228286743164062\n",
            "2.3336374759674072\n",
            "2.473203659057617\n",
            "2.4791884422302246\n",
            "2.5707945823669434\n",
            "2.357677459716797\n",
            "2.3646697998046875\n",
            "2.442559242248535\n",
            "2.578591823577881\n",
            "2.4906506538391113\n",
            "2.3707873821258545\n",
            "2.492460250854492\n",
            "2.4448227882385254\n",
            "2.3810958862304688\n",
            "2.47471284866333\n",
            "2.4432194232940674\n",
            "2.5397772789001465\n",
            "2.458826780319214\n",
            "2.5034775733947754\n",
            "2.4057564735412598\n",
            "2.6281323432922363\n",
            "2.574594020843506\n",
            "2.394920825958252\n",
            "2.3731515407562256\n",
            "2.4143998622894287\n",
            "2.45477557182312\n",
            "2.626499891281128\n",
            "2.3811919689178467\n",
            "2.4881961345672607\n",
            "2.4456260204315186\n",
            "2.354674816131592\n",
            "2.466996908187866\n",
            "2.3659427165985107\n",
            "2.4728994369506836\n",
            "2.413217306137085\n",
            "2.429978370666504\n",
            "2.438260555267334\n",
            "2.5866196155548096\n",
            "2.571626663208008\n",
            "2.472327470779419\n",
            "2.5557355880737305\n",
            "2.441525936126709\n",
            "2.42154860496521\n",
            "2.390444278717041\n",
            "2.4625961780548096\n",
            "2.482247829437256\n",
            "2.3807811737060547\n",
            "2.3800437450408936\n",
            "2.4754538536071777\n",
            "2.4250290393829346\n",
            "2.540611982345581\n",
            "2.4273855686187744\n",
            "2.5206947326660156\n",
            "2.405838966369629\n",
            "2.5373146533966064\n",
            "2.403286933898926\n",
            "2.3868789672851562\n",
            "2.436859369277954\n",
            "2.5529565811157227\n",
            "2.4114513397216797\n",
            "2.463592767715454\n",
            "2.488860845565796\n",
            "2.4265923500061035\n",
            "2.4475808143615723\n",
            "2.52205491065979\n",
            "2.559950113296509\n",
            "2.5390243530273438\n",
            "2.5384020805358887\n",
            "2.290286064147949\n",
            "2.440746307373047\n",
            "2.5849034786224365\n",
            "2.480712413787842\n",
            "2.4238953590393066\n",
            "2.4769670963287354\n",
            "2.468752145767212\n",
            "2.4205572605133057\n",
            "2.342711925506592\n",
            "2.4599692821502686\n",
            "2.4135069847106934\n",
            "2.397263765335083\n",
            "2.5543813705444336\n",
            "2.3189823627471924\n",
            "2.444532871246338\n",
            "2.377878189086914\n",
            "2.501819372177124\n",
            "2.4499499797821045\n",
            "2.445582866668701\n",
            "2.4260976314544678\n",
            "2.5001790523529053\n",
            "2.462062120437622\n",
            "2.5810086727142334\n",
            "2.431492805480957\n",
            "2.3826942443847656\n",
            "2.30063796043396\n",
            "2.347358465194702\n",
            "2.5336408615112305\n",
            "2.5343117713928223\n",
            "2.506269693374634\n",
            "2.440411329269409\n",
            "2.495435953140259\n",
            "2.4900505542755127\n",
            "2.420490264892578\n",
            "2.478973865509033\n",
            "2.3525805473327637\n",
            "2.372891426086426\n",
            "2.455437421798706\n",
            "2.471587896347046\n",
            "2.490841865539551\n",
            "2.483287811279297\n",
            "2.4681334495544434\n",
            "2.457463264465332\n",
            "2.3693289756774902\n",
            "2.519782781600952\n",
            "2.464447259902954\n",
            "2.47864031791687\n",
            "2.4289937019348145\n",
            "2.4407784938812256\n",
            "2.3860061168670654\n",
            "2.3815083503723145\n",
            "2.4089441299438477\n",
            "2.4419057369232178\n",
            "2.407198190689087\n",
            "2.458650827407837\n",
            "2.384901762008667\n",
            "2.55027437210083\n",
            "2.5541868209838867\n",
            "2.524517774581909\n",
            "2.4616522789001465\n",
            "2.468982696533203\n",
            "2.537386178970337\n",
            "2.4434654712677\n",
            "2.450868844985962\n",
            "2.4540555477142334\n",
            "2.356320858001709\n",
            "2.4656221866607666\n",
            "2.5291695594787598\n",
            "2.4187233448028564\n",
            "2.342937469482422\n",
            "2.4155757427215576\n",
            "2.5124104022979736\n",
            "2.4582722187042236\n",
            "2.4201886653900146\n",
            "2.508643865585327\n",
            "2.3498013019561768\n",
            "2.657410144805908\n",
            "2.4120278358459473\n",
            "2.4904286861419678\n",
            "2.3907957077026367\n",
            "2.423175573348999\n",
            "2.4994447231292725\n",
            "2.565189838409424\n",
            "2.479806661605835\n",
            "2.4598896503448486\n",
            "2.4404189586639404\n",
            "2.4976072311401367\n",
            "2.366636037826538\n",
            "2.366716146469116\n",
            "2.3708584308624268\n",
            "2.4629406929016113\n",
            "2.456648111343384\n",
            "2.346512794494629\n",
            "2.5329792499542236\n",
            "2.425948143005371\n",
            "2.562357187271118\n",
            "2.4310977458953857\n",
            "2.4243767261505127\n",
            "2.4508490562438965\n",
            "2.417471170425415\n",
            "2.3830349445343018\n",
            "2.3771166801452637\n",
            "2.4836313724517822\n",
            "2.400019884109497\n",
            "2.494662046432495\n",
            "2.455864667892456\n",
            "2.469498634338379\n",
            "2.469700813293457\n",
            "2.5721235275268555\n",
            "2.4270360469818115\n",
            "2.4723927974700928\n",
            "2.4879298210144043\n",
            "2.4955697059631348\n",
            "2.401972532272339\n",
            "2.493159294128418\n",
            "2.555583953857422\n",
            "2.518260955810547\n",
            "2.4425835609436035\n",
            "2.558328151702881\n",
            "2.4657912254333496\n",
            "2.556671142578125\n",
            "2.3114776611328125\n",
            "2.4865057468414307\n",
            "2.435300350189209\n",
            "2.4753501415252686\n",
            "2.325535774230957\n",
            "2.543391466140747\n",
            "2.465277671813965\n",
            "2.4784836769104004\n",
            "2.40950870513916\n",
            "2.414194345474243\n",
            "2.494565010070801\n",
            "2.3231325149536133\n",
            "2.3816540241241455\n",
            "2.485135793685913\n",
            "2.3709428310394287\n",
            "2.3234708309173584\n",
            "2.3713319301605225\n",
            "2.384782314300537\n",
            "2.4871678352355957\n",
            "2.56809663772583\n",
            "2.4942803382873535\n",
            "2.4560201168060303\n",
            "2.5384011268615723\n",
            "2.365037202835083\n",
            "2.4403281211853027\n",
            "2.3801748752593994\n",
            "2.4639947414398193\n",
            "2.497602701187134\n",
            "2.4620769023895264\n",
            "2.470813274383545\n",
            "2.502396583557129\n",
            "2.505291223526001\n",
            "2.3280539512634277\n",
            "2.3979430198669434\n",
            "2.5208802223205566\n",
            "2.3600902557373047\n",
            "2.4984405040740967\n",
            "2.4489450454711914\n",
            "2.3973476886749268\n",
            "2.3570306301116943\n",
            "2.3782241344451904\n",
            "2.4799411296844482\n",
            "2.373061180114746\n",
            "2.4451005458831787\n",
            "2.4150025844573975\n",
            "2.5245254039764404\n",
            "2.380756139755249\n",
            "2.4410183429718018\n",
            "2.4956510066986084\n",
            "2.504319429397583\n",
            "2.414501190185547\n",
            "2.478513479232788\n",
            "2.496452808380127\n",
            "2.3960154056549072\n",
            "2.41618013381958\n",
            "2.471757173538208\n",
            "2.453817129135132\n",
            "2.422138214111328\n",
            "2.4690377712249756\n",
            "2.530364513397217\n",
            "2.4936747550964355\n",
            "2.5287747383117676\n",
            "2.536515712738037\n",
            "2.429757833480835\n",
            "2.460986852645874\n",
            "2.526796817779541\n",
            "2.3711299896240234\n",
            "2.2946646213531494\n",
            "2.363265037536621\n",
            "2.401151418685913\n",
            "2.439030885696411\n",
            "2.2949917316436768\n",
            "2.5222275257110596\n",
            "2.5103278160095215\n",
            "2.5016727447509766\n",
            "2.3938140869140625\n",
            "2.4944608211517334\n",
            "2.453096628189087\n",
            "2.362685203552246\n",
            "2.439361333847046\n",
            "2.518070936203003\n",
            "2.416128635406494\n",
            "2.4390084743499756\n",
            "2.377729892730713\n",
            "2.4206719398498535\n",
            "2.4404733180999756\n",
            "2.429131031036377\n",
            "2.425477981567383\n",
            "2.4910976886749268\n",
            "2.4962260723114014\n",
            "2.4824235439300537\n",
            "2.5264484882354736\n",
            "2.5033671855926514\n",
            "2.4836645126342773\n",
            "2.4440977573394775\n",
            "2.4130306243896484\n",
            "2.452531099319458\n",
            "2.501281499862671\n",
            "2.5385854244232178\n",
            "2.4951744079589844\n",
            "2.433671474456787\n",
            "2.4711475372314453\n",
            "2.4515061378479004\n",
            "2.4114277362823486\n",
            "2.5425593852996826\n",
            "2.549203634262085\n",
            "2.4715194702148438\n",
            "2.3526294231414795\n",
            "2.463383197784424\n",
            "2.5737650394439697\n",
            "2.4306981563568115\n",
            "2.503840446472168\n",
            "2.407778024673462\n",
            "2.5606229305267334\n",
            "2.5859222412109375\n",
            "2.512192487716675\n",
            "2.389204978942871\n",
            "2.3358376026153564\n",
            "2.4425361156463623\n",
            "2.4779670238494873\n",
            "2.4902431964874268\n",
            "2.3990085124969482\n",
            "2.4757511615753174\n",
            "2.415381908416748\n",
            "2.3735861778259277\n",
            "2.3739359378814697\n",
            "2.495100736618042\n",
            "2.505552291870117\n",
            "2.431532859802246\n",
            "2.4436426162719727\n",
            "2.470299005508423\n",
            "2.560885429382324\n",
            "2.542757034301758\n",
            "2.5513715744018555\n",
            "2.5245580673217773\n",
            "2.4762542247772217\n",
            "2.4833621978759766\n",
            "2.435274839401245\n",
            "2.3935728073120117\n",
            "2.383866786956787\n",
            "2.4530208110809326\n",
            "2.434314489364624\n",
            "2.541529893875122\n",
            "2.4754152297973633\n",
            "2.446255683898926\n",
            "2.370009660720825\n",
            "2.424258232116699\n",
            "2.535404920578003\n",
            "2.529829263687134\n",
            "2.4684109687805176\n",
            "2.457479953765869\n",
            "2.4452505111694336\n",
            "2.4056570529937744\n",
            "2.388672113418579\n",
            "2.3760452270507812\n",
            "2.3849143981933594\n",
            "2.419790029525757\n",
            "2.3532497882843018\n",
            "2.356138229370117\n",
            "2.3676939010620117\n",
            "2.49057674407959\n",
            "2.445584774017334\n",
            "2.4685375690460205\n",
            "2.3958334922790527\n",
            "2.357968807220459\n",
            "2.441770076751709\n",
            "2.482192039489746\n",
            "2.661194324493408\n",
            "2.5241270065307617\n",
            "2.5536680221557617\n",
            "2.4313931465148926\n",
            "2.3919332027435303\n",
            "2.4390616416931152\n",
            "2.488090991973877\n",
            "2.4986603260040283\n",
            "2.43002986907959\n",
            "2.3884215354919434\n",
            "2.3382856845855713\n",
            "2.457207441329956\n",
            "2.480588436126709\n",
            "2.446721315383911\n",
            "2.279331684112549\n",
            "2.466012477874756\n",
            "2.4791903495788574\n",
            "2.5264627933502197\n",
            "2.623349189758301\n",
            "2.5018277168273926\n",
            "2.430262565612793\n",
            "2.459200143814087\n",
            "2.5505893230438232\n",
            "2.368227481842041\n",
            "2.48140025138855\n",
            "2.3585569858551025\n",
            "2.472646474838257\n",
            "2.439314126968384\n",
            "2.500420331954956\n",
            "2.5383050441741943\n",
            "2.4422340393066406\n",
            "2.424142360687256\n",
            "2.3760030269622803\n",
            "2.51173996925354\n",
            "2.478717565536499\n",
            "2.418454170227051\n",
            "2.370069980621338\n",
            "2.404623508453369\n",
            "2.380542516708374\n",
            "2.5605058670043945\n",
            "2.2703943252563477\n",
            "2.4669485092163086\n",
            "2.499495506286621\n",
            "2.4463932514190674\n",
            "2.599346160888672\n",
            "2.3427908420562744\n",
            "2.341256856918335\n",
            "2.385570764541626\n",
            "2.715940475463867\n",
            "2.5346498489379883\n",
            "2.556514024734497\n",
            "2.550847053527832\n",
            "2.2631313800811768\n",
            "2.5365591049194336\n",
            "2.421292304992676\n",
            "2.38395619392395\n",
            "2.375875473022461\n"
          ]
        }
      ],
      "source": [
        "# Training Loop.\n",
        "\n",
        "batch_size = 32\n",
        "for i in range(20000):\n",
        "  xb, yb = get_data('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step() # Update weights.\n",
        "\n",
        "  print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1ovrev3hJqZ",
        "outputId": "d7657f5d-88a3-45de-ba3e-444f59ff5f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "thit RD:\n",
            "Gom.\n",
            "I INomere y ghesen cond\n",
            "YCouchin, w?\n",
            "\n",
            "Te counere ne ung;\n",
            "GE n;\n",
            "II t.\n",
            "\n",
            "He verve o was.\n",
            "\n",
            "\n",
            "Hes aift NTHEDIO:\n",
            "Sl:\n",
            "Whinke ngt?\n",
            "MAms NGO shemo too mo anthatinthakes f utous as Agonteopr botherore thind spat PTEShiat ureraierio pr son me LO:\n",
            "Wats me S:\n",
            "To tllingewe lley ayom\n",
            "\n",
            "Mo;\n",
            "Latanssuromas:\n",
            "\n",
            "Y:\n",
            "PE:\n",
            "Therucover, min ld te o e, un rd o s hthecals,\n",
            "\n",
            "WI thank m:\n",
            "\n",
            "\n",
            "NTharu t irlendoucin,\n",
            "Y: Tisteriomad t.\n",
            "Yor f.\n",
            "\n",
            "S:\n",
            "G, g w,\n",
            "\n",
            "Hee:\n",
            "\n",
            "\n",
            "NGLI,\n",
            "JOreden t t IVo sl\n",
            "LLTre!\n",
            "\n",
            "\n",
            "APrs whes ge d f.\n",
            "\n",
            "WI beco\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(torch.tensor([[0]]), 500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELCTvVm0EvIV"
      },
      "source": [
        "## Attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilhjTabp_UIP",
        "outputId": "fdaf3a3e-f6b6-4a89-aac4-35e09f750a75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-2.0985, -0.9119],\n",
              "         [ 0.4256,  2.2927],\n",
              "         [ 1.3981,  0.7014],\n",
              "         [ 0.5085, -0.2177],\n",
              "         [ 0.1882,  1.4853],\n",
              "         [ 1.6761, -1.1527],\n",
              "         [ 0.3777, -0.7961]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.9248, -1.6166],\n",
              "         [ 0.4086, -0.2344],\n",
              "         [ 0.4223,  0.2934],\n",
              "         [ 1.6078, -2.2084],\n",
              "         [ 1.5166, -0.3172],\n",
              "         [ 0.4452,  1.5462],\n",
              "         [ 0.1232, -1.8541]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.5607, -0.6266],\n",
              "         [-0.5190,  2.6530],\n",
              "         [-0.5988,  0.6044],\n",
              "         [ 0.4774,  0.9256],\n",
              "         [-1.7807,  0.9799],\n",
              "         [-0.9383,  0.0622],\n",
              "         [-0.7525, -1.4755]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [ 0.5311, -0.4315],\n",
              "         [ 0.7802, -1.1138],\n",
              "         [ 0.9651,  0.6793],\n",
              "         [ 0.7532,  0.3092],\n",
              "         [-0.7913, -0.7759],\n",
              "         [ 0.5681,  1.0482],\n",
              "         [-0.2692,  0.3178]]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mathematical trick. (Attention me idea:)\n",
        "# Currently we are using only the the last token in time to predict the next token/ Context window ka to scope aaya hi nahi hai.\n",
        "# Attention -> Let the tokens within the context window, talk to each other.\n",
        "\n",
        "# One good way -> in time T, if we represent each token as an average of all the tokens up until in the past. (Future tokens won't talk).\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn((B, T, C))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjhGl8tpFm4v",
        "outputId": "77aaf79f-a54c-4455-fb36-2b86ac14b33c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.4752, -0.3331],\n",
              "        [-2.0985, -0.9119],\n",
              "        [ 0.4256,  2.2927],\n",
              "        [ 1.3981,  0.7014],\n",
              "        [ 0.5085, -0.2177],\n",
              "        [ 0.1882,  1.4853],\n",
              "        [ 1.6761, -1.1527],\n",
              "        [ 0.3777, -0.7961]])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0] # 1 batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wYVSofYFrHC",
        "outputId": "7d14285a-6745-441a-8512-2e05b3351b15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.3116, -0.6225])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0][:2, :].mean(dim = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qKs8RNoHnY2"
      },
      "source": [
        "### Method 1 (Simplest)/Loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EDq_uXiF4pF",
        "outputId": "fec4e120-a62a-442c-f5d8-9151a6fb772c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This avg representation is also referred to as BOW.\n",
        "# We will se 3 different methods to calculate this average of all the tokens in the past.\n",
        "\n",
        "xbow1 = torch.zeros((B, T, C))\n",
        "xbow1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbemC9vtGfB_",
        "outputId": "5571cca5-83a0-4504-e38b-6cedbd360e81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.3116, -0.6225],\n",
              "         [-0.0659,  0.3492],\n",
              "         [ 0.3001,  0.4373],\n",
              "         [ 0.3418,  0.3063],\n",
              "         [ 0.3162,  0.5028],\n",
              "         [ 0.5105,  0.2663],\n",
              "         [ 0.4939,  0.1335]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.5272, -0.7374],\n",
              "         [-0.8819, -0.5697],\n",
              "         [-0.5559, -0.3539],\n",
              "         [-0.1231, -0.7248],\n",
              "         [ 0.1501, -0.6569],\n",
              "         [ 0.1923, -0.3422],\n",
              "         [ 0.1837, -0.5312]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.3433, -1.0400],\n",
              "         [-0.4018,  0.1910],\n",
              "         [-0.4511,  0.2944],\n",
              "         [-0.2654,  0.4206],\n",
              "         [-0.5179,  0.5138],\n",
              "         [-0.5780,  0.4493],\n",
              "         [-0.5998,  0.2087]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.2451, -0.3431],\n",
              "         [ 0.0966, -0.6000],\n",
              "         [ 0.3138, -0.2802],\n",
              "         [ 0.4017, -0.1623],\n",
              "         [ 0.2028, -0.2646],\n",
              "         [ 0.2550, -0.0770],\n",
              "         [ 0.1895, -0.0277]]])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xbow1[b, t] = x[b][:t + 1, :].mean(dim = 0)\n",
        "\n",
        "xbow1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwMo27JlHPXX",
        "outputId": "a4d9cd89-f90f-4943-e1d4-35e1eca868cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-2.0985, -0.9119],\n",
              "         [ 0.4256,  2.2927],\n",
              "         [ 1.3981,  0.7014],\n",
              "         [ 0.5085, -0.2177],\n",
              "         [ 0.1882,  1.4853],\n",
              "         [ 1.6761, -1.1527],\n",
              "         [ 0.3777, -0.7961]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.9248, -1.6166],\n",
              "         [ 0.4086, -0.2344],\n",
              "         [ 0.4223,  0.2934],\n",
              "         [ 1.6078, -2.2084],\n",
              "         [ 1.5166, -0.3172],\n",
              "         [ 0.4452,  1.5462],\n",
              "         [ 0.1232, -1.8541]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.5607, -0.6266],\n",
              "         [-0.5190,  2.6530],\n",
              "         [-0.5988,  0.6044],\n",
              "         [ 0.4774,  0.9256],\n",
              "         [-1.7807,  0.9799],\n",
              "         [-0.9383,  0.0622],\n",
              "         [-0.7525, -1.4755]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [ 0.5311, -0.4315],\n",
              "         [ 0.7802, -1.1138],\n",
              "         [ 0.9651,  0.6793],\n",
              "         [ 0.7532,  0.3092],\n",
              "         [-0.7913, -0.7759],\n",
              "         [ 0.5681,  1.0482],\n",
              "         [-0.2692,  0.3178]]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x # But ye method kehte hai efficient nahi hai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XekMtLDNH1T1"
      },
      "source": [
        "### Method 2: Matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDYk7ftH5u_",
        "outputId": "a237f778-89a0-4c54-e3bf-32b1a704001d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.ones((3, 3)) # Suppose ye Time dimension me hai (T, T)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-weehbUHzNh",
        "outputId": "06ae507d-f8c8-44f5-b37c-97e8647b4bb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4, 2],\n",
              "        [4, 2],\n",
              "        [3, 4]])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b = torch.randint(1, 5, (3, 2)) # Ye Time * Channel dimension hai (T, C)\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0651xG8IlLl",
        "outputId": "bc47b44d-41a9-4e33-ea3e-2b903d32f113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril() # Lower triangular matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI7oVBZ3Ir9t",
        "outputId": "acf65fc3-54bd-42d1-9a51-4eb4b713b4ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 4.,  2.],\n",
              "        [ 8.,  4.],\n",
              "        [11.,  8.]])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril() @ b.float() # Sum uptill that token in the time. (Exactly what we want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKmPhzAuI5Mj",
        "outputId": "83e7901a-fdc7-42d4-e911-f234ff274972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [2.],\n",
              "        [3.]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril().sum(dim = 1, keepdim = True) # & dividing by this will solve our purpose (of getting the average upto the ith token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fot9JRPXJN-l",
        "outputId": "249807f4-ffcc-4f52-a88f-30183dd5be36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.0000, 2.0000],\n",
              "        [4.0000, 2.0000],\n",
              "        [3.6667, 2.6667]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril() @ b.float() / a.tril().sum(1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO_98gdvJYvy",
        "outputId": "42874209-d232-4fb0-8c25-d0f81ce10474"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.ones((T, T))\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU-LUEC8JkYj",
        "outputId": "d5890deb-0510-4764-bdd2-fd94ecc85272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x299uYYJluA",
        "outputId": "6b4bd09e-fbc8-4bef-a380-ae0ecbfaebc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.6233, -1.2450],\n",
              "         [-0.1977,  1.0477],\n",
              "         [ 1.2004,  1.7491],\n",
              "         [ 1.7089,  1.5314],\n",
              "         [ 1.8971,  3.0168],\n",
              "         [ 3.5732,  1.8640],\n",
              "         [ 3.9508,  1.0679]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-3.0543, -1.4747],\n",
              "         [-2.6458, -1.7092],\n",
              "         [-2.2235, -1.4157],\n",
              "         [-0.6157, -3.6241],\n",
              "         [ 0.9008, -3.9414],\n",
              "         [ 1.3461, -2.3952],\n",
              "         [ 1.4693, -4.2493]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.6865, -2.0800],\n",
              "         [-1.2055,  0.5730],\n",
              "         [-1.8043,  1.1774],\n",
              "         [-1.3269,  2.1030],\n",
              "         [-3.1077,  3.0829],\n",
              "         [-4.0460,  3.1451],\n",
              "         [-4.7985,  1.6696]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.4902, -0.6862],\n",
              "         [ 0.2899, -1.8000],\n",
              "         [ 1.2550, -1.1208],\n",
              "         [ 2.0083, -0.8116],\n",
              "         [ 1.2169, -1.5875],\n",
              "         [ 1.7850, -0.5393],\n",
              "         [ 1.5158, -0.2215]]])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tril() @ x # (T, T) @ (B, T, C) -> Broadcast across all batches (Matching occurs from right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBkk53TrJ2bj",
        "outputId": "dcceccd9-854d-43c5-f34d-dd39b9d35065"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.3116, -0.6225],\n",
              "         [-0.0659,  0.3492],\n",
              "         [ 0.3001,  0.4373],\n",
              "         [ 0.3418,  0.3063],\n",
              "         [ 0.3162,  0.5028],\n",
              "         [ 0.5105,  0.2663],\n",
              "         [ 0.4939,  0.1335]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.5272, -0.7374],\n",
              "         [-0.8819, -0.5697],\n",
              "         [-0.5559, -0.3539],\n",
              "         [-0.1231, -0.7248],\n",
              "         [ 0.1501, -0.6569],\n",
              "         [ 0.1923, -0.3422],\n",
              "         [ 0.1837, -0.5312]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.3433, -1.0400],\n",
              "         [-0.4018,  0.1910],\n",
              "         [-0.4511,  0.2944],\n",
              "         [-0.2654,  0.4206],\n",
              "         [-0.5179,  0.5138],\n",
              "         [-0.5780,  0.4493],\n",
              "         [-0.5998,  0.2087]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.2451, -0.3431],\n",
              "         [ 0.0966, -0.6000],\n",
              "         [ 0.3138, -0.2802],\n",
              "         [ 0.4017, -0.1623],\n",
              "         [ 0.2028, -0.2646],\n",
              "         [ 0.2550, -0.0770],\n",
              "         [ 0.1895, -0.0277]]])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow2 = (a.tril() @ x) / a.tril().sum(1, True)\n",
        "xbow2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPdXjrjbKJfB",
        "outputId": "e72bff6d-cee6-4173-eeb0-37c4aac85ab3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(xbow1, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6Z--sC4Kdvk",
        "outputId": "a9f6257f-e41a-4e62-a9c9-6378e041c8aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Or we can tell it something like:\n",
        "\n",
        "wei = torch.ones((T, T)).tril() / torch.ones((T, T)).tril().sum(1, True)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbF107vJKwEG",
        "outputId": "1391d8fc-8dbf-4ee9-95cf-09641bc543ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.3116, -0.6225],\n",
              "         [-0.0659,  0.3492],\n",
              "         [ 0.3001,  0.4373],\n",
              "         [ 0.3418,  0.3063],\n",
              "         [ 0.3162,  0.5028],\n",
              "         [ 0.5105,  0.2663],\n",
              "         [ 0.4939,  0.1335]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.5272, -0.7374],\n",
              "         [-0.8819, -0.5697],\n",
              "         [-0.5559, -0.3539],\n",
              "         [-0.1231, -0.7248],\n",
              "         [ 0.1501, -0.6569],\n",
              "         [ 0.1923, -0.3422],\n",
              "         [ 0.1837, -0.5312]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.3433, -1.0400],\n",
              "         [-0.4018,  0.1910],\n",
              "         [-0.4511,  0.2944],\n",
              "         [-0.2654,  0.4206],\n",
              "         [-0.5179,  0.5138],\n",
              "         [-0.5780,  0.4493],\n",
              "         [-0.5998,  0.2087]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.2451, -0.3431],\n",
              "         [ 0.0966, -0.6000],\n",
              "         [ 0.3138, -0.2802],\n",
              "         [ 0.4017, -0.1623],\n",
              "         [ 0.2028, -0.2646],\n",
              "         [ 0.2550, -0.0770],\n",
              "         [ 0.1895, -0.0277]]])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow2 = wei @ x\n",
        "xbow2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHH_C6kmK0Ls",
        "outputId": "b108a2cb-8185-4696-ea28-17eb4c5cebb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(xbow1, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARR5HxTOKP_B"
      },
      "source": [
        "### Method 3: (The one to use)/Sigmoid (Data-based weights):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH3fzjDAKTuP",
        "outputId": "10d4d863-7f5f-4f01-d65b-4593ba86b525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ab see upar sab theek hai ki we are taking the averages of all the previous tokens.\n",
        "# But why assign equal weightages to all/ Its possible ki certain tokens mights find certain tokens more helpful/ more affinity/\n",
        "# So, why not take a weighted average/ And the weights will update accordingly (in the training)\n",
        "\n",
        "tril = torch.ones(T, T).tril()\n",
        "tril"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMFEyWEXP_sk"
      },
      "source": [
        "You don't want the weights to be all zeros initially only. Something that you want to learn (data-dependent way). Some tokens will find some tokens more or less interesting.\n",
        "\n",
        "Getting these weights is the problem that attention solves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic0i8GXVOZXW",
        "outputId": "6141a240-1c34-4d6d-b562-6c3dfdf6b9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.zeros(T, T) # Something the model will learn.\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Ts8dkoOf1w",
        "outputId": "ff4cba5b-9537-4f17-fad1-0389dfffb28f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = wei.masked_fill(tril == 0, -float('inf')) # Future tokens can't talk. (Decoder)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8mhrwooOyFY",
        "outputId": "76d439f7-279e-4784-e43f-31d57e0087dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.softmax(1) # Counts to probabilities/Conversion.\n",
        "# Currently all previous tokens have the same weight. But the idea is that they should be learned during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsLbVJNGPD1J",
        "outputId": "9c8c2719-9efd-4126-a3e7-46ab5a39bf7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.3116, -0.6225],\n",
              "         [-0.0659,  0.3492],\n",
              "         [ 0.3001,  0.4373],\n",
              "         [ 0.3418,  0.3063],\n",
              "         [ 0.3162,  0.5028],\n",
              "         [ 0.5105,  0.2663],\n",
              "         [ 0.4939,  0.1335]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.5272, -0.7374],\n",
              "         [-0.8819, -0.5697],\n",
              "         [-0.5559, -0.3539],\n",
              "         [-0.1231, -0.7248],\n",
              "         [ 0.1501, -0.6569],\n",
              "         [ 0.1923, -0.3422],\n",
              "         [ 0.1837, -0.5312]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.3433, -1.0400],\n",
              "         [-0.4018,  0.1910],\n",
              "         [-0.4511,  0.2944],\n",
              "         [-0.2654,  0.4206],\n",
              "         [-0.5179,  0.5138],\n",
              "         [-0.5780,  0.4493],\n",
              "         [-0.5998,  0.2087]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.2451, -0.3431],\n",
              "         [ 0.0966, -0.6000],\n",
              "         [ 0.3138, -0.2802],\n",
              "         [ 0.4017, -0.1623],\n",
              "         [ 0.2028, -0.2646],\n",
              "         [ 0.2550, -0.0770],\n",
              "         [ 0.1895, -0.0277]]])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow3 = wei.softmax(1) @ x\n",
        "xbow3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxjl3HAOPIrT",
        "outputId": "fa5aa838-241d-47f7-aab8-7e6dcc396b4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(xbow1, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYwxsDkESuJT"
      },
      "source": [
        "### How to get that weight matrix in a data-driven way?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql4s7XFTSxrd"
      },
      "outputs": [],
      "source": [
        "# Key & Query Vectors.\n",
        "# Each token in time emmits a Key & Query vector. -> So, Key is like, what I have/who I am, and query is like what I am looking for.\n",
        "# Idea is if we perform a dot product of Key & Query vectors/ Like ith position in time vaale token ke query vector ka picchle valo ke key vectors se.\n",
        "# The more the value/ the more is affinity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI1J-y9BTkv6",
        "outputId": "ebacaf9f-4e83-42ea-d13a-3a465d9b5ba6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Current / Existing idea.\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "tril = torch.ones((T, T)).tril()\n",
        "tril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReTOvNdsT6Tg",
        "outputId": "fc2cf1c7-2b6e-4362-a4d9-9647e36beed8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.zeros(T, T) # This we want to learn in some way (Initial affinities).\n",
        "wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsxhU46UNNu"
      },
      "source": [
        "**(Decoder block)/ Future tokens can't communicate.\n",
        "If we don't apply this then all tokens can fully communicate. Examples in Anomaly detection/ Sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XHART1dUEJp",
        "outputId": "67ffc6e5-b73a-4cf8-9159-eea1e2de7836"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = wei.masked_fill(tril == 0, float('-inf')) # Decoder block.\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xppiQRbUUuHe",
        "outputId": "a9787c67-938c-4033-d3e9-6b69911c21bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = wei.softmax(1) # Normalization/ (weights calculation) (T, T)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbK22oj1U2qn",
        "outputId": "0777e438-5f14-47b4-eb93-bb2037893dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4752, -0.3331],\n",
              "         [-0.3116, -0.6225],\n",
              "         [-0.0659,  0.3492],\n",
              "         [ 0.3001,  0.4373],\n",
              "         [ 0.3418,  0.3063],\n",
              "         [ 0.3162,  0.5028],\n",
              "         [ 0.5105,  0.2663],\n",
              "         [ 0.4939,  0.1335]],\n",
              "\n",
              "        [[-1.1295,  0.1419],\n",
              "         [-1.5272, -0.7374],\n",
              "         [-0.8819, -0.5697],\n",
              "         [-0.5559, -0.3539],\n",
              "         [-0.1231, -0.7248],\n",
              "         [ 0.1501, -0.6569],\n",
              "         [ 0.1923, -0.3422],\n",
              "         [ 0.1837, -0.5312]],\n",
              "\n",
              "        [[-0.1258, -1.4535],\n",
              "         [-0.3433, -1.0400],\n",
              "         [-0.4018,  0.1910],\n",
              "         [-0.4511,  0.2944],\n",
              "         [-0.2654,  0.4206],\n",
              "         [-0.5179,  0.5138],\n",
              "         [-0.5780,  0.4493],\n",
              "         [-0.5998,  0.2087]],\n",
              "\n",
              "        [[-1.0214, -0.2547],\n",
              "         [-0.2451, -0.3431],\n",
              "         [ 0.0966, -0.6000],\n",
              "         [ 0.3138, -0.2802],\n",
              "         [ 0.4017, -0.1623],\n",
              "         [ 0.2028, -0.2646],\n",
              "         [ 0.2550, -0.0770],\n",
              "         [ 0.1895, -0.0277]]])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei @ x # Currently the weights are also same for all the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjR3ynY2U_Sm",
        "outputId": "78f6bd02-cae7-49d9-9946-08b5adbfd004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Attention idea.\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn((B, T, C))\n",
        "\n",
        "head_size = 16 # Hyper Parameter.\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "# A linear layer with input features as C (32) length of my token embeddings and the output features length is Head size\n",
        "# C as input size because all tokens in Time T are represented by a vector of size C only right.\n",
        "\n",
        "tril = torch.ones((T, T)).tril()\n",
        "tril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssIV9-C_WBYM",
        "outputId": "38597b6c-19d7-425a-e45f-8dace89d0597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 32]), Linear(in_features=32, out_features=16, bias=False))"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# wei = torch.zeros((T, T)) # Initial affinities --> Ye nahi karna hai ab.\n",
        "x.shape, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcpn2EFnWN6-",
        "outputId": "ed4551ca-3631-4672-b2be-94cf955a25ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]),\n",
              " tensor([[[ 4.3157e-01,  7.4206e-01,  1.3768e-01,  7.3645e-02,  3.4282e-01,\n",
              "            1.8170e-02,  6.4777e-01,  5.1194e-01,  3.3060e-01,  1.6556e-01,\n",
              "           -1.1616e+00, -2.4909e-01, -7.1097e-01, -5.4473e-01, -9.3143e-01,\n",
              "           -3.4398e-01],\n",
              "          [-4.8183e-01,  1.1450e-01,  1.0956e+00, -5.6095e-01,  4.0720e-01,\n",
              "           -1.0027e+00, -9.1336e-01, -2.1585e-01,  3.0891e-01, -3.1538e-02,\n",
              "           -1.3119e+00, -9.0480e-01, -7.0076e-01,  6.8252e-01, -5.7555e-01,\n",
              "           -3.5477e-01],\n",
              "          [-8.6975e-01,  1.1035e+00,  4.0402e-01, -2.8101e-01, -5.0694e-01,\n",
              "            2.0276e-01, -1.4554e-01,  4.7682e-01,  1.2649e+00,  6.2719e-01,\n",
              "            2.2743e-01, -8.7544e-03, -6.1369e-01,  9.3223e-01,  1.1623e+00,\n",
              "            6.7829e-01],\n",
              "          [ 2.7745e-01, -8.5276e-01,  3.6115e-01,  7.8373e-01,  1.4838e-02,\n",
              "           -5.0581e-01, -7.5652e-01,  5.8800e-02, -4.1435e-01,  1.4338e-03,\n",
              "            2.8093e-03, -4.3071e-01,  5.6401e-01,  7.1772e-02,  3.9642e-01,\n",
              "           -1.2286e-01],\n",
              "          [ 3.5275e-01, -6.1117e-01,  8.1472e-01,  8.2871e-01,  5.7562e-01,\n",
              "            1.3730e-01, -5.8215e-01, -3.7658e-01,  1.0728e-01, -5.3297e-01,\n",
              "            4.1370e-01, -5.5971e-01,  9.9324e-01, -6.0454e-01,  6.6801e-01,\n",
              "            8.7645e-02],\n",
              "          [-9.7334e-01, -7.9117e-02,  5.8325e-01,  9.9914e-01, -4.1203e-01,\n",
              "           -1.9322e-01,  2.7341e-01, -1.8537e-01,  1.0618e+00,  1.5084e-01,\n",
              "            6.3151e-01, -1.7691e-01,  5.2603e-01, -1.3158e-01,  4.1374e-01,\n",
              "            9.2915e-01],\n",
              "          [-7.0800e-01, -1.5376e-02, -6.5882e-01, -9.6243e-02,  7.8958e-01,\n",
              "           -3.7802e-02,  3.4722e-01,  4.6991e-02,  1.1991e-01, -1.5766e-01,\n",
              "            1.1127e-01,  1.2675e-01,  4.5496e-01, -5.5020e-01, -2.7430e-01,\n",
              "           -1.3891e-01],\n",
              "          [ 1.5913e-01, -3.6131e-01,  6.8991e-01,  6.2330e-01, -1.7872e-01,\n",
              "           -1.8285e-02, -3.4754e-01,  9.9018e-02, -9.5070e-02,  1.0156e+00,\n",
              "           -7.8917e-01,  3.2208e-01, -1.5393e-01,  8.7833e-01, -2.0092e-01,\n",
              "            1.5981e-01]],\n",
              " \n",
              "         [[-9.7969e-01,  1.1310e-01,  4.3821e-01, -3.3460e-01,  1.7233e-01,\n",
              "           -9.0236e-01, -9.5917e-01,  2.7983e-01,  9.6746e-01,  2.9435e-01,\n",
              "           -1.4052e+00,  7.9423e-01, -2.5719e-01, -2.4063e-01, -3.7094e-01,\n",
              "           -6.7162e-01],\n",
              "          [ 1.0775e-01, -1.2528e+00,  2.4811e-01,  9.5234e-01, -1.3249e-01,\n",
              "           -3.8945e-01,  4.3827e-01,  7.2987e-01,  1.3334e+00,  1.1756e-01,\n",
              "           -1.2791e+00,  4.9743e-01, -3.5314e-01, -5.3922e-01,  2.1265e-02,\n",
              "           -3.3141e-01],\n",
              "          [-5.8152e-02,  1.2988e+00,  1.8564e-01,  1.8983e-02,  8.1001e-01,\n",
              "            1.8053e-01,  9.6110e-01, -5.2683e-01, -9.9869e-01, -6.1845e-01,\n",
              "            8.2153e-01, -1.1314e-01, -1.0179e+00, -4.8917e-01, -2.9387e-01,\n",
              "            9.6933e-01],\n",
              "          [ 9.6716e-01,  5.3887e-01, -1.3827e+00, -2.1067e-01,  5.2572e-01,\n",
              "           -3.1234e-01, -1.7414e-01, -3.3661e-01, -5.9231e-02,  6.0182e-02,\n",
              "           -3.3270e-01, -9.8469e-01,  2.1482e-01,  1.2164e-01, -2.9159e-01,\n",
              "           -1.4729e-01],\n",
              "          [ 8.6784e-01,  3.3823e-03,  8.1097e-01, -1.5670e-01,  7.4300e-01,\n",
              "           -3.9579e-01, -3.6160e-01,  7.8847e-01,  2.2791e-01,  1.2226e-01,\n",
              "           -5.1905e-01, -9.3428e-01, -4.9730e-01,  1.4890e-01, -1.3220e+00,\n",
              "           -6.2231e-01],\n",
              "          [-3.3326e-01, -3.0597e-02, -9.1088e-01,  1.7695e-03, -5.1503e-01,\n",
              "            3.3138e-01,  8.3291e-01,  1.6526e-01,  4.9030e-01,  1.5052e-01,\n",
              "           -2.6181e-01,  1.4809e-01,  7.0847e-02, -1.4391e-01, -1.0868e+00,\n",
              "           -1.0929e+00],\n",
              "          [-1.0956e-01,  2.1411e-01, -1.7022e+00, -1.3401e+00, -1.5410e-01,\n",
              "            3.5761e-01,  6.3708e-01, -6.3862e-01, -7.9543e-01, -8.5202e-01,\n",
              "            6.4514e-01,  1.8002e-01, -2.4818e-02, -1.1164e+00, -5.5167e-02,\n",
              "           -9.4339e-01],\n",
              "          [ 3.3323e-01, -9.2821e-01,  1.0512e+00, -2.6472e-01,  1.5545e-01,\n",
              "            2.4298e-01,  1.8605e-01, -7.5045e-01, -7.1475e-01, -1.2581e+00,\n",
              "            8.5406e-01,  3.5898e-02, -2.0108e-01,  6.2148e-01,  1.4008e+00,\n",
              "            1.4714e-01]],\n",
              " \n",
              "         [[-3.5733e-01, -1.5769e-01,  8.7461e-01,  8.9048e-01,  8.9442e-02,\n",
              "           -8.0611e-01, -3.5569e-01, -2.7247e-02, -5.0592e-01,  4.1028e-02,\n",
              "            4.6415e-01, -1.1405e-01,  2.8087e-01,  7.2229e-01,  2.6094e-01,\n",
              "            8.7222e-01],\n",
              "          [-3.0244e-01,  9.0701e-01,  4.4406e-01, -3.9861e-01,  4.4104e-01,\n",
              "            6.3199e-01,  7.4795e-02, -6.5705e-01, -1.6145e-02,  7.4229e-03,\n",
              "            4.0191e-01, -1.0290e+00,  2.0908e-01,  1.0120e+00,  3.2669e-03,\n",
              "            7.6112e-01],\n",
              "          [-3.0877e-01,  9.0313e-01,  9.9187e-02, -1.4959e-01, -1.1250e-01,\n",
              "            1.0029e+00, -2.4594e-01, -7.1407e-01,  4.4710e-01,  4.8317e-01,\n",
              "            3.3011e-01, -3.3197e-01,  3.0020e-01, -1.6022e-01,  3.7184e-01,\n",
              "            4.6925e-01],\n",
              "          [ 6.2629e-01, -3.4878e-01,  1.3033e+00,  5.7958e-01,  2.6136e-01,\n",
              "           -1.0224e-01,  8.5234e-02,  2.3423e-01, -3.9120e-01,  4.2453e-01,\n",
              "           -6.1634e-01,  2.7928e-01,  1.4158e-01, -2.7001e-01, -4.6687e-01,\n",
              "           -1.3562e-02],\n",
              "          [ 5.6956e-01,  1.3337e-02,  5.9168e-01, -1.1253e+00,  3.9388e-01,\n",
              "           -4.4089e-01, -4.2610e-01, -1.0432e+00, -5.8486e-01, -1.0351e+00,\n",
              "            4.3401e-01, -5.3337e-01, -7.3336e-01,  3.2373e-01,  5.5624e-01,\n",
              "            4.1582e-02],\n",
              "          [ 3.1615e-01,  2.4120e-01, -4.5685e-01, -2.4793e-01, -1.1462e+00,\n",
              "            7.4820e-01,  4.6476e-01, -8.1004e-01, -1.4184e+00,  8.7592e-02,\n",
              "            1.3326e+00, -6.4656e-01,  8.2517e-02, -5.7269e-01,  1.8852e-01,\n",
              "            1.3291e-01],\n",
              "          [-6.8609e-01, -2.9221e-01,  5.7635e-01,  1.8069e-01,  5.6319e-01,\n",
              "           -7.5944e-01, -2.5030e-01,  9.8709e-02,  9.6521e-01, -8.0429e-01,\n",
              "           -1.9949e-01, -4.5325e-01,  1.3065e-01, -3.6012e-01, -2.9152e-01,\n",
              "           -7.2662e-01],\n",
              "          [-8.5994e-01, -4.2453e-02,  3.5626e-01, -5.5051e-01,  2.5676e-01,\n",
              "           -4.7726e-02, -6.2218e-01, -2.1031e-01, -2.4271e-01,  6.0195e-01,\n",
              "           -4.6007e-01, -6.2425e-01, -1.0708e-01, -9.8227e-02,  3.7431e-02,\n",
              "            4.3317e-01]],\n",
              " \n",
              "         [[-8.2276e-01, -9.9295e-01,  1.0113e+00,  1.0234e+00,  1.9395e-01,\n",
              "            4.2578e-01,  2.7926e-01,  1.0643e+00,  1.3757e+00, -4.7991e-02,\n",
              "           -2.9590e-02,  1.1909e-01,  1.0557e+00,  3.3792e-02, -8.2247e-03,\n",
              "            2.0240e-01],\n",
              "          [ 3.1238e-01,  2.2844e-01,  6.4302e-04, -8.2334e-01, -7.6817e-01,\n",
              "           -9.3243e-02,  9.3460e-03,  5.6534e-01, -7.5477e-01,  3.5579e-01,\n",
              "            1.3711e-01,  1.4269e-01, -3.5309e-01, -1.3453e+00,  2.1456e-01,\n",
              "            3.2167e-02],\n",
              "          [ 4.2970e-01,  3.5411e-02, -3.2090e-01,  4.9079e-01,  8.3846e-01,\n",
              "            8.1087e-01,  2.0658e-01,  4.2375e-01,  5.5859e-01, -7.6772e-01,\n",
              "            4.3672e-01, -9.9894e-03,  7.6776e-02, -2.2164e-01, -1.0343e-01,\n",
              "           -5.2503e-01],\n",
              "          [ 3.3557e-01, -4.6170e-01,  1.1774e-01, -6.8915e-01, -6.2666e-01,\n",
              "           -2.1530e-01,  1.3028e-01,  1.4123e+00,  3.6235e-01,  9.8717e-01,\n",
              "           -1.0085e+00,  8.0822e-01, -8.6478e-01,  3.0834e-01, -7.9436e-01,\n",
              "           -7.8427e-01],\n",
              "          [-3.0917e-01, -2.6590e-02, -1.1543e+00,  4.7104e-01, -5.9879e-02,\n",
              "            6.8905e-01,  3.6685e-01,  2.7240e-01,  1.0275e+00,  7.2176e-01,\n",
              "            6.4324e-01, -6.6101e-01,  1.8034e+00, -1.9369e-01,  2.5314e-01,\n",
              "            7.1237e-01],\n",
              "          [ 6.6919e-01, -5.0430e-01, -3.4585e-01,  3.5411e-02, -1.7505e-01,\n",
              "            8.1312e-01, -5.4094e-01, -7.2517e-02, -2.5174e-01,  5.8180e-01,\n",
              "           -4.2500e-01, -4.4698e-01,  6.8194e-01,  4.3999e-01, -5.7076e-01,\n",
              "           -7.4514e-01],\n",
              "          [-2.2773e-01, -1.5661e-01,  6.4844e-01, -2.1879e-01,  2.4838e-01,\n",
              "           -3.3245e-01, -1.9736e-01, -2.3652e-01,  5.0828e-01, -1.3174e-01,\n",
              "           -7.8329e-01, -4.1429e-01, -6.7991e-01, -1.2624e-01,  1.2754e-01,\n",
              "            5.9409e-01],\n",
              "          [ 8.2953e-01, -1.3034e-01,  8.7725e-01,  7.2300e-01, -5.6100e-01,\n",
              "           -4.4817e-01,  3.5008e-01,  5.2952e-01, -3.8714e-02, -6.1440e-03,\n",
              "            8.2686e-02,  2.3892e-01, -8.2022e-01,  3.7937e-01, -8.3004e-01,\n",
              "            9.4499e-02]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k = key(x) # Per batch har 8 tokens have a key vector.\n",
        "k.shape, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CntwnA7WbgU",
        "outputId": "d0aac90b-df8d-4c06-aa26-818198aedab3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]),\n",
              " tensor([[[ 8.2798e-02,  3.7758e-01, -1.5011e-01, -1.6374e-01,  8.0848e-01,\n",
              "            4.0795e-01,  2.6162e-01,  6.9286e-02,  3.4164e-01,  7.5425e-02,\n",
              "           -3.6533e-01,  3.0754e-01, -2.5684e-01,  3.8492e-01, -4.5687e-01,\n",
              "            4.6824e-01],\n",
              "          [-7.7159e-02,  1.1028e+00,  2.3982e-01, -7.7130e-02,  5.9606e-01,\n",
              "           -4.7385e-01,  1.3238e+00, -3.9902e-01, -8.4110e-01,  1.1358e+00,\n",
              "            1.9339e-01,  1.2161e-01,  1.1745e+00, -3.7020e-01,  6.9384e-01,\n",
              "            1.1583e-01],\n",
              "          [-1.5508e+00, -1.0002e+00,  4.0072e-01, -1.6033e+00, -1.1085e-01,\n",
              "           -1.1220e+00, -3.3912e-02, -3.9096e-01, -7.7762e-02,  3.7208e-01,\n",
              "           -1.2672e+00,  1.0435e+00,  2.7223e-01,  3.0767e-01, -5.5513e-01,\n",
              "            9.9991e-01],\n",
              "          [-2.0473e-01,  3.6583e-01,  8.3551e-01,  1.1878e-01, -2.2045e-01,\n",
              "           -1.1245e-01, -1.7858e-01,  3.0638e-01,  3.3029e-01,  2.6949e-01,\n",
              "           -2.1673e-01, -1.1344e-01,  3.5774e-01, -2.2349e-01,  8.7292e-03,\n",
              "            8.6606e-04],\n",
              "          [ 9.0416e-01, -4.4854e-02,  3.8865e-01,  1.5979e-01, -3.0295e-01,\n",
              "           -5.7020e-01,  4.3479e-01, -5.7326e-02, -3.1694e-01, -4.7686e-02,\n",
              "            4.8621e-01, -1.6328e-02,  2.4160e-02, -4.3843e-01,  5.2077e-01,\n",
              "            4.6365e-01],\n",
              "          [-1.3848e-01, -5.0525e-01,  9.1254e-01, -3.1702e-01,  4.0468e-01,\n",
              "           -8.3876e-01,  4.7142e-02, -8.8800e-01, -4.5796e-01, -2.1162e-01,\n",
              "           -1.0315e+00,  5.9535e-01,  3.2087e-01, -1.6374e-02,  2.0590e-01,\n",
              "            8.8468e-02],\n",
              "          [-1.7057e-01, -1.7320e-01, -9.2826e-01,  4.9782e-01, -9.8775e-01,\n",
              "           -5.6833e-02, -6.6358e-01,  3.8194e-01,  7.1767e-01, -1.0008e+00,\n",
              "            5.0118e-01,  3.6725e-01, -4.6600e-01, -2.0480e-01,  5.3991e-01,\n",
              "           -2.6163e-01],\n",
              "          [-1.0684e+00, -3.6893e-01,  1.2834e-01,  5.4003e-01,  7.5755e-01,\n",
              "            2.8272e-01,  1.9221e-01,  4.0785e-01,  9.5933e-01, -5.5190e-01,\n",
              "            5.2850e-01, -6.9125e-01, -5.1715e-01,  1.4009e+00, -2.2613e-01,\n",
              "           -2.5003e-01]],\n",
              " \n",
              "         [[-1.4917e+00, -1.5871e-01, -8.8827e-02,  6.9491e-01, -2.6645e-01,\n",
              "            3.7934e-01,  2.0593e-01, -3.6161e-01, -1.7118e-02, -8.5387e-01,\n",
              "           -1.2592e+00, -3.1445e-01, -1.8867e-01,  3.6034e-01,  4.6171e-01,\n",
              "           -3.1504e-01],\n",
              "          [-6.6530e-01, -3.4462e-01,  1.5705e-01,  8.9439e-03,  6.3508e-01,\n",
              "           -1.1818e+00, -3.2444e-01, -5.8125e-01, -9.9379e-01, -1.0350e+00,\n",
              "            8.8830e-01,  1.0651e-01, -6.8806e-01,  5.8098e-01, -1.2492e+00,\n",
              "           -6.2366e-01],\n",
              "          [ 8.7184e-01, -1.0263e+00,  2.4158e-02,  2.7390e-01,  3.2808e-01,\n",
              "            7.6992e-01,  1.2384e-02,  7.4635e-02,  2.7678e-01, -7.8106e-03,\n",
              "           -6.4529e-01,  2.4787e-02,  6.9280e-01, -7.4817e-01,  4.2635e-01,\n",
              "            2.7911e-01],\n",
              "          [ 1.2161e+00, -2.2055e-01, -1.5497e-01, -3.1353e-01,  6.1975e-02,\n",
              "            3.1090e-01,  1.4490e+00,  2.7432e-02, -1.9335e-02,  3.5464e-01,\n",
              "           -4.3359e-01, -8.3013e-01,  1.2706e+00,  5.3552e-01,  6.1115e-01,\n",
              "            2.8793e-02],\n",
              "          [ 6.7506e-01,  4.0806e-02,  2.9231e-01, -3.4221e-01, -2.1290e-01,\n",
              "           -1.6206e-01,  7.4067e-01,  1.5669e-01, -1.4750e-01,  1.1311e+00,\n",
              "            3.8233e-01,  5.9118e-01, -1.0518e-01,  6.8883e-01,  5.5748e-01,\n",
              "           -9.3466e-02],\n",
              "          [-1.6034e-01,  5.1773e-01,  6.2550e-01, -5.4457e-01,  6.1848e-01,\n",
              "            1.2329e-01, -3.3068e-01, -5.8118e-01,  1.4099e-02, -4.3832e-01,\n",
              "           -5.5282e-02, -1.6160e-01, -3.7644e-02,  3.8007e-01, -3.0271e-01,\n",
              "           -3.0672e-01],\n",
              "          [ 3.4165e-01,  1.0656e+00, -1.0667e+00,  3.8168e-02, -5.5313e-01,\n",
              "            1.0700e+00, -1.5119e+00,  4.4467e-01,  3.4081e-01, -2.1923e-01,\n",
              "            3.5075e-01,  2.8797e-01, -3.3950e-01, -1.1009e+00, -5.8295e-01,\n",
              "            1.3267e-01],\n",
              "          [ 6.3461e-01, -9.8825e-01, -1.5567e-01, -7.6281e-01, -7.3161e-01,\n",
              "           -1.0594e+00, -1.5684e+00, -3.3569e-02, -1.0364e+00, -6.8814e-01,\n",
              "            5.4981e-01, -1.5283e+00, -4.4023e-02, -1.4638e+00, -8.2864e-01,\n",
              "           -9.0721e-02]],\n",
              " \n",
              "         [[ 3.2466e-01, -8.4890e-01,  1.7382e-01,  1.3632e-01, -1.5118e-01,\n",
              "           -3.8294e-01,  1.1476e+00, -6.1355e-01, -4.4431e-02, -3.2999e-01,\n",
              "           -9.6186e-01, -4.5620e-01,  1.1288e+00, -2.7855e-01,  9.4948e-01,\n",
              "           -8.1069e-01],\n",
              "          [ 3.1738e-01, -7.0222e-01, -7.0348e-01, -5.9103e-01, -3.5213e-02,\n",
              "            2.4301e-03, -4.2629e-01,  1.2069e+00,  5.2442e-01,  3.9214e-01,\n",
              "            3.3406e-01,  4.4723e-03, -5.0785e-01,  6.3775e-01,  1.0099e+00,\n",
              "            6.6576e-01],\n",
              "          [-4.4487e-01,  5.2918e-02,  4.7698e-01, -1.0398e-01, -2.8811e-01,\n",
              "            1.7818e-02,  6.8797e-01, -1.9307e-02,  1.6221e-01,  2.8194e-01,\n",
              "           -9.5580e-01,  4.2772e-01,  1.5950e-01,  5.0228e-02,  9.3774e-01,\n",
              "            1.8020e-01],\n",
              "          [ 3.5782e-02,  7.3097e-01,  2.7974e-01,  8.0132e-01, -6.0388e-02,\n",
              "            1.1701e-01,  9.3471e-02, -3.6190e-01,  8.9711e-02,  3.3207e-01,\n",
              "            6.6056e-01,  2.1789e-01, -6.4715e-01, -1.9602e-01, -5.7438e-02,\n",
              "           -9.3035e-02],\n",
              "          [ 3.0187e-01, -6.2214e-01, -1.9050e-01,  3.0311e-01, -3.2268e-01,\n",
              "            7.0249e-01,  3.2058e-01,  5.6791e-02,  6.6721e-01,  1.3263e+00,\n",
              "            1.8970e-01,  1.2694e-01,  7.2700e-01, -1.4064e+00, -3.8976e-01,\n",
              "            5.1011e-01],\n",
              "          [ 1.3954e+00, -2.4808e-01,  6.4621e-01, -9.6870e-01, -1.0560e-01,\n",
              "            4.9071e-01,  6.8530e-01,  4.6551e-01,  4.4766e-01,  2.0391e-01,\n",
              "           -3.8071e-01, -1.8744e-01,  3.7724e-01,  6.0834e-01,  8.7333e-01,\n",
              "            1.1514e-01],\n",
              "          [ 2.5761e-02, -7.8872e-01,  2.8492e-01,  6.5191e-01, -9.5690e-01,\n",
              "            2.9220e-01, -4.5552e-01,  1.5351e-01,  1.5106e+00, -6.8548e-01,\n",
              "            1.0589e+00, -2.9888e-01, -2.9993e-01,  2.6034e-01, -1.5050e-01,\n",
              "           -1.9047e-01],\n",
              "          [ 4.7253e-01,  1.5002e-01, -5.7333e-01, -6.1101e-01,  9.8098e-02,\n",
              "           -1.0011e-01, -1.6056e-01,  8.9594e-01, -4.3379e-01,  2.1805e-01,\n",
              "            2.6121e-01,  6.0310e-01,  1.1285e-01,  3.0800e-01,  9.9477e-01,\n",
              "            3.4438e-01]],\n",
              " \n",
              "         [[-4.5478e-01,  6.3658e-03,  4.4540e-01, -4.6680e-01,  4.9885e-01,\n",
              "           -1.4667e+00,  9.9050e-01, -8.3053e-01, -7.7686e-01,  3.1810e-02,\n",
              "           -7.7713e-01,  1.2650e+00,  3.3034e-01, -2.5629e-01, -1.7508e-04,\n",
              "           -5.3016e-01],\n",
              "          [ 3.3309e-01, -6.9981e-01,  3.4845e-01, -2.2417e-01, -2.3957e-01,\n",
              "            1.6245e+00,  5.6312e-01,  1.1419e-01,  8.3629e-01, -3.8367e-01,\n",
              "            6.0994e-02,  7.0661e-02, -5.2458e-01,  1.2934e+00, -5.0615e-02,\n",
              "            3.3461e-01],\n",
              "          [ 1.9742e-01, -6.3358e-01, -5.8668e-01, -7.3279e-02, -6.0474e-02,\n",
              "           -1.2906e+00, -8.7424e-01, -1.2346e-01,  2.7012e-01,  6.4102e-01,\n",
              "            2.8115e-01,  3.2878e-01, -3.6859e-02, -2.0405e-01,  4.0828e-01,\n",
              "           -1.4008e-01],\n",
              "          [-3.8180e-01,  3.1752e-01, -6.3914e-01, -3.9584e-01,  7.5952e-01,\n",
              "            6.9516e-02,  1.0314e+00, -1.0644e+00, -4.5615e-01,  6.4345e-01,\n",
              "            2.5648e-01, -8.8720e-02,  2.6870e-01,  5.8802e-01, -6.8756e-01,\n",
              "           -1.0584e+00],\n",
              "          [-2.2933e-01,  9.8429e-01,  1.4174e+00,  4.4096e-01, -2.5720e-01,\n",
              "            7.3647e-01,  7.3898e-01, -2.7771e-01,  6.2169e-01, -3.5256e-01,\n",
              "           -7.8377e-01,  3.2693e-01,  4.2181e-01, -6.3467e-02, -2.5426e-01,\n",
              "            1.4278e-01],\n",
              "          [ 3.3553e-01, -3.2045e-02,  3.3188e-02, -7.3278e-01,  7.6319e-01,\n",
              "            5.8036e-02,  5.0493e-01,  4.3658e-01,  3.2510e-01,  4.5201e-01,\n",
              "            4.7325e-01, -2.0759e-01,  2.4714e-02,  1.0033e+00,  2.0754e-01,\n",
              "            3.7906e-03],\n",
              "          [-2.1385e-01,  8.2908e-02,  1.1029e+00,  3.6717e-01,  5.4085e-01,\n",
              "            1.3266e-01,  3.8806e-01, -6.5048e-01, -1.0955e+00,  4.2928e-01,\n",
              "           -3.7181e-01,  3.4088e-01,  5.0547e-01, -2.5925e-01,  7.8183e-02,\n",
              "           -1.4885e-01],\n",
              "          [ 4.5747e-01, -2.7411e-01,  6.5762e-01,  2.6005e-01,  8.1769e-01,\n",
              "           -4.8519e-01,  9.2733e-01, -1.1989e+00, -3.2536e-01,  9.4376e-01,\n",
              "           -4.5828e-01, -6.0607e-01,  5.2385e-01,  1.2948e-01, -9.0952e-02,\n",
              "           -7.7330e-01]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q = query(x) # Per batch every token in Time T has a head-size (16) length query vector.\n",
        "q.shape, q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNN_4y96WjyV",
        "outputId": "71b6e475-3099-49ef-dca0-fc51bd639260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ab inhe multiple karna hai. Like q at i for all the keys before i.\n",
        "# And result mujhe T, T me laana hai (8, 8)\n",
        "\n",
        "k.shape, q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPa1DAPwXDDn",
        "outputId": "e78e6bd1-39d8-4b2d-c69a-283913e9b77c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 16]), torch.Size([8, 16]))"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simplify-> 1 batch.\n",
        "\n",
        "# q1 q1\n",
        "# q2 q2\n",
        "\n",
        "# k1 k1 k1 k1 -> Key vector for the first token.\n",
        "# k2 k2 k2 k2 -> Key vector for the second token.\n",
        "\n",
        "q[0].shape, k[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7myK9ifXjpi",
        "outputId": "e17f1dc3-76bd-4dfd-9733-a91766e270c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 8])"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k[0].T.shape # Transpose keys.\n",
        "# k1 k2\n",
        "# k1 k2\n",
        "# k1 k2\n",
        "# k1 k2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm5YyYmzYKnT"
      },
      "source": [
        "Every query of my ith is being multiplied by the key of others in the past."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr0Q4XOIX6Fc",
        "outputId": "d1a4ac9a-82ce-44ec-c925-caa8da2ca7d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.4833e+00,  4.4066e-01,  6.9455e-01, -1.5006e+00, -1.1953e+00,\n",
              "         -5.6557e-01,  5.2630e-01,  4.1084e-01],\n",
              "        [-3.1310e-03, -2.1112e+00,  1.3183e-01, -5.1706e-01,  2.6835e-01,\n",
              "          7.7266e-01,  1.1052e+00, -3.9765e-01],\n",
              "        [-6.9502e-01,  3.8330e+00,  4.9514e-01, -7.0815e-01, -2.5026e+00,\n",
              "          3.3687e-01,  8.1964e-01,  1.6262e+00],\n",
              "        [ 5.6323e-01,  1.1812e+00,  1.2683e+00,  3.3366e-01,  6.9156e-01,\n",
              "          1.2798e+00, -3.9815e-01,  7.4339e-01],\n",
              "        [-5.3599e-01, -1.5408e+00, -6.0025e-01,  7.7762e-01,  1.1905e+00,\n",
              "          5.5721e-01, -9.1375e-01, -3.9100e-01],\n",
              "        [-2.1092e-01,  2.6348e+00, -1.6495e+00,  9.9660e-01,  9.4299e-01,\n",
              "         -3.7353e-01, -1.1364e-01,  1.1746e+00],\n",
              "        [-1.4393e+00, -1.8231e+00,  1.1462e+00,  2.0692e-01, -2.1074e-02,\n",
              "          9.2613e-01, -1.6808e-01, -1.4690e+00],\n",
              "        [-3.9401e-01,  1.8549e+00,  2.4465e+00, -1.1454e-01,  7.9159e-02,\n",
              "          1.9028e+00,  5.6284e-01,  2.4558e-01]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Exactly what you want.\n",
        "(q[0] @ k[0].T) # -> Every query in a row is being multipled with evry key of its own and previous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXWFA9zUYS7i",
        "outputId": "fd89f524-34c0-4d7d-8ba6-8be683f9ca6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]),\n",
              " torch.Size([8, 4, 16]),\n",
              " torch.Size([4, 16, 8]),\n",
              " torch.Size([16, 8, 4]))"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we want to broadcast this operation across batches.\n",
        "\n",
        "k.shape, k.transpose(-3, -2).shape, k.transpose(-2, -1).shape, k.transpose(-1, -3).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuPeXGqdY2fS",
        "outputId": "7d3fed01-5085-425d-80e8-6046d8b2341f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 8])"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(q @ k.transpose(-2, -1)).shape # Now we will have different T, T weights for all the batches.\n",
        "# Which is even better. As every batch is a bit different, right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7etAMVG6gFO8",
        "outputId": "9a89affc-0c70-45cf-8842-77f30f9dd97c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4833e+00,  4.4066e-01,  6.9455e-01, -1.5006e+00, -1.1953e+00,\n",
              "          -5.6557e-01,  5.2630e-01,  4.1084e-01],\n",
              "         [-3.1310e-03, -2.1112e+00,  1.3183e-01, -5.1706e-01,  2.6835e-01,\n",
              "           7.7266e-01,  1.1052e+00, -3.9765e-01],\n",
              "         [-6.9502e-01,  3.8330e+00,  4.9514e-01, -7.0815e-01, -2.5026e+00,\n",
              "           3.3687e-01,  8.1964e-01,  1.6262e+00],\n",
              "         [ 5.6323e-01,  1.1812e+00,  1.2683e+00,  3.3366e-01,  6.9156e-01,\n",
              "           1.2798e+00, -3.9815e-01,  7.4339e-01],\n",
              "         [-5.3599e-01, -1.5408e+00, -6.0025e-01,  7.7762e-01,  1.1905e+00,\n",
              "           5.5721e-01, -9.1375e-01, -3.9100e-01],\n",
              "         [-2.1092e-01,  2.6348e+00, -1.6495e+00,  9.9660e-01,  9.4299e-01,\n",
              "          -3.7353e-01, -1.1364e-01,  1.1746e+00],\n",
              "         [-1.4393e+00, -1.8231e+00,  1.1462e+00,  2.0692e-01, -2.1074e-02,\n",
              "           9.2613e-01, -1.6808e-01, -1.4690e+00],\n",
              "         [-3.9401e-01,  1.8549e+00,  2.4465e+00, -1.1454e-01,  7.9159e-02,\n",
              "           1.9028e+00,  5.6284e-01,  2.4558e-01]],\n",
              "\n",
              "        [[ 1.7390e+00,  1.7094e+00, -7.6062e-01, -1.1312e+00, -1.6115e+00,\n",
              "           8.8230e-01, -3.6549e-01,  5.9537e-01],\n",
              "         [ 4.9264e-01, -2.2031e+00,  2.4449e+00, -1.1791e-01,  1.7007e+00,\n",
              "           5.1249e-02,  1.6541e+00,  1.8810e+00],\n",
              "         [-8.3228e-01,  2.6444e+00, -1.9952e+00,  1.6956e-01, -9.6132e-02,\n",
              "          -4.7634e-01, -6.3259e-01,  6.7621e-01],\n",
              "         [-3.4974e+00,  1.9659e-02, -1.0767e+00,  2.1513e+00,  6.0297e-02,\n",
              "           3.7641e-01,  1.0456e-01,  9.6940e-01],\n",
              "         [-1.1314e+00, -2.5160e-01, -3.0023e-01, -6.2036e-01, -5.3172e-01,\n",
              "          -3.1924e-01, -9.5981e-01,  7.7921e-01],\n",
              "         [ 8.9230e-01, -1.8672e+00,  1.1730e+00,  2.3550e-01,  1.3229e+00,\n",
              "          -6.4396e-01,  8.2970e-02,  1.0314e+00],\n",
              "         [ 2.9964e-01, -1.4276e+00,  4.5949e-01,  1.3701e+00, -2.1535e-01,\n",
              "           9.8680e-01,  2.5143e+00, -3.5185e+00],\n",
              "         [-6.8141e-01, -1.7701e+00, -6.3570e-01,  2.0972e+00,  2.7350e+00,\n",
              "          -1.1018e+00,  3.0191e+00,  4.7561e-01]],\n",
              "\n",
              "        [[-5.3399e-01, -1.2437e+00, -1.0704e+00,  9.0416e-01,  1.5325e-01,\n",
              "           7.3029e-02,  1.1892e+00, -7.3146e-01],\n",
              "         [ 3.6905e-02, -4.7525e-01, -4.9809e-01, -1.5467e+00, -7.3753e-02,\n",
              "          -1.1302e+00, -1.2727e+00,  1.0410e-01],\n",
              "         [ 1.1091e-01, -2.1625e-01,  3.6722e-01,  5.7487e-01, -8.4908e-01,\n",
              "          -1.2115e+00, -2.7897e-01,  5.0289e-01],\n",
              "         [ 5.3757e-01,  3.8670e-01,  1.0350e+00,  2.4090e-01, -2.9414e-01,\n",
              "           1.0314e+00, -4.5476e-01, -6.2212e-01],\n",
              "         [-1.2952e+00, -1.5122e+00,  1.3997e+00,  1.0962e+00, -3.8553e+00,\n",
              "           1.1559e+00, -1.0439e+00, -5.9114e-02],\n",
              "         [-9.1818e-01,  8.5412e-01,  1.8692e-01,  9.0452e-01,  1.1010e+00,\n",
              "          -3.8470e-01, -1.3243e+00, -7.1211e-01],\n",
              "         [ 4.1317e-01, -4.6992e-01,  1.2940e-01, -9.0038e-01, -3.7497e-01,\n",
              "          -3.2052e-01,  1.8461e+00, -1.4160e+00],\n",
              "         [-2.0878e-02, -5.4490e-01, -3.7414e-01, -8.9224e-01,  2.4032e-01,\n",
              "           2.7004e-01, -2.1049e+00, -4.5521e-01]],\n",
              "\n",
              "        [[-1.4583e+00,  4.1997e-01, -1.9354e+00,  7.8515e-01, -3.3406e+00,\n",
              "          -1.7470e+00,  2.8430e-01, -1.9070e-01],\n",
              "         [ 2.2013e+00, -2.0728e+00,  1.4679e+00,  1.2155e+00,  3.8312e-01,\n",
              "           9.9695e-01,  3.5531e-01,  1.1615e+00],\n",
              "         [-8.4426e-01,  5.4600e-01, -1.2635e+00,  8.3507e-01,  3.8819e-02,\n",
              "          -1.1011e-01,  2.1762e-02, -3.9228e-01],\n",
              "         [-2.3021e+00, -1.3750e+00,  1.9442e-01, -4.7226e-01,  4.1381e-01,\n",
              "           1.2771e+00, -1.5310e+00, -1.3777e+00],\n",
              "         [ 2.6789e+00, -9.9912e-01,  3.8458e-01, -3.2415e-01, -1.9097e-01,\n",
              "          -7.3636e-01,  9.4632e-01,  1.0154e+00],\n",
              "         [ 2.6365e-01, -1.0024e+00,  5.4843e-01,  8.6698e-01,  8.1948e-01,\n",
              "           2.2186e-01, -2.1592e-01, -1.0179e-01],\n",
              "         [ 1.8026e-01,  1.8099e-02, -8.3803e-01, -1.2539e+00, -1.4167e+00,\n",
              "           1.2521e-01, -1.4739e-02, -2.8149e-02],\n",
              "         [-3.8792e-01, -1.3569e+00, -5.9427e-01, -7.5030e-01, -3.4202e-01,\n",
              "           1.4128e+00,  2.5006e-01,  7.4614e-02]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Operation across batches.\n",
        "\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd6Nmkp7gLm_",
        "outputId": "971338fa-b9d3-4fca-ccfc-bdfeb4272ea1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.4833e+00,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-3.1310e-03, -2.1112e+00,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-6.9502e-01,  3.8330e+00,  4.9514e-01,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 5.6323e-01,  1.1812e+00,  1.2683e+00,  3.3366e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-5.3599e-01, -1.5408e+00, -6.0025e-01,  7.7762e-01,  1.1905e+00,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-2.1092e-01,  2.6348e+00, -1.6495e+00,  9.9660e-01,  9.4299e-01,\n",
              "          -3.7353e-01,        -inf,        -inf],\n",
              "         [-1.4393e+00, -1.8231e+00,  1.1462e+00,  2.0692e-01, -2.1074e-02,\n",
              "           9.2613e-01, -1.6808e-01,        -inf],\n",
              "         [-3.9401e-01,  1.8549e+00,  2.4465e+00, -1.1454e-01,  7.9159e-02,\n",
              "           1.9028e+00,  5.6284e-01,  2.4558e-01]],\n",
              "\n",
              "        [[ 1.7390e+00,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 4.9264e-01, -2.2031e+00,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-8.3228e-01,  2.6444e+00, -1.9952e+00,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-3.4974e+00,  1.9659e-02, -1.0767e+00,  2.1513e+00,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-1.1314e+00, -2.5160e-01, -3.0023e-01, -6.2036e-01, -5.3172e-01,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 8.9230e-01, -1.8672e+00,  1.1730e+00,  2.3550e-01,  1.3229e+00,\n",
              "          -6.4396e-01,        -inf,        -inf],\n",
              "         [ 2.9964e-01, -1.4276e+00,  4.5949e-01,  1.3701e+00, -2.1535e-01,\n",
              "           9.8680e-01,  2.5143e+00,        -inf],\n",
              "         [-6.8141e-01, -1.7701e+00, -6.3570e-01,  2.0972e+00,  2.7350e+00,\n",
              "          -1.1018e+00,  3.0191e+00,  4.7561e-01]],\n",
              "\n",
              "        [[-5.3399e-01,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 3.6905e-02, -4.7525e-01,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 1.1091e-01, -2.1625e-01,  3.6722e-01,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 5.3757e-01,  3.8670e-01,  1.0350e+00,  2.4090e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-1.2952e+00, -1.5122e+00,  1.3997e+00,  1.0962e+00, -3.8553e+00,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-9.1818e-01,  8.5412e-01,  1.8692e-01,  9.0452e-01,  1.1010e+00,\n",
              "          -3.8470e-01,        -inf,        -inf],\n",
              "         [ 4.1317e-01, -4.6992e-01,  1.2940e-01, -9.0038e-01, -3.7497e-01,\n",
              "          -3.2052e-01,  1.8461e+00,        -inf],\n",
              "         [-2.0878e-02, -5.4490e-01, -3.7414e-01, -8.9224e-01,  2.4032e-01,\n",
              "           2.7004e-01, -2.1049e+00, -4.5521e-01]],\n",
              "\n",
              "        [[-1.4583e+00,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 2.2013e+00, -2.0728e+00,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-8.4426e-01,  5.4600e-01, -1.2635e+00,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-2.3021e+00, -1.3750e+00,  1.9442e-01, -4.7226e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 2.6789e+00, -9.9912e-01,  3.8458e-01, -3.2415e-01, -1.9097e-01,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 2.6365e-01, -1.0024e+00,  5.4843e-01,  8.6698e-01,  8.1948e-01,\n",
              "           2.2186e-01,        -inf,        -inf],\n",
              "         [ 1.8026e-01,  1.8099e-02, -8.3803e-01, -1.2539e+00, -1.4167e+00,\n",
              "           1.2521e-01, -1.4739e-02,        -inf],\n",
              "         [-3.8792e-01, -1.3569e+00, -5.9427e-01, -7.5030e-01, -3.4202e-01,\n",
              "           1.4128e+00,  2.5006e-01,  7.4614e-02]]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Bradcasting across batches.\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # Decoder mechanism. (Future tokens can't communicate)/ Which makes sense here.\n",
        "wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2XEw2Kw_48"
      },
      "source": [
        "### Dim = 1 or -1 for softmax? Answer: -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc0jCNDZwKJB",
        "outputId": "71ffd604-2d1a-4b1a-b87f-de00411dfbf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8917, 0.1083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0103, 0.9557, 0.0339, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1762, 0.3270, 0.3567, 0.1401, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0859, 0.0314, 0.0805, 0.3194, 0.4827, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0387, 0.6668, 0.0092, 0.1296, 0.1228, 0.0329, 0.0000, 0.0000],\n",
              "         [0.0260, 0.0177, 0.3448, 0.1348, 0.1073, 0.2767, 0.0926, 0.0000],\n",
              "         [0.0222, 0.2107, 0.3808, 0.0294, 0.0357, 0.2211, 0.0579, 0.0422]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[0.4422, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1000, 0.0017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0501, 0.6573, 0.0795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1762, 0.0464, 0.1723, 0.1661, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0587, 0.0030, 0.0266, 0.2590, 0.4153, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0813, 0.1983, 0.0093, 0.3224, 0.3243, 0.0694, 0.0000, 0.0000],\n",
              "         [0.0238, 0.0023, 0.1525, 0.1464, 0.1237, 0.2546, 0.3250, 0.0000],\n",
              "         [0.0677, 0.0909, 0.5597, 0.1061, 0.1367, 0.6760, 0.6750, 1.0000]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0].softmax(dim = 1), wei[0].softmax(dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODtpIn6qw2Pu",
        "outputId": "99ae4fec-b8dd-492e-c312-f48b76b61681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8917, 0.1083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0103, 0.9557, 0.0339, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1762, 0.3270, 0.3567, 0.1401, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0859, 0.0314, 0.0805, 0.3194, 0.4827, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0387, 0.6668, 0.0092, 0.1296, 0.1228, 0.0329, 0.0000, 0.0000],\n",
              "         [0.0260, 0.0177, 0.3448, 0.1348, 0.1073, 0.2767, 0.0926, 0.0000],\n",
              "         [0.0222, 0.2107, 0.3808, 0.0294, 0.0357, 0.2211, 0.0579, 0.0422]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.9368, 0.0632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0297, 0.9610, 0.0093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0030, 0.1021, 0.0341, 0.8607, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1088, 0.2621, 0.2497, 0.1813, 0.1981, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2146, 0.0136, 0.2842, 0.1113, 0.3301, 0.0462, 0.0000, 0.0000],\n",
              "         [0.0588, 0.0105, 0.0690, 0.1715, 0.0351, 0.1169, 0.5383, 0.0000],\n",
              "         [0.0107, 0.0036, 0.0112, 0.1726, 0.3267, 0.0070, 0.4340, 0.0341]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6253, 0.3747, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3319, 0.2393, 0.4288, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2354, 0.2025, 0.3871, 0.1750, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0362, 0.0291, 0.5361, 0.3957, 0.0028, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0395, 0.2323, 0.1192, 0.2443, 0.2974, 0.0673, 0.0000, 0.0000],\n",
              "         [0.1323, 0.0547, 0.0996, 0.0356, 0.0601, 0.0635, 0.5543, 0.0000],\n",
              "         [0.1634, 0.0967, 0.1147, 0.0683, 0.2121, 0.2185, 0.0203, 0.1058]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.9863, 0.0137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1763, 0.7078, 0.1159, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0457, 0.1154, 0.5543, 0.2846, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8114, 0.0205, 0.0818, 0.0403, 0.0460, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1400, 0.0395, 0.1861, 0.2560, 0.2441, 0.1343, 0.0000, 0.0000],\n",
              "         [0.2262, 0.1923, 0.0817, 0.0539, 0.0458, 0.2140, 0.1861, 0.0000],\n",
              "         [0.0742, 0.0282, 0.0604, 0.0517, 0.0777, 0.4494, 0.1405, 0.1179]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Across batches.\n",
        "\n",
        "wei = wei.softmax(dim = -1)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT8GK3t6gev6",
        "outputId": "d017c2a2-f759-4931-bf64-de08ebce7b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[2.0071e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.5848e-01, 8.8304e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [6.5133e-02, 7.6257e-02, 3.2374e-01, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [3.4626e-02, 8.6648e-02, 8.7932e-02, 1.7886e-01, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.6026e-01, 1.2114e-01, 1.9711e-01, 8.5972e-02, 2.8101e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [5.7162e-02, 1.1309e-01, 1.0710e-01, 3.5905e-01, 2.0909e-01,\n",
              "          2.4342e-01, 0.0000e+00, 0.0000e+00],\n",
              "         [7.8271e-02, 9.8534e-02, 2.1223e-01, 2.3451e-01, 9.3926e-02,\n",
              "          2.3527e-01, 6.6809e-01, 0.0000e+00],\n",
              "         [4.5354e-02, 4.1603e-01, 7.1894e-02, 1.4162e-01, 4.1598e-01,\n",
              "          5.2131e-01, 3.3191e-01, 1.0000e+00]],\n",
              "\n",
              "        [[2.0877e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [7.5346e-02, 1.1783e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [8.5850e-03, 2.8004e-01, 2.0809e-01, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [3.5330e-02, 9.4566e-02, 2.1250e-01, 6.8687e-02, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [5.9940e-02, 9.3570e-02, 1.1212e-01, 4.0717e-02, 1.4242e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [4.1659e-01, 1.6728e-02, 1.9160e-01, 8.5234e-01, 1.9226e-01,\n",
              "          1.7522e-02, 0.0000e+00, 0.0000e+00],\n",
              "         [1.7255e-01, 8.0082e-02, 2.6066e-01, 1.7627e-02, 4.6648e-01,\n",
              "          4.3649e-01, 4.6013e-01, 0.0000e+00],\n",
              "         [2.2880e-02, 4.2323e-01, 1.5033e-02, 2.0625e-02, 1.9883e-01,\n",
              "          5.4599e-01, 5.3987e-01, 1.0000e+00]],\n",
              "\n",
              "        [[3.6460e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.4414e-02, 5.0446e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [4.0414e-02, 1.0865e-01, 9.2286e-02, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.4226e-03, 1.0586e-01, 2.1484e-01, 5.7997e-02, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.0189e-04, 1.8620e-02, 3.5961e-02, 2.4333e-02, 1.7334e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [1.8797e-01, 1.7882e-01, 5.4902e-02, 2.1370e-01, 3.1303e-01,\n",
              "          1.4147e-01, 0.0000e+00, 0.0000e+00],\n",
              "         [6.9271e-01, 3.4063e-02, 1.3945e-01, 4.8780e-01, 8.2344e-02,\n",
              "          5.4659e-01, 8.5057e-01, 0.0000e+00],\n",
              "         [5.1504e-02, 4.9525e-02, 4.6256e-01, 2.1617e-01, 4.3129e-01,\n",
              "          3.1194e-01, 1.4943e-01, 1.0000e+00]],\n",
              "\n",
              "        [[1.1377e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [8.2129e-02, 3.1823e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.9275e-01, 2.0321e-01, 1.3278e-02, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [4.1911e-02, 1.0657e-01, 9.3318e-03, 2.0605e-02, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [2.7150e-02, 3.0627e-01, 8.7000e-01, 5.6989e-01, 4.0733e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "         [3.6961e-01, 3.3737e-02, 1.5187e-02, 1.2873e-01, 4.1064e-01,\n",
              "          1.9076e-01, 0.0000e+00, 0.0000e+00],\n",
              "         [1.2633e-01, 6.3694e-02, 5.0626e-03, 7.3796e-02, 1.3606e-01,\n",
              "          7.1653e-01, 9.4665e-01, 0.0000e+00],\n",
              "         [4.8738e-02, 2.5470e-01, 8.7138e-02, 2.0697e-01, 4.5972e-02,\n",
              "          9.2716e-02, 5.3355e-02, 1.0000e+00]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Across batches.\n",
        "\n",
        "wei = wei.softmax(1)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2plDfMChG2a",
        "outputId": "08855c06-b168-458b-c3ca-67ff7be6a9d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 8]), torch.Size([4, 8, 32]))"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1 way is\n",
        "\n",
        "wei.shape, x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQdh_011hV3-",
        "outputId": "a5f6e7df-7dac-4955-e4dd-856b4ece6708"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.8460e-01, -1.4461e-01,  9.0019e-02,  ...,  1.7141e-01,\n",
              "          -4.9286e-02,  6.7666e-02],\n",
              "         [-3.1187e-01, -1.9069e-01,  8.5496e-03,  ...,  5.0071e-02,\n",
              "          -4.4716e-02,  9.9576e-02],\n",
              "         [-1.1831e-01, -2.7097e-01, -2.2114e-01,  ..., -5.0821e-01,\n",
              "           5.3429e-01,  1.5294e-01],\n",
              "         ...,\n",
              "         [-2.9712e-01, -2.8764e-01,  2.6062e-01,  ...,  1.0546e-01,\n",
              "           4.2910e-01, -1.3861e-01],\n",
              "         [-2.2432e-02, -9.1413e-01, -7.7480e-01,  ...,  7.5201e-01,\n",
              "           9.6961e-01, -2.6405e-02],\n",
              "         [-2.1500e-01, -2.0906e+00, -1.9840e-01,  ...,  1.3441e+00,\n",
              "          -3.5827e-01,  5.3981e-01]],\n",
              "\n",
              "        [[-3.0760e-01, -3.7542e-01, -1.0754e-01,  ..., -5.9489e-02,\n",
              "          -1.4631e-02,  2.1001e-02],\n",
              "         [-1.1553e-01, -1.3798e-01, -4.7470e-02,  ..., -3.6192e-02,\n",
              "           9.5579e-03,  8.3032e-03],\n",
              "         [-6.5852e-02,  1.4588e-01,  1.6687e-02,  ..., -2.8465e-01,\n",
              "           4.3099e-01, -5.2758e-01],\n",
              "         ...,\n",
              "         [-4.3223e-01, -5.5029e-01, -4.3628e-01,  ...,  9.9557e-02,\n",
              "          -8.3231e-01, -3.0055e-01],\n",
              "         [ 1.4667e-01, -4.2930e-01,  9.1437e-01,  ..., -9.3613e-01,\n",
              "          -9.9220e-02, -5.5777e-01],\n",
              "         [ 4.8183e-01, -1.7314e+00, -3.4636e-01,  ..., -1.4586e-01,\n",
              "           4.0325e-01, -1.2293e+00]],\n",
              "\n",
              "        [[ 6.1342e-04,  7.2049e-04,  7.1561e-05,  ...,  1.2208e-04,\n",
              "           3.3861e-04,  4.5543e-04],\n",
              "         [ 9.6232e-02, -4.4913e-01, -7.7305e-01,  ...,  1.0075e-01,\n",
              "           3.7246e-01, -4.8026e-01],\n",
              "         [ 2.2222e-02, -7.6688e-02, -2.0581e-02,  ...,  1.0729e-02,\n",
              "           3.8158e-02, -2.5468e-02],\n",
              "         ...,\n",
              "         [ 1.3361e-01, -8.7225e-02, -7.3094e-03,  ...,  3.5681e-01,\n",
              "           3.5420e-01, -9.8369e-02],\n",
              "         [ 3.2332e+00, -2.3090e-01,  2.2144e-02,  ...,  2.7676e-01,\n",
              "          -8.9966e-01, -3.5184e-01],\n",
              "         [-2.4937e-01, -2.2478e+00, -1.3307e-01,  ..., -1.4703e+00,\n",
              "          -1.8279e+00,  6.5844e-01]],\n",
              "\n",
              "        [[-2.6277e-03,  3.3952e-03,  6.3521e-03,  ..., -1.1563e-03,\n",
              "           7.4199e-03, -1.0553e-02],\n",
              "         [ 1.1890e-02,  5.6765e-02,  4.4587e-02,  ..., -1.8370e-02,\n",
              "           4.0140e-02, -4.7022e-02],\n",
              "         [ 1.5348e-01,  3.2547e-01,  1.5626e-01,  ..., -1.0435e-01,\n",
              "           8.5537e-02, -8.2860e-02],\n",
              "         ...,\n",
              "         [-3.3118e-01,  5.6110e-01, -1.0983e+00,  ...,  8.3108e-01,\n",
              "          -2.8214e-01,  7.5116e-02],\n",
              "         [ 8.5463e-01, -3.0978e-01, -3.5424e+00,  ...,  1.8912e+00,\n",
              "          -1.1922e+00,  2.1160e+00],\n",
              "         [ 1.3477e+00,  1.6274e+00, -1.1920e+00,  ..., -1.3920e+00,\n",
              "          -4.5586e-01,  2.2494e+00]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei @ x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS-BArcJiNAH"
      },
      "source": [
        "**One more important thing for a single self attention head**\n",
        "\n",
        "The product of query & key vectors -> Weights\n",
        "We don't * Weights directly with X\n",
        "We bring in other weights/parameters -> Values. (This is what I have, I am looking for and this is what I will contribute)/ Think of X has hidden/private"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHgGDWeukW44",
        "outputId": "ac82a3b5-565e-4c62-9318-0fcb8a2b5578"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=32, out_features=16, bias=True)"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Value is what gets aggregated in terms of this single head between the different nodes/tokens. What I will communicate.\n",
        "\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjFRHRxfkaQG",
        "outputId": "970b1f1d-4d73-406c-eef2-1f353a37f9b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = value(x)\n",
        "v.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEbiAUnLkceC",
        "outputId": "e57dcc7d-fb86-4595-f413-bde0596106a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 8])"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EssYRZkwkhnd",
        "outputId": "81d9b13c-c9d0-4f55-cb50-b52f47c4aab2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = wei @ v\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjyeFFYoo16X"
      },
      "source": [
        "### Attention Theory:\n",
        "\n",
        "1. Attention is a communication mechanism (Nodes in a directed graph)/connected with each other. These nodes have some information vectors and you are just doing sorts of weighted average (in a data way/for the incoming nodes).\n",
        "\n",
        "2. There is no notion of space here. Aap ko khud se positional embeddings of tokens daalni hogi.\n",
        "\n",
        "3. Batch-wise saare operations hote hai/ Vo independent of each hai. At max only 8 tokens are communicating.\n",
        "\n",
        "4. Attention works on any graph/ And our Language model ke case me it is like\n",
        "1->self1->2->1->self2->3->1->2->self3 like this.\n",
        "\n",
        "5. Self Vs Cross Attention: Self attention me all Key, Queries, and Values, are all obtained from x. While in Cross attention. Query(x) comes from x, while we bring in keys and values from some other sources/encoding blocks.\n",
        "(Self-serving nodes.)\n",
        "\n",
        "6. Attention supports both/ Encoding and Decoding mechanism. (Like if the Future tokens can talk with the past ones or not.)\n",
        "\n",
        "```wei.masked_fill(tril == 0, float('inf'))```\n",
        "Ye line hata do to all tokens can fully communicate (Anomaly detction/Sentiment analysis with TFs)\n",
        "\n",
        "7. Scaled Self-Attention comes from the Research Paper (Attention is all you need).\n",
        "Output/Attention(Key, Queries, Values) = Sigmoid(Queries * Keys.T) * Values / (Head_Size)**0.5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HsgxnwSr8NQ"
      },
      "source": [
        "### Why Scaled Attention?\n",
        "\n",
        "1. Weights (Queries * Keys.T) ke variance ko unit karne ke liye.\n",
        "2. Yadi weights high ho/ To unpar softmax lagane se one-hot vectors ban jaaege/ High probabilities, to only limited tokens you are considering meaningful, which you don't want atleast initially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMaN_yfCr-iM",
        "outputId": "50ab97ca-095f-498f-9eb0-38232fd7596c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B, T, C = 4, 8, 32\n",
        "head_size = 16\n",
        "\n",
        "k = torch.randn((B, T, head_size))\n",
        "q = torch.randn((B, T, head_size))\n",
        "k.shape, q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scwoEhdGr20M",
        "outputId": "809756d1-4238-4274-e1f1-670355e4cee7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 8]),\n",
              " tensor([[[ 4.7481e+00, -2.4683e+00, -1.0876e+00, -9.7772e+00, -2.3396e-01,\n",
              "            6.1394e+00,  3.3382e+00,  2.2931e+00],\n",
              "          [ 2.7978e-02, -3.1605e+00,  1.2273e+00, -5.1468e+00,  3.1734e+00,\n",
              "            3.2849e+00,  1.0947e+01, -2.7826e+00],\n",
              "          [ 2.9049e+00,  1.9506e+00, -2.7506e+00,  1.6472e+00, -6.7642e-01,\n",
              "           -2.0537e-01, -3.6162e-01, -8.8007e-01],\n",
              "          [ 8.0480e-01, -4.1009e+00,  8.8652e-01, -4.5195e+00,  5.3437e+00,\n",
              "           -1.0637e+00,  3.1171e+00,  2.6227e+00],\n",
              "          [-5.2459e+00,  2.4201e+00, -3.4533e+00, -2.0364e+00,  2.0968e+00,\n",
              "           -2.2742e+00,  4.3225e+00,  2.8033e+00],\n",
              "          [ 2.9932e+00, -2.6038e+00,  2.6847e+00, -7.3418e-01, -3.2006e+00,\n",
              "           -1.9112e+00,  2.2291e+00,  1.9676e+00],\n",
              "          [-5.7295e+00,  3.0572e+00,  5.9835e+00,  6.3362e+00, -6.7752e+00,\n",
              "            7.3328e+00, -2.6064e-01, -2.0147e+00],\n",
              "          [ 4.1902e-01, -1.9252e+00, -1.2346e+00,  1.3810e+00,  3.1504e-01,\n",
              "           -5.0383e+00, -7.6571e-01,  2.5307e+00]],\n",
              " \n",
              "         [[ 1.7241e+00,  1.0910e+00,  1.1654e+00,  5.1550e+00,  2.1065e+00,\n",
              "            1.6293e+00,  4.5628e+00,  3.2541e+00],\n",
              "          [ 3.3256e+00, -3.3895e+00,  1.8330e+00,  8.3644e-01, -4.6448e+00,\n",
              "            5.2581e+00,  9.7054e-01,  6.8858e-01],\n",
              "          [-3.3412e+00,  9.1237e+00,  3.0946e+00,  1.2818e+00, -2.7246e+00,\n",
              "           -2.8867e+00, -2.1582e-01,  9.4141e-01],\n",
              "          [ 3.2750e+00, -3.1330e+00, -2.7894e+00,  1.8807e-01,  7.7343e+00,\n",
              "           -6.2303e+00, -6.8340e-01,  3.3899e+00],\n",
              "          [ 3.5965e-01, -1.0468e+00,  8.0326e-01, -4.2563e+00, -4.8191e+00,\n",
              "            8.0796e-01, -1.4010e+00,  7.9063e-01],\n",
              "          [-1.2845e+00,  9.7821e+00,  3.6992e+00,  2.5722e+00, -1.6115e+01,\n",
              "            1.4594e+00,  4.8297e+00, -3.3382e+00],\n",
              "          [-2.3172e-01,  3.3545e+00,  1.9174e-01,  2.6305e+00,  2.5691e+00,\n",
              "            1.1520e-02,  5.4361e+00,  3.5584e+00],\n",
              "          [ 5.9253e+00, -4.4791e+00, -5.6097e+00, -2.0648e+00,  3.5165e+00,\n",
              "           -5.9119e+00, -6.0990e+00,  8.9995e-01]],\n",
              " \n",
              "         [[ 2.3298e+00, -3.1867e+00,  3.1579e+00, -6.6782e+00,  3.5397e+00,\n",
              "           -4.7116e+00,  3.3236e-02,  2.6511e+00],\n",
              "          [-9.6591e+00,  5.1419e+00, -2.6282e+00, -6.4686e-01,  3.6828e+00,\n",
              "           -6.7375e-01, -6.9270e+00, -4.5795e+00],\n",
              "          [-1.2709e+00,  3.5573e+00, -8.9747e-01,  1.7822e+00,  1.5565e+00,\n",
              "           -2.2913e-01,  8.6586e-01, -1.2179e+00],\n",
              "          [ 9.0274e-01,  2.6820e+00, -1.2910e+00,  2.8459e+00,  2.4012e+00,\n",
              "           -5.0163e+00,  2.4085e+00,  7.3507e+00],\n",
              "          [-2.8442e+00,  1.8725e+00, -2.3184e+00,  4.7221e+00, -1.9478e+00,\n",
              "           -9.1200e-01, -5.9230e-01, -1.5298e+00],\n",
              "          [ 1.7180e+00,  5.7842e+00,  5.7850e+00, -1.4554e+00, -4.2825e+00,\n",
              "           -2.1695e+00, -1.4961e-01,  4.5675e-01],\n",
              "          [ 1.8862e+00,  2.7793e+00, -3.5723e-01, -2.8942e+00, -2.1638e+00,\n",
              "            2.7742e+00, -1.0807e+00, -3.2395e+00],\n",
              "          [-6.6914e+00,  3.6633e+00,  1.2463e+00,  7.9428e-01, -3.0737e+00,\n",
              "           -1.1516e+00, -2.1977e+00, -1.3915e+00]],\n",
              " \n",
              "         [[ 3.3887e+00, -2.0595e+00, -2.5013e+00, -1.1455e+00,  4.3084e+00,\n",
              "           -3.0745e+00, -4.1657e+00, -3.0228e+00],\n",
              "          [-1.3499e+00,  3.5679e+00, -1.5322e+00,  1.0272e-01, -3.4244e+00,\n",
              "            5.0063e+00, -7.7213e-01, -3.6144e+00],\n",
              "          [ 1.1222e+01, -2.5133e+00,  5.7549e+00,  4.5034e-01,  2.5760e+00,\n",
              "            3.3641e+00, -2.6380e+00, -4.9174e-01],\n",
              "          [ 1.1555e+01, -1.2262e+01,  6.2397e+00,  2.8020e+00, -6.5823e+00,\n",
              "            6.5658e+00, -8.1396e+00,  5.1051e+00],\n",
              "          [-1.4551e+00,  2.7691e+00,  1.9610e+00,  1.0874e+00,  2.0734e+00,\n",
              "           -2.7044e-02, -9.6716e-01,  3.9717e+00],\n",
              "          [-6.3005e+00, -1.1496e+00, -6.2286e+00, -7.4490e+00,  1.7772e+00,\n",
              "           -4.9893e-01,  6.1872e-01, -2.7234e+00],\n",
              "          [ 7.2836e-02,  2.8577e+00, -6.2650e+00, -3.0660e+00,  3.1866e+00,\n",
              "            1.7816e-01,  7.0069e-02, -4.0100e+00],\n",
              "          [ 2.2416e+00, -4.5227e+00,  1.3960e+00,  3.1660e+00, -2.9025e-01,\n",
              "            3.6415e+00, -9.1456e+00,  3.1287e+00]]]))"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = q @ k.transpose(-2, -1)\n",
        "wei.shape, wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPAQBalElHY5",
        "outputId": "28ce34c5-d1bd-4163-b4fc-a8bc7e00c4e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(1.0057), tensor(0.9344), tensor(15.4895))"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var(), q.var(), wei.var() # So, the variance of weights is of the order of head_size\n",
        "# Is se bachne ke liye we divide it by sqrt(head_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LbOl7ems1jm",
        "outputId": "ed7ca240-2e29-4115-e7bd-101324c7c867"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9681)"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(wei / (head_size)**0.5).var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1FrLlrdtU8S",
        "outputId": "57f013db-554d-4f12-da88-6a2085b1fbe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 4.7481e+00, -2.4683e+00, -1.0876e+00, -9.7772e+00, -2.3396e-01,\n",
              "           6.1394e+00,  3.3382e+00,  2.2931e+00],\n",
              "         [ 2.7978e-02, -3.1605e+00,  1.2273e+00, -5.1468e+00,  3.1734e+00,\n",
              "           3.2849e+00,  1.0947e+01, -2.7826e+00],\n",
              "         [ 2.9049e+00,  1.9506e+00, -2.7506e+00,  1.6472e+00, -6.7642e-01,\n",
              "          -2.0537e-01, -3.6162e-01, -8.8007e-01],\n",
              "         [ 8.0480e-01, -4.1009e+00,  8.8652e-01, -4.5195e+00,  5.3437e+00,\n",
              "          -1.0637e+00,  3.1171e+00,  2.6227e+00],\n",
              "         [-5.2459e+00,  2.4201e+00, -3.4533e+00, -2.0364e+00,  2.0968e+00,\n",
              "          -2.2742e+00,  4.3225e+00,  2.8033e+00],\n",
              "         [ 2.9932e+00, -2.6038e+00,  2.6847e+00, -7.3418e-01, -3.2006e+00,\n",
              "          -1.9112e+00,  2.2291e+00,  1.9676e+00],\n",
              "         [-5.7295e+00,  3.0572e+00,  5.9835e+00,  6.3362e+00, -6.7752e+00,\n",
              "           7.3328e+00, -2.6064e-01, -2.0147e+00],\n",
              "         [ 4.1902e-01, -1.9252e+00, -1.2346e+00,  1.3810e+00,  3.1504e-01,\n",
              "          -5.0383e+00, -7.6571e-01,  2.5307e+00]],\n",
              "\n",
              "        [[ 1.7241e+00,  1.0910e+00,  1.1654e+00,  5.1550e+00,  2.1065e+00,\n",
              "           1.6293e+00,  4.5628e+00,  3.2541e+00],\n",
              "         [ 3.3256e+00, -3.3895e+00,  1.8330e+00,  8.3644e-01, -4.6448e+00,\n",
              "           5.2581e+00,  9.7054e-01,  6.8858e-01],\n",
              "         [-3.3412e+00,  9.1237e+00,  3.0946e+00,  1.2818e+00, -2.7246e+00,\n",
              "          -2.8867e+00, -2.1582e-01,  9.4141e-01],\n",
              "         [ 3.2750e+00, -3.1330e+00, -2.7894e+00,  1.8807e-01,  7.7343e+00,\n",
              "          -6.2303e+00, -6.8340e-01,  3.3899e+00],\n",
              "         [ 3.5965e-01, -1.0468e+00,  8.0326e-01, -4.2563e+00, -4.8191e+00,\n",
              "           8.0796e-01, -1.4010e+00,  7.9063e-01],\n",
              "         [-1.2845e+00,  9.7821e+00,  3.6992e+00,  2.5722e+00, -1.6115e+01,\n",
              "           1.4594e+00,  4.8297e+00, -3.3382e+00],\n",
              "         [-2.3172e-01,  3.3545e+00,  1.9174e-01,  2.6305e+00,  2.5691e+00,\n",
              "           1.1520e-02,  5.4361e+00,  3.5584e+00],\n",
              "         [ 5.9253e+00, -4.4791e+00, -5.6097e+00, -2.0648e+00,  3.5165e+00,\n",
              "          -5.9119e+00, -6.0990e+00,  8.9995e-01]],\n",
              "\n",
              "        [[ 2.3298e+00, -3.1867e+00,  3.1579e+00, -6.6782e+00,  3.5397e+00,\n",
              "          -4.7116e+00,  3.3236e-02,  2.6511e+00],\n",
              "         [-9.6591e+00,  5.1419e+00, -2.6282e+00, -6.4686e-01,  3.6828e+00,\n",
              "          -6.7375e-01, -6.9270e+00, -4.5795e+00],\n",
              "         [-1.2709e+00,  3.5573e+00, -8.9747e-01,  1.7822e+00,  1.5565e+00,\n",
              "          -2.2913e-01,  8.6586e-01, -1.2179e+00],\n",
              "         [ 9.0274e-01,  2.6820e+00, -1.2910e+00,  2.8459e+00,  2.4012e+00,\n",
              "          -5.0163e+00,  2.4085e+00,  7.3507e+00],\n",
              "         [-2.8442e+00,  1.8725e+00, -2.3184e+00,  4.7221e+00, -1.9478e+00,\n",
              "          -9.1200e-01, -5.9230e-01, -1.5298e+00],\n",
              "         [ 1.7180e+00,  5.7842e+00,  5.7850e+00, -1.4554e+00, -4.2825e+00,\n",
              "          -2.1695e+00, -1.4961e-01,  4.5675e-01],\n",
              "         [ 1.8862e+00,  2.7793e+00, -3.5723e-01, -2.8942e+00, -2.1638e+00,\n",
              "           2.7742e+00, -1.0807e+00, -3.2395e+00],\n",
              "         [-6.6914e+00,  3.6633e+00,  1.2463e+00,  7.9428e-01, -3.0737e+00,\n",
              "          -1.1516e+00, -2.1977e+00, -1.3915e+00]],\n",
              "\n",
              "        [[ 3.3887e+00, -2.0595e+00, -2.5013e+00, -1.1455e+00,  4.3084e+00,\n",
              "          -3.0745e+00, -4.1657e+00, -3.0228e+00],\n",
              "         [-1.3499e+00,  3.5679e+00, -1.5322e+00,  1.0272e-01, -3.4244e+00,\n",
              "           5.0063e+00, -7.7213e-01, -3.6144e+00],\n",
              "         [ 1.1222e+01, -2.5133e+00,  5.7549e+00,  4.5034e-01,  2.5760e+00,\n",
              "           3.3641e+00, -2.6380e+00, -4.9174e-01],\n",
              "         [ 1.1555e+01, -1.2262e+01,  6.2397e+00,  2.8020e+00, -6.5823e+00,\n",
              "           6.5658e+00, -8.1396e+00,  5.1051e+00],\n",
              "         [-1.4551e+00,  2.7691e+00,  1.9610e+00,  1.0874e+00,  2.0734e+00,\n",
              "          -2.7044e-02, -9.6716e-01,  3.9717e+00],\n",
              "         [-6.3005e+00, -1.1496e+00, -6.2286e+00, -7.4490e+00,  1.7772e+00,\n",
              "          -4.9893e-01,  6.1872e-01, -2.7234e+00],\n",
              "         [ 7.2836e-02,  2.8577e+00, -6.2650e+00, -3.0660e+00,  3.1866e+00,\n",
              "           1.7816e-01,  7.0069e-02, -4.0100e+00],\n",
              "         [ 2.2416e+00, -4.5227e+00,  1.3960e+00,  3.1660e+00, -2.9025e-01,\n",
              "           3.6415e+00, -9.1456e+00,  3.1287e+00]]])"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7A7RDfaty_c"
      },
      "outputs": [],
      "source": [
        "# So, apply scaling/ division by the sqrt(head_size) first and then apply softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_TcLrZax4RF"
      },
      "source": [
        "# Master Code (Similar to the Research Paper: Attention is all you need):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXZzltP1w4Q"
      },
      "source": [
        "## Base/Platform:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaZAh61Xzb6F"
      },
      "source": [
        "1. Self attention can't tolerate very high learning rates.\n",
        "\n",
        "2. Even reduce (3e-4) because of a very highly complex model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBkVhvcNM7XL",
        "outputId": "d1792b07-60ec-4cbb-fa2d-45fea4c54a67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0-1): 2 x Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Module list holds a list of layers/ nn.Module objects.\n",
        "# Pytorch handles (parameters) and updation at the backend.\n",
        "\n",
        "nn.ModuleList([nn.Linear(1, 1), nn.Linear(1, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjfENsW3E25z",
        "outputId": "d5261883-3522-414f-f210-1675895d40a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
              "  (1): Linear(in_features=3, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.Sequential(*[nn.Linear(3, 3)]*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekvZ9RBFaDQ"
      },
      "source": [
        "## Optimization and other pointers (Mentioned in the Research Paper):\n",
        "\n",
        "1. Residual/Skip connections (which solves vanishing gradient problem). During B.P as straight highway for the loss to propagate.\n",
        "x = x (original/highway) + x (passed through the network)\n",
        "\n",
        "Allow deeper networks to learn effectively.\n",
        "(Gradient atakta nahi hai in the deep layers)\n",
        "\n",
        "2. Certain Projection layers.\n",
        "\n",
        "**I guess idea is after multi head attention and Feed forward you have a few Linear layers to change the dimension/(projection). Possibly like agar head_size and n_embed alag-lag hai.**\n",
        "\n",
        "In MultiHeadAttention, after concatenating the heads, you typically apply a linear transformation to project the concatenated result back into the original embedding space. This linear layer isn't a skip connection by itself but is used to reduce or change the dimensionality after combining the attention heads. It can be seen as a \"projection\" because it adjusts the output space after attention.\n",
        "\n",
        "In FeedForward, applying a linear layer also isn't a skip connection. It's just a transformation applied after the activation function (like ReLU), often used to map the output to a desired size or shape.\n",
        "\n",
        "**Feed Forward network me according to paper x -> 4x and back from 4x to x kind of projection hoti hai.**\n",
        "\n",
        "---\n",
        "\n",
        "Avoid overfitting.\n",
        "\n",
        "\n",
        "3. Layer Normalization.\n",
        "\n",
        "Idea is jo actiavtions/data/matrices flowing through the network, sometimes, they may explode/blow up or vanish (jis se training sahi se nahi ho paati). Slow down/unstable.\n",
        "\n",
        "Normalize activations across a feature (training faster and generalize better). In case of Language models -> Ye hum -> across each token (Channel dimension pe karte hai.)\n",
        "\n",
        "**Pre-norm formulation: Slight deviation from the Research Paper.**\n",
        "The layer norms are applied before the Multi-Head Attention Block and the Computation one (Feed forward).\n",
        "\n",
        "**One layer norm after all the Transformer blocks -> before feeding it to the LM Head (Jaha pe projection hogi towards -> Vocabulary and Softmax calculations)**\n",
        "\n",
        "\n",
        "Layer norms have some trainable parameters.\n",
        "\n",
        "4. Dropout (Generic).\n",
        "\n",
        "* Dropout after projection layer in Multi-Head Attention\n",
        "* Dropout after projection layer in FFWD N/W\n",
        "* Dropout after the weights matrix calculation in Single Self-Attention Head (B, T, T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2M2_SEs10Nd"
      },
      "source": [
        "## Code (Evolution):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhEmBWc1zC8U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFFS9HK1KuqS",
        "outputId": "20291d5e-03bb-4463-b0ff-d5e3fe3e0110"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available() # GPU check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is0-TUFxtCu",
        "outputId": "deef1d8b-a066-4722-8067-41d9a411f629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e7928321710>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameters.\n",
        "\n",
        "batch_size = 64 # # Samples/Sequences of Input tokens to process in ||. (NN batch inputs).\n",
        "block_size = 256 # Context Window (Time dimension).\n",
        "epochs = 5000 # One complete pass of the training data set.\n",
        "eval_interval = 500 # After how many iterations/epochs we will compute the loss. (Andrej K's logic).\n",
        "eval_iters = 200 # How many unique samples of the training and validation data to use to compute the loss (Since every sample is more/less noisy).\n",
        "learning_rate = 3e-4 # Self-attention can't tolerate very high learning rates. (Even reduce/ Highly complex model)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU VS CPU.\n",
        "n_embed = 384 # Embedding vector length.\n",
        "n_heads = 6 # Number of Single self-attention heads in the Mutli-head attention block.\n",
        "n_layers = 6 # Number of Transformer blocks (Multi-head attention + Feed forward) # (Communication + Build on it (Computation)).\n",
        "dropout = 0.2 # Dropout rate.\n",
        "\n",
        "# Reproducibility.\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yCZxUUC2Ug2",
        "outputId": "3ba09faf-a6ea-4922-accf-6fb3c9d189f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-05 06:25:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-05 06:25:27 (18.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-MD2cwlz-jZ",
        "outputId": "3acc734d-64fd-4922-938a-17e5e371e996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieyHG91405ew"
      },
      "outputs": [],
      "source": [
        "vocabulary = sorted(list(set(text))) # All the possible tokens, the model can see and emmit.\n",
        "vocab_size = len(vocabulary) # Number of distinct tokens.\n",
        "\n",
        "stoi = {s:i for i, s in enumerate(vocabulary)}\n",
        "itos = {i:s for i, s in enumerate(vocabulary)}\n",
        "\n",
        "encode = lambda text: [stoi[s] for s in text]\n",
        "decode = lambda encoding: ''.join([itos[i] for i in encoding])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS9_R_J_3Jkn",
        "outputId": "2655e2d9-8b1c-42a4-b060-02dc97afa1e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([20, 47, 2], 'Hi!')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode('Hi!'), decode([20, 47, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6MFKiE94IXl",
        "outputId": "165addd2-9253-4bbe-9302-43b7963121c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56,  ..., 45,  8,  0])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(encode(text))\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxtkFAY84OaJ",
        "outputId": "ad830e2a-a590-40b7-8ef5-cca9b55663aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1115394, 1115394)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data), len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfuDt0cM3eYD",
        "outputId": "c1a8c902-eaf9-463c-b945-ee32bed879c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training and Validation data split.\n",
        "\n",
        "split_size = int(0.9 * len(data))\n",
        "training_data = data[:split_size]\n",
        "validation_data = data[split_size:]\n",
        "len(training_data) + len(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jinOMTtz47gc"
      },
      "outputs": [],
      "source": [
        "# Get the batches of data (X and Y).\n",
        "# 1. Training data (X and Y) ko we will move to the device.\n",
        "\n",
        "def get_batch(data_type):\n",
        "  _data = training_data if data_type == 'train' else validation_data\n",
        "  idxs = torch.randint(0, len(_data) - block_size, (batch_size, )) # Batch size number of random indeces.\n",
        "  xb = torch.stack([_data[idx: idx + block_size] for idx in idxs], dim = 0)\n",
        "  yb = torch.stack([_data[idx + 1: idx + block_size + 1] for idx in idxs], dim = 0)\n",
        "\n",
        "  # Move the training data to device (GPU/CPU).\n",
        "  xb = xb.to(device) # Re-initialization is required.\n",
        "  yb = yb.to(device)\n",
        "\n",
        "  return xb, yb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZpPuU3EWkKE"
      },
      "source": [
        "**GO THROUGH FROM BOTTOM TO TOP:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGOyk_2I8TGF"
      },
      "source": [
        "NOTE:\n",
        "\n",
        "```n_embed = head_size```\n",
        "\n",
        "Overall head_size after concatenation inside the Multi-Head Attention Block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3NvE-Mp6J9F"
      },
      "outputs": [],
      "source": [
        "# Intersperse the communication (Bikher do/open multiple communication channels with Multi-Head Attention & then computation (FFWDN))\n",
        "class Block(nn.Module):\n",
        "  '''Transformer Block (Communication followed by Computation).'''\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    # head_size is the final head size that is required after concatenation (which will be equal to n_embed only).\n",
        "    '''Layer Norm (Across Channel/For all tokens)'''\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.mha = MultiHeadAttention(n_heads, head_size)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.ffwd = FeedForward()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x -> B, T, C (n_embed) -> B, T, C (head_size or n_embed) (After MHA) -> B, T, C (head_size or n_embed) -> After Feed Forward.\n",
        "    '''\n",
        "    Residual/Skip Connection. (x + )\n",
        "    Pre-Norm Formulation (Layer norm before Multi-head attention and FFWD)\n",
        "    '''\n",
        "    x = x + self.mha(self.ln1(x))\n",
        "    return x + self.ffwd(self.ln2(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# You opened a lot of communication channels to allow the tokens to talk (Multi-Head Attention).\n",
        "# But you need to learn/work on that knowledge and then move to logits.\n",
        "# Basically lm_head (final projection to the vocabulary) ke pehle complexity add kar rahe hai.\n",
        "class FeedForward(nn.Module):\n",
        "  '''A simple feed forward NN after the Multi-Head Attention.'''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 1 X 4 X 1 is suggested in the Research Paper.\n",
        "    self.nn = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed), # An extra layer before the language model head.\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),  # Projection layer (added here only).\n",
        "        nn.Dropout(dropout) # Dropout layer.\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x -> (B, T, C (n_embed)) -> (B, T, C (n_embed))\n",
        "    return self.nn(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Multiple Self-Attention Heads in parallel and their concatenation.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  '''Opening multiple channels of communication (via multiple single self-attention heads), since these tokens have a lot to talk about.'''\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    # head_size is the final head size that is required after concatenation (which will be equal to n_embed only).\n",
        "    # Fir usi par next block operate karega.\n",
        "    self.sa_heads = nn.ModuleList([Head(head_size // n_heads) for _ in range(n_heads)]) # A list of Self-attention heads.\n",
        "\n",
        "    self.projection = nn.Linear(head_size, n_embed) # Dono same hi hai. Jaan ke alag-lag likha hai.\n",
        "    # A projection layer to change the dimensionality.\n",
        "\n",
        "    '''Dropout layer after the projections.'''\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x -> B, T, C (n_embed) -> (B, T, C (head_size)/(n_heads))*n_heads\n",
        "    '''\n",
        "    return torch.concat([sa_head(x) for sa_head in self.sa_heads], dim = -1) # Along the last axis (C vaali). (B, T, C (head_size)/(n_heads))*n_heads\n",
        "    '''\n",
        "    out = torch.concat([sa_head(x) for sa_head in self.sa_heads], dim = -1) # Along the last axis (C vaali). (B, T, C (head_size)/(n_heads))*n_heads\n",
        "    return self.dropout(self.projection(out)) # (B, T, C (head_szie)) -> (B, T, C (n_embed)) # Both are same. Neeche Language Model me jo Blocks ke constructor me call hua hai.\n",
        "    # ++ self.dropout() at the end.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# A Single Self-Attention Head/Block.\n",
        "class Head(nn.Module):\n",
        "  '''Single Self-Attention Head.'''\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    # Key, Query, and Value vectors per token.\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False) # They will operate on x, right, and x is B, T, C (n_embed)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "\n",
        "    # Fixed size ka lower triangular matrix hai, but we will slice it on the basis of T (The current tokens in time in x).\n",
        "    self.register_buffer('tril', torch.ones((block_size, block_size)).tril()) # Not to be considered as a model parameter. It is a buffer.\n",
        "\n",
        "    self.head_size = head_size # Trying to pass constant values to the forward/any other method.\n",
        "\n",
        "    '''Dropout layer after the previous/other tokens' weights learned.'''\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape # (B, T, C (n_embed)) # T can vary between 1 to block_size.\n",
        "\n",
        "    k = self.key(x) # (B, T, C (n_embed)) @ (n_embed, head_size) -> (B, T, C (head_size))\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size**(-0.5)) # Scaled self-attention # (B, T, C (head_size)) @ (B, C(head_size), T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Decoder mechanism (Future tokens can't talk). (B, T, T)\n",
        "    wei = wei.softmax(dim = -1) # Along the last dimension. (B, T, T)\n",
        "\n",
        "    wei = self.dropout(wei) # Dropout layer.\n",
        "\n",
        "    return wei @ v # (B, T, T) @ (B, T, C (head_size)) -> (B, T, C (head_size))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# A Large Language Model to replicate the Research Paper (Attention is all you need!) structure.\n",
        "class LanguageModel(nn.Module):\n",
        "  '''Large Language Model.'''\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Attributes.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # [Number of embeddings: vocab_size (65), C (Embedding dimension) = n_embed]\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed) # [Number of embeddings: block_size, C (n_embed)]. An embedding per position.\n",
        "\n",
        "    '''\n",
        "    # Single Self-Attention Head.\n",
        "    self.sa_head = Head(head_size)\n",
        "\n",
        "    # Multi-Head Attention Block.\n",
        "    self.sa_heads = MultiHeadAttention(n_heads, head_size)\n",
        "\n",
        "    # Feed-Forward Network.\n",
        "    self.ffwd = FeedForward() # One Linear layer -> B, T, C (head_size) -> B, T, C (head_size) + Relu -> B, T, C (vocab_size)\n",
        "    '''\n",
        "\n",
        "    # Transformer Blocks: Multi-Head Attention + Computation.\n",
        "    # This syntax will get the individual entries/layers/models from the list and pass it to the nn.Sequential model.\n",
        "    self.blocks = nn.Sequential(*[Block(n_heads, n_embed) for _ in range(n_layers)]) # Passing in the final head_size required.\n",
        "\n",
        "    '''Layer Norm before the final projections to the vocabulary.'''\n",
        "    self.ln = nn.LayerNorm(n_embed)\n",
        "\n",
        "    # Language Model Head (To project back to the vocab_size for output).\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size) # Get the vocab size features (all possible tokens) back from the embedding dimension.\n",
        "    # Ek alag se LM head chaahiye to convert head_size (n_embed) -> vocab_size.\n",
        "\n",
        "  def forward(self, x, y = None):\n",
        "    B, T = x.shape # T can be anything between 1 to block_size.\n",
        "\n",
        "    token_embeddings = self.token_embedding_table(x) # (B, T) -> (B, T, C (n_embed))\n",
        "    # position_embeddings = self.position_embedding_table(torch.arange(block_size)) -> No, not the block size because input tokens can be less than block_size,\n",
        "    # right.\n",
        "    position_embeddings = self.position_embedding_table(torch.arange(T, device = device)) # (T, ) -> (T, C (n_embed))\n",
        "    # Perhaps we are creating a new tensor here (torch.arange(T)) and need to move that to device as well.\n",
        "    x = token_embeddings + position_embeddings # (B, T, C (n_embed)) + (T, C (n_embed))\n",
        "\n",
        "    '''\n",
        "    # Single Self-Attention Head.\n",
        "    x = self.sa_head(x) # (B, T, C (n_embed)) -> (B, T, C (head_size))\n",
        "\n",
        "    # Multi-Head Attention Block.\n",
        "    x = self.sa_heads(x) # (B, T, C (n_embed)) -> (B, T, C (n_heads * head_size/n_heads)) -> (B, T, C (head_size))\n",
        "\n",
        "    # Feed-Forward Network.\n",
        "    logits = self.ffwd(x) # (B, T, C (head_size)) -> (B, T, C (vocab_size))\n",
        "    '''\n",
        "\n",
        "    # Transformer Blocks.\n",
        "    x = self.blocks(x) # (B, T, C (n_embed)) -> (B, T, C (head_size)) # And dono same hai.\n",
        "\n",
        "    # Final Layer Norm.\n",
        "    x = self.ln(x) # (B, T, C (n_embed or head_size)) But standardized across the C (-1) dimension.\n",
        "\n",
        "    # Language Model head.\n",
        "    logits = self.lm_head(x) # (B, T, C (n_embed)) -> (B, T, C (vocab_size = 65))\n",
        "\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits.view(B * T, vocab_size), y.view(B * T))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, x, max_tokens):\n",
        "    '''\n",
        "    # Bi-Gram model.\n",
        "    for _ in range(max_tokens):\n",
        "      logits, loss = self(x) # (B, T, C (vocab_size))\n",
        "      logits = logits[:, -1, :] # (B, C (vocab_size)) -> Last token in the Time dimension.\n",
        "\n",
        "      probabilities = logits.softmax(dim = 1) # (B, C)\n",
        "      x_next = torch.multinomial(probabilities, 1, replacement = True) # (B, 1)\n",
        "      x = torch.concat([x, x_next], dim = 1) # (B, T) + (B, 1) -> (B, T + 1)\n",
        "    '''\n",
        "\n",
        "    for _ in range(max_tokens):\n",
        "      x_block = x[:, -block_size:] # Restricting to only the last block_size tokens. (B, T)\n",
        "      logits, loss = self(x_block) # (B, T, C (vocab_size))\n",
        "      logits = logits[:, -1, :] # (B, C (vocab_size)) -> Last token in the Time dimension.\n",
        "\n",
        "      probabilities = logits.softmax(dim = 1) # (B, C)\n",
        "      x_next = torch.multinomial(probabilities, 1, replacement = True) # (B, 1)\n",
        "      x = torch.concat([x, x_next], dim = 1) # (B, T) + (B, 1) -> (B, T + 1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw-snY0N-4Gs",
        "outputId": "c8c2ddd3-ce51-4585-ceff-b150e4f4a58e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Head(\n",
              "  (key): Linear(in_features=384, out_features=16, bias=False)\n",
              "  (query): Linear(in_features=384, out_features=16, bias=False)\n",
              "  (value): Linear(in_features=384, out_features=16, bias=False)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Validating that self.head_size:\n",
        "# To Buffer and Constant/No-Tensors work.\n",
        "\n",
        "head = Head(16)\n",
        "head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4MA5x-f_D89",
        "outputId": "86b5d626-d6c7-4a89-e7cc-c9fc2fa3a124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 384])\n",
            "torch.Size([16, 384])\n",
            "torch.Size([16, 384])\n"
          ]
        }
      ],
      "source": [
        "for parameter in head.parameters():\n",
        "  print(parameter.shape) # Only key, query, and value vectors (Only weights. No Biases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAHUARjF8OFD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad() # Ensure/Explicitly telling that do not involve/consider any calculations here for the gradient computation.\n",
        "def estimate_loss():\n",
        "  model.eval() # Model is in the evaluation phase.\n",
        "\n",
        "  losses = {\n",
        "      'train': torch.zeros((eval_iters, )),\n",
        "      'val': torch.zeros((eval_iters, ))\n",
        "  }\n",
        "  for data_type in losses:\n",
        "    for i in range(eval_iters):\n",
        "      xb, yb = get_batch(data_type)\n",
        "      logits, loss = model(xb, yb)\n",
        "      losses[data_type][i] = loss.item()\n",
        "    losses[data_type] = losses[data_type].mean().item()\n",
        "\n",
        "  model.train() # Model is back in the training phase.\n",
        "\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DwlkrEuPclq",
        "outputId": "be24127d-7118-4f2d-9b4d-37d6b2243b8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (sa_heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffwd): FeedForward(\n",
              "        (nn): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Model ko we will move to device.\n",
        "\n",
        "model = LanguageModel()\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e3RmTiCHYQ3",
        "outputId": "7217e340-3d52-43a6-e131-60bc847631bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randn((3, 2)).numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7tSlia5Hdfr",
        "outputId": "ed6e27ea-cbf0-4d4c-fcca-56453b22e1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on built-in function numel in module torch:\n",
            "\n",
            "numel(...)\n",
            "    numel(input) -> int\n",
            "    \n",
            "    Returns the total number of elements in the :attr:`input` tensor.\n",
            "    \n",
            "    Args:\n",
            "        input (Tensor): the input tensor.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> a = torch.randn(1, 2, 3, 4, 5)\n",
            "        >>> torch.numel(a)\n",
            "        120\n",
            "        >>> a = torch.zeros(4,4)\n",
            "        >>> torch.numel(a)\n",
            "        16\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(torch.numel) # Number of elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llpuFQ_cQEUx",
        "outputId": "f24fbd22-6d88-43d5-f5e5-d7b3ff3c49e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([65, 384])\n",
            "torch.Size([256, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([64, 384])\n",
            "torch.Size([384, 384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([1536, 384])\n",
            "torch.Size([1536])\n",
            "torch.Size([384, 1536])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([384])\n",
            "torch.Size([65, 384])\n",
            "torch.Size([65])\n"
          ]
        }
      ],
      "source": [
        "# Very important to visualize as to what is happening.\n",
        "\n",
        "for parameters in model.parameters():\n",
        "  print(parameters.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUYmT5VnHkHI",
        "outputId": "9fc1db61-88fa-4206-fccb-fde0a14cb9b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.788929"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Our model has roughly 11 Million parameters. WOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "sum([parameter.numel() for parameter in model.parameters()]) / 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c_EKmNlPnAh",
        "outputId": "7bd3e5ad-3d6c-46d4-b1ad-5aeb9fa4bb3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0003\n",
              "    maximize: False\n",
              "    weight_decay: 0.01\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Optimizer.\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSZvbL5lQR31",
        "outputId": "f406b38a-d047-4cf6-e85e-5330e5f95efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 Training Loss: 3.5282 Validation Loss: 3.5493\n",
            "Epoch: 500 Training Loss: 1.8810 Validation Loss: 1.9957\n",
            "Epoch: 1000 Training Loss: 1.5328 Validation Loss: 1.7204\n",
            "Epoch: 1500 Training Loss: 1.3953 Validation Loss: 1.6077\n",
            "Epoch: 2000 Training Loss: 1.3086 Validation Loss: 1.5537\n",
            "Epoch: 2500 Training Loss: 1.2519 Validation Loss: 1.5178\n",
            "Epoch: 3000 Training Loss: 1.2020 Validation Loss: 1.4974\n",
            "Epoch: 3500 Training Loss: 1.1597 Validation Loss: 1.4808\n",
            "Epoch: 4000 Training Loss: 1.1222 Validation Loss: 1.4816\n",
            "Epoch: 4500 Training Loss: 1.0850 Validation Loss: 1.4761\n",
            "Epoch: 4999 Training Loss: 1.0497 Validation Loss: 1.4937\n"
          ]
        }
      ],
      "source": [
        "# Training loop.\n",
        "\n",
        "# epochs = 2000\n",
        "# eval_interval = 10\n",
        "# eval_iters = 2\n",
        "for i in range(epochs):\n",
        "  xb, yb = get_batch('train') # Get a batch of my training data.\n",
        "  logits, loss = model(xb, yb) # Forward pass.\n",
        "\n",
        "  optimizer.zero_grad(set_to_none = True) # Set gradients to zero.\n",
        "  loss.backward() # Back propagation of loss/ Compute gradients.\n",
        "  optimizer.step() # Update all parameters.\n",
        "\n",
        "  if (i % eval_interval) == 0 or (i == epochs - 1):\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Epoch: {i} Training Loss: {losses['train']:.4f} Validation Loss: {losses['val']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHhL2d03WEc8",
        "outputId": "c9fc4329-f3fa-4d02-953d-18327b4cf4e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "But with price of tribe steels unwhanty\n",
            "merfly.\n",
            "\n",
            "Officion:\n",
            "Treason, and first are more talions,\n",
            "Provost in the hour, no more honour'd moan;\n",
            "But sound my slavise to begin your hands.\n",
            "\n",
            "Clown:\n",
            "I have been a clog botten of right;\n",
            "Men the dischanged your own way, here is,\n",
            "The garden, in the presecreaty floughing I cleft.\n",
            "Then, high I cargue to again.\n",
            "\n",
            "AUFIDIUS:\n",
            "A gone!\n",
            "Yet please look to none, my lord, I pray not.\n",
            "Here comes he comes the king my body!\n",
            "\n",
            "CORIOLANUS:\n",
            "He hath for his right murderers sun \n"
          ]
        }
      ],
      "source": [
        "# Inference.\n",
        "# 3. Move the testing data to the device\n",
        "\n",
        "context = torch.zeros((1, 1), dtype = torch.int, device = device) # Move the testing data onto device. Starting with a new line character.\n",
        "print(decode(model.generate(context, 500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLs-MfuQ9P0Z"
      },
      "outputs": [],
      "source": [
        "with open('tiny-shakespeare-output.txt', 'w') as file:\n",
        "  file.write(decode(model.generate(context, 10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phXHr9vBApZR"
      },
      "source": [
        "# Encoder & Decoder Mechanism.\n",
        "\n",
        "What we implemented is a Decoder only transformer.\n",
        "\n",
        "Encoder & Cross-Attention:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRyZuwxkA1cW"
      },
      "outputs": [],
      "source": [
        "# ----------------ENCODER----------------|----------------DECODER----------------\n",
        "# Les réseaux de neurones sont géniaux|<START>Neural networks are awesome<END> (<START> & <END> are special tokens)\n",
        "# The Research Paper diagram is for Machine Translation.\n",
        "\n",
        "# If you want to condition the generation based upon some additional information (like the French sentence, here).\n",
        "# Encoder for French -> No triangular mask (They are free to talk with all) # Just encode the content of French Sentence.\n",
        "# Decoder will get the keys and values from encoders (queries still from x)\n",
        "# Every single block of the Decoder (Transformer) me feed karenge keys and values\n",
        "\n",
        "\n",
        "# Condition the English Decoding not just because of what it knows but also the context of what it has fully seen for the French sentence.\n",
        "# Our case we just have text and we want to to emmit it (So no conditional decoding/generation -> Hence, decoder only block)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkjFkc45Drmv"
      },
      "source": [
        "# AK's Nano-GPT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ1Y6WcNDki5"
      },
      "outputs": [],
      "source": [
        "# Instead of concatenating all the heads input (in Multi-Head Attention)\n",
        "# We can also treat it like 4 D tensors (where the number of heads become one dimension).\n",
        "\n",
        "# Gelu instead of Relu (Open AI) activation.\n",
        "\n",
        "# Training Chat-GPT\n",
        "# 2 phase ->\n",
        "# 1. Pre-training phase: Similar to what we have done above (We train a decoder only transformer)\n",
        "# And we basically train it on a big chunk of the internet (Trillions of tokens) (3 Billion tokens pe train -> GPT3). We can check the GPT Research Papers\n",
        "# That will have all parameters, batch_size, head_size, layers, et cetera.\n",
        "# But our model has per character level token/ Open AI/Other models -> Sub words-specific tokens (3/4th of a word -> 1 token)\n",
        "\n",
        "# Now it just blabbers internet. If you ask it a question, it will try generating more further, right? Because that's what its trained for?\n",
        "# Then how come Chat GPT? Unaligned???\n",
        "# 2nd. stage to fine-tune it to align it be an assistant.\n",
        "# Take 1000s of exaples/Not internet -> Questions and Answers.\n",
        "# Fine tune GPT with Supervised Learning. (Explain A: Answer A)\n",
        "# Reward model (Best option/answers/labeller)/ Reinforcement learning. -> Whole aligning/fine-tuning stage.\n",
        "# Takes a model from a document completer -> QA agent. (Internal datasets) / Hard to replicate."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "47EIq9E7I49l",
        "f7INPYL9zFDH"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
